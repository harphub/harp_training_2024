---
title: Point Verifcation Workflow
---

## Workflow Steps

When doing a point verification there are a number of steps to the workflow. Some of these steps you have to do every time, and others might depend on the forecast parameter being verified, the scores you want to compute, or any conditions that you want to apply to the verification. The compulsory steps can be described as:

-   Read forecast
-   Read observations
-   Join
-   Verify
-   (Save / Plot)

In this tutorial we will go through each of these steps in turn and then introduce some optional steps, increasing the complexity as we go. It is assumed that SQLite files for both forecasts and observations have been prepared.

## Basic deterministic verification

Here we will demonstrate the workflow for a simple deterministic verification of 2m temperature. The forecasts come from the AROME-Arctic model that is run operationally by MET Norway, and we will do the verification for August 2023.

### Read forecast

As always, the first thing we need to do is to attach the packages that we are going to need. There may be some unfamiliar packages here, but they will be explained as we begin to use functions from them.

```{r attach-pkgs, message=FALSE}
library(harp)
library(here)
library(dplyr)
library(forcats)
```

All of our forecasts and observations use the same root directories so we'll set them here

```{r fcst-obs-dirs}
fcst_dir <- here("data", "FCTABLE")
obs_dir  <- here("data", "OBSTABLE")
```

Forecasts are read in with `read_point_forecast()`. We will read in the 00:00 UTC forecasts for lead times 0 - 24 hours every 3 hours.

```{r simple-read-pnt-fcst, message=FALSE}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083100, "24h"),
  fcst_model = "arome_arctic",
  fcst_type  = "det",
  parameter  = "T2m",
  file_path  = fcst_dir
)
```

### Read observations

Observations are read in with `read_point_obs()`. Observations files often contain more times and locations than we have for the forecasts. Therefore, we can tell the the function which times and locations to read with the help of `unique_valid_dttm()` and `unique_station()`.

```{r simple-read-pnt-obs, message=FALSE}
obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
)
```

### Join

Now that we have the forecasts and observations, we need to match the forecasts and observations to the same date-times and locations. We do this by joining the forecast and observations to each other using `join_to_fcst()`. Basically this is doing an inner join between the forecast and observations data frames with an extra check to make sure the forecast and observations data have the same units.

```{r simple-join, message=FALSE}
fcst <- join_to_fcst(fcst, obs)
```

### Verify

We are now ready to verify. For deterministic forecasts this is done with `det_verify()`. By default the verification is stratified by lead time. All we need to tell the function is which column contains the observations. In this case, that would be "T2m".

```{r simple-verif, message=FALSE}
det_verify(fcst, T2m)
```

We can also compute categorical scores by adding some thresholds. This will compute scores for \>= threshold categories.

```{r simple-verif-thresh, message=FALSE}
det_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))
```

### Save / Plot

Once the verification is done we can save the data using `save_point_verif()` and plot the data using `plot_point_verif()`. For plotting we give the function the verification data and the score we want to plot. The scores are basically column names in any of the verification data's data frames.

First we need to write the verification to a variable, and then save.

```{r simple-verif-thresh-var, message=FALSE}
verif <- det_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))

save_point_verif(verif, verif_path = here("data", "verification", "det"))
```

We can now plot some scores

```{r simple-verif-plt, message=FALSE, warning=FALSE, fig.align='center'}
plot_point_verif(verif, bias)

plot_point_verif(verif, rmse)

plot_point_verif(verif, frequency_bias)
```

The last plot looks strange. That's because the scores exist for each threshold and they are not being separated. We can separate them by mapping the colour to each threshold, by faceting by threshold (faceting means to separate into panels), or by filtering to a single threshold.

```{r simple-verif-plt-col-fct, message=FALSE, warning=FALSE, fig.align='center'}
plot_point_verif(verif, frequency_bias, colour_by = threshold)

plot_point_verif(verif, frequency_bias, facet_by = vars(threshold))

plot_point_verif(verif, frequency_bias, filter_by = vars(threshold == 285))
```

::: {.callout-note collapse="true"}
## **Note**: `facet_by` and `filter_by` values must be wrapped in `vars()`

This is because faceting and filtering can be done by more than one variable and the `vars()` function facilitates that.
:::

For hexbin plots, the plots are automatically faceted by the grouping variable of the verification - in this case lead time.

```{r simple-verif-plt-hex-no-eval, warning=FALSE, eval=FALSE}
plot_point_verif(verif, hexbin)
```

```{r simple-verif-plt-hex, warning=FALSE, fig.align='center', echo=FALSE}
plot_point_verif(verif, hexbin, hex_colour = "transparent")
```

## Basic ensemble verification

The workflow for verifying ensemble forecasts is much the same as that for deterministic verification. The only real difference is using the `ens_verify()` function to compute the score. In this example we will use data from the MEPS model for the same time period as before.

```{r simple-ens-verif-read-fcst, message=FALSE}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083100, "1d"),
  fcst_model = "meps",
  fcst_type  = "eps",
  parameter  = "T2m", 
  file_path  = fcst_dir 
)

obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
)

fcst <- join_to_fcst(fcst, obs)
```

```{r simple-ens-verif-verify, message=FALSE}
verif <- ens_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))

verif

save_point_verif(verif, here("data", "verification", "ens"))
```

Since this is ensemble data, ensemble specific scores have been computed as well as deterministic summary scores for each member. There are a couple of scores that `plot_point_verif()` can derive that are not columns in the data frames - these are `spread_skill` and `brier_score_decomposition`.

```{r ens-verif-plt, message=FALSE, warning=FALSE, fig.align='center'}
plot_point_verif(verif, spread_skill)

plot_point_verif(verif, crps)

plot_point_verif(verif, brier_score, facet_by = vars(threshold))

plot_point_verif(verif, brier_score_decomposition, facet_by = vars(threshold))

plot_point_verif(verif, reliability, facet_by = vars(threshold))
```

Again the last one has issues with overplotting. This is because there should be one plot for each threshold and each grouping variable (in this case `lead_time`), so we need to filter.

```{r ens-verif-plt-rel, message=FALSE, warning=FALSE, fig.align='center'}
plot_point_verif(
  verif, 
  reliability, 
  facet_by = vars(threshold),
  filter_by = vars(lead_time == 12)
)
```

## Comparing forecast models

Verification scores are often useful for comparing the performance of different models, or model developments. With harp it is straightforward to compare different models - it is simply case of reading multiple forecasts in in one go. Here we are going to do a deterministic comparison of the AROME-Arctic model and member 0 of the MEPS and IFSENS ensembles. We need to read the ensemble and deterministic forecasts in separately due to the different `fcst_type` argument. When there are multiple forecast models, we get a `harp_list`, which works in the same way as a standard list in R. This means that when we read for the second time we can put the output in a named element of the `harp_list`

```{r read-many-fcst, message=FALSE, warning=FALSE}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083100, "1d"),
  fcst_model = c("meps", "ifsens"),
  fcst_type  = "eps",
  parameter  = "T2m", 
  members    = 0, 
  file_path  = fcst_dir 
) |> 
  as_det()

fcst$arome_arctic <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083100, "1d"),
  fcst_model = "arome_arctic",
  fcst_type  = "det",
  parameter  = "T2m", 
  file_path  = fcst_dir 
)

fcst
```

When we have multiple forecast models we should ensure that we are comparing like with like. We therefore only want to verify the cases that are common to all of the forecast models. We can select these cases using `common_cases()`. Note the number of rows in each of the data frames before selecting the common cases above and after selecting the common cases below.

```{r common-cases}
fcst <- common_cases(fcst)

fcst
```

The rest of the verification workflow is exactly the same.

```{r common-cases-read-obs-and-join, message=FALSE}
obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
)

fcst <- join_to_fcst(fcst, obs)
```

```{r common-cases-verify, message=FALSE}
verif <- det_verify(fcst, T2m , thresholds = seq(280, 290, 2.5))

save_point_verif(verif, here("data", "verification", "det"))
```

When plotting the scores, each of the forecast models is automatically assigned a different colour.

```{r common-cases-plt, fig.align='center'}
plot_point_verif(verif, stde)
```

We could however plot each forecast model in a separate panel and assign, for example, threshold to control the colour.

```{r common-cases-fct-model, warning=FALSE, fig.align='center'}
plot_point_verif(
  verif, 
  frequency_bias, 
  colour_by = threshold,
  facet_by  = vars(fcst_model)
)
```

## Observation errors
### Gross error check

When reading observations, `read_point_obs()` will do a gross error check on the observations to make sure that they have realistic values. You can set the bounds with the `min_obs_allowed` and `max_obs_allowed` arguments. For some parameters, there are default values for the minimum and maximum allowed. These can be seen in the parameter definitions with `get_param_def()`. For example, for 2m temperature we see that the minimum and maximum values are 223 and 333 Kelvin. 

```{r get-param-def-t2m}
get_param_def("t2m")
```

### Checking observations against forecasts

Another check that can be done is to compare the values of the observations with forecasts. This is done with the `check_obs_against_fcst()` function. By default the data are grouped by each station ID and time of day, where the day is split into 4 parts of the day [00:00, 06:00), [06, 12:00), [12:00, 18:00) and [18:00, 00:00) and the standard deviation of the forecast for each of these is computed. The observations are then compared with the forecasts, and if the difference between the, is larger than a certain number of standard deviations then that row is removed from the data. 

Using the default value of 6 standard deviations we see that no observations are removed.

```{r check-obs-agnst-fcst}
fcst <- check_obs_against_fcst(fcst, T2m)
```

If we make the check more strict to remove all cases where the observations are more than 1 standard deviation away from the forecast, we see that now 723 cases are removed. We can see the removed cases in the `"removed_cases"` attribute of the result. 

```{r check-obs-agnst-fcst-1sd}
fcst <- check_obs_against_fcst(fcst, T2m, num_sd_allowed = 1)

attr(fcst, "removed_cases")
```

### Ensemble rescaling

When verifying ensembles, we can take the observation error into account by attempting to rescale the distribution of the ensemble forecast to that of the observations. This is done by adding an assumed error distribution to the ensemble forecast by sampling from the error distribution and adding each random draw to an ensemble member. If the error distribution has a mean of 0, the impact will be negligible on most scores. However, for the rank histogram and ensemble spread, this will in effect take the observation errors into account. In __harp__ we call this jittering the forecast and is done by using the `jitter_fcst()` function. 

Take for example the rank histograms for our MEPS and IFSENS ensembles for 2m temperature.  

```{r rank-hist, message=FALSE, warning=FALSE, fig.align='center'}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083112, "12h"), 
  fcst_model = c("meps", "ifsens"),
  fcst_type  = "eps", 
  parameter  = "T2m",
  file_path  = fcst_dir 
) |> 
  scale_param(-273.15, "degC") |> 
  common_cases()

obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
) |> 
  scale_param(-273.15, "degC", col = T2m)

fcst <- join_to_fcst(fcst, obs)

verif <- ens_verify(fcst, T2m)

plot_point_verif(
  verif, 
  normalized_rank_histogram, 
  rank_is_relative = TRUE
)
```

```{r spread-skill, warning=FALSE, fig.align='center'}
plot_point_verif(
  verif, 
  spread_skill
)
```

We see that both models are underdispersed with a U-shaped rank histogram. If we say that the observations have a noraml error distribution with a mean of 0 and standard deviation of 1 we can jitter the ensemble forecast by adding random draws from that distribution. 

```{r jitter-fcst-rh, message=FALSE, warning=FALSE, fig.align='center'}
fcst <- jitter_fcst(
  fcst, 
  function(x) x + rnorm(length(x), mean = 0, sd = 1)
)

verif <- ens_verify(fcst, T2m)

plot_point_verif(
  verif, 
  normalized_rank_histogram, 
  rank_is_relative = TRUE
)
```

```{r jitter-fcst-ss, warning=FALSE, fig.align='center'}
plot_point_verif(
  verif, 
  spread_skill
)
```

Now we see that the forecast is much less underdispersed with many of the ranks have a normalized frequency close to 1 and the ensemble spread much closer to the ensemble RMSE. 

::: {.callout-warning}
## Warning: Error Distributions
__harp__ does not include any estimates for error distributions. It is the user's responsibility to provide those error distributions. In this example, a normal distribution with a mean of 0 and standard deviation of 1 is for illustrative purposes only.  
:::

## Grouped Verification and Scaling

### Basic Grouping

So far we have just done verification stratified by `lead_time`. We can use the `groupings` argument to tell the verification function how to group the data together to compute scores. The most common grouping after lead time would be to group by the forecast cycle. Rather than read in only the forecasts initialized at 00:00 UTC, we will read in the 12:00 UTC forecasts as well.

```{r simple-grp-read-fcst, message=FALSE, warning=FALSE}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083112, "12h"),
  fcst_model = c("meps", "ifsens"),
  fcst_type  = "eps",
  parameter  = "T2m", 
  members    = 0, 
  file_path  = fcst_dir 
) |> 
  as_det()

fcst$arome_arctic <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083112, "12h"),
  fcst_model = "arome_arctic",
  fcst_type  = "det",
  parameter  = "T2m", 
  file_path  = fcst_dir 
)

fcst <- common_cases(fcst)
```

The forecasts are in Kelvin, but it may be more useful to have them in Â°C. We can scale the data using `scale_param()`. At a minimum we need to give the function the scaling to apply and a new name for the units. By default the scaling is additive, but it can be made multiplicative by setting `mult = TRUE`.

```{r scale-fcst}
fcst <- scale_param(fcst, -273.15, new_units = "degC")
```

Now we can read the observations and join to the forecast.

```{r simple-grp-read-obs, message=FALSE}
obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
)
```

```{r simple-grp-join-fail, error=TRUE}
fcst <- join_to_fcst(fcst, obs)
```

Here the join fails because the forecasts and observations do not have the same units. We therefore also need to scale the observations with `scale_param()`. When scaling observations, the name of the observations column also needs to be provided.

```{r simple-grp-scale-obs-join, message=FALSE}
obs <- scale_param(obs, -273.15, new_units = "degC", col = T2m)

fcst <- join_to_fcst(fcst, obs)
```

Now we can verify. This time we will tell `det_verify()` that we want scores for each lead time and forecast cycle.

```{r simple-grp-verif, warning=FALSE, message=FALSE, fig.align='center'}
verif <- det_verify(
  fcst, 
  T2m, 
  thresholds = seq(10, 20, 2.5),
  groupings  = c("lead_time", "fcst_cycle")  
)

plot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))
```

We now have plots for each forecast cycle, but what if we also want the combined forecast cycles as well? There are two ways we could tackle that - firstly we could compute the verification with `groupings = "lead_time"` (i.e. the default) and bind the output to what we already have with `bind_point_verif()`.

```{r simple-grp-bind, message=FALSE, warning=FALSE, fig.align='center'}
verif <- bind_point_verif(
  verif, 
  det_verify(fcst, T2m, thresholds = seq(10, 20, 2.5))
)

plot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))
```

### Grouping Lists

An easier way would be to pass a list to `groupings`. Each element in the list is treated as a separate verification and then they are bound together at the end.

```{r simple-grp-list, message=FALSE, warning=FALSE, fig.align='center'}
verif <- det_verify(
  fcst, 
  T2m, 
  thresholds = seq(10, 20, 2.5),
  groupings  = list(
    "lead_time",
    c("lead_time", "fcst_cycle")  
  )
)

save_point_verif(verif, here("data", "verification", "det", "fcst-cycle"))

plot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))
```

::: callout-tip
## **Tip**: Controlling the order of facets

Facets are plotted in alphabetical order if the faceting variable is a string or coerced into a string. This means that the facets can be in an unexpected order. The `fct_*()` functions from the *forcats* package can be used to help reorder the facets. Quite often the order in which the values of the variable appear in the data is the order you want the facets to be in so `fct_inorder()` can be used to reorder the facets.
:::

```{r simple-grp-fct-inorder, warning=FALSE, fig.align='center'}
plot_point_verif(verif, rmse, facet_by = vars(fct_inorder(fcst_cycle)))
```

### Complex Grouping Lists

#### Adding a station characteristic

It is often desirable to group stations together by common characteristics, whether that be location, whether it's on the coast or in the mountains or any other characteristic. **harp** includes a built in data frame of station groups that can be joined to the forecast and used to group the verification. We can join the station groups using `join_to_fcst()` with `force = TRUE` since we don't want a check on common units between the data frames to be joined together.

```{r cmplx-grp-join, message=FALSE, warning=FALSE}
fcst <- join_to_fcst(fcst, station_groups, force = TRUE)
```

This has added a `"station_group"` column to the forecast which we can use in the `groupings` argument. Now we want verification for each lead time for all forecast cycles and station groups; for each lead time and each forecast cycle for all station groups; for each lead time and each station group for all forecast cycles; and for each lead time for each station group for each forecast cycle. Therefore we need a list of 4 different groupings.

```{r cmplx-grp-verif, message=FALSE, warning=FALSE}
verif <- det_verify(
  fcst, 
  T2m, 
  thresholds = seq(10, 20, 2.5),
  groupings  = list(
    "lead_time",
    c("lead_time", "fcst_cycle"),
    c("lead_time", "station_group"),
    c("lead_time", "fcst_cycle", "station_group")
  ) 
)

save_point_verif(verif, here("data", "verification", "det", "stations"))
```

Plotting now becomes quite complicated as you have to do a lot of filtering and faceting. For example:

```{r cmplx-grp-plt, message=FALSE, warning=FALSE, fig.align='center', fig.height=12}
plot_point_verif(
  verif, 
  equitable_threat_score, 
  facet_by  = vars(station_group),
  filter_by = vars(grepl(";", fcst_cycle), threshold == 15) 
)
```

You may have noticed the saving of each verification result into specific directories. This is so that they can be plotted using an interactive app that runs in a web browser. The app is made using *R Shiny* so we refer to it as a Shiny app. `save_point_verif()` uses very specific file names that describe some of the information about the verification object, but the grouping strategy is not one of those pieces of information, hence the separate directories. The Shiny app will create dropdown menus allowing you to choose the group for which you want to see the scores.

You can start the shiny app with:

```{r shiny-app, eval=FALSE}
shiny_plot_point_verif(
  start_dir           = here("data", "verification"),
  full_dir_navigation = FALSE, 
  theme               = "light"
) 
```

::: {.callout-tip collapse="true"}
## **Tip**: Shiny App Options

When you start the shiny app it is best to give it a directory to start from to aid navigation. By setting `full_dir_navigation = FALSE` the use of modal windows to navigate your directory system is disabled, and all directories below the start directory are searched for **harp** point verification files and the *Select Verfication Directory* dropdown is populated - this is experimental and may not give the smoothest ride, but is often less cumbersome than navigation by modal windows. Finally you can choose between "light", "dark" and "white" for the colour theme of the app (the default is "dark").
:::

### Changing the Time Axis

You can also use the `groupings` argument to specify different time axes to use for the verification. So far we have used the lead time for the time axis. However we could also use the time of day to get the diurnal cycle, or simply get the scores for each date-time in the data set. In this example we will still group by the forecast cycle as well. To get the time of day, we first need to run `expand_date()` on the data to get a column for `"valid_hour"`.

```{r time-axis-verif, message=FALSE, warning=FALSE}
fcst <- expand_date(fcst, valid_dttm)

verif <- det_verify(
  fcst, 
  T2m, 
  thresholds = seq(10, 20, 2.5),
  groupings  = list(
    "lead_time",
    c("lead_time", "fcst_cycle"),
    "valid_hour",
    c("valid_hour", "fcst_cycle"),
    "valid_dttm",
    c("valid_dttm", "fcst_cycle")
  ) 
)
```

::: {.callout-note collapse="true"}
## **Note**: A necessary hack

Since the data are 6 hourly there are only 4 valid hours in the data set. When there are fewer than five different values in group the verification functions separate them with a ";" in the group value when they are all together, otherwise they are labelled as "All". `plot_point_verif()` only searches for "All" when figuring out which data to get for each different x-axis.
:::

We need to use `mutate_list()` in conjunction with `case_when()` from the *dplyr* package to modify the `"valid_hour"` column where all valid hours are collected together. `mutate_list()` use `mutate()` from the *dplyr* package to modify columns of data frames in a list whilst retaining the attributes of the list.

```{r hack-forward-slash-bodge, message=FALSE}
verif <- mutate_list(
  verif, 
  valid_hour = case_when(
    grepl(";", valid_hour) ~ "All",
    .default = valid_hour
  )
)

save_point_verif(verif, here("data", "verification", "det", "fcst-cycle"))
```

We can now use the `x-axis` argument to `plot_point_verif()` to decide which times to use on the x-axis.

```{r plt-x-axis, warning=FALSE, fig.align='center'}
plot_point_verif(verif, mae, facet_by = vars(fct_inorder(fcst_cycle)))

plot_point_verif(
  verif, 
  mae, 
  x_axis   = valid_hour, 
  facet_by = vars(fct_inorder(fcst_cycle))
)

plot_point_verif(
  verif, 
  mae, 
  x_axis     = valid_dttm, 
  facet_by   = vars(fct_inorder(fcst_cycle)),
  point_size = 0 
)

```

## Vertical Profiles

Another application where grouping is important in the verification of vertical profiles. In general the workflow is once again the same, but there are some aspects where you need to take into account that the data are on vertical levels.

The first difference is that when reading in the data, you need to tell the read function what vertical coordinate the data are on via the `vertical_coordinate` argument. In most cases the data will be on pressure levels, but they could also be on height levels or model levels.

The data we are using here come from the AROME-Arctic model and the control member of MEPS, which at MET Norway is archived as MEPS_det (i.e. MEPS deterministic). There are forecasts available every 6 hours at 00-, 06-, 12- and 18-UTC.

```{r vrt-prf-read-fcst, message=FALSE, warning=FALSE}
fcst <- read_point_forecast(
  dttm                = seq_dttm(2023080100, 2023083118, "6h"),
  fcst_model          = c("meps", "arome_arctic"),
  fcst_type           = "det",
  parameter           = "T",
  file_path           = fcst_dir,
  vertical_coordinate = "pressure"
) |> 
  scale_param(-273.15, "degC")
```

When finding the common cases, the default behaviour is to compare the `"fcst_dttm"`, `"lead_time"` and `"SID"` columns. When finding common cases for vertical profiles we also need to make sure that only the vertical levels that are common to all forecast models are included in the verification. We do this by adding the pressure column (`p`) to `common_cases()`.

```{r vrt-prf-common-cases}
fcst <- common_cases(fcst, p)
```

Similar to reading the forecasts, we also need to tell `read_point_obs` that the vertical coordinate is pressure.

```{r vrt-prf-read-obs, message=FALSE}
obs <- read_point_obs(
  dttm                = unique_valid_dttm(fcst),
  parameter           = "T", 
  stations            = unique_stations(fcst),
  obs_path            = obs_dir,
  vertical_coordinate = "pressure"
) |> 
  scale_param(-273.15, "degC", col = T)
```

Joining works exactly the same as for single level variables.

```{r vrt-prf-join, message=FALSE}
fcst <- join_to_fcst(fcst, obs)
```

Now we can verify, making sure that we have `"p"` as one of the grouping variables.

```{r vrt-prf-verif, message=FALSE}
verif <- det_verify(
  fcst, 
  T, 
  groupings = list(
    c("lead_time", "p"),
    c("lead_time", "p", "fcst_cycle")
  )
)

save_point_verif(
  verif, 
  here("data", "verification", "det", "fcst-cycle")
)
```

We can now plot the profile verification using `plot_profile_verif()` making sure to filter and facet appropriately (for example, there is one profile for each lead time).

```{r vrt-prf-plt, warning=FALSE, fig.align='center', fig.height=10}
plot_profile_verif(
  verif, 
  mae, 
  filter_by = vars(grepl(";", fcst_cycle)),
  facet_by  = vars(lead_time)
)
```

We could also make a plot for a single vertical level in the normal way. We may also want to remove times when there are very few cases.

```{r vrt-prf-one-lvl-plt, warning=FALSE, fig.align='center'}
plot_point_verif(
  verif, 
  bias, 
  filter_by = vars(p == 925, num_cases > 5),
  facet_by  = vars(fct_inorder(fcst_cycle)) 
)
```

## Conditional Verification
### Classification by observed value

On occasion, it may be instructive to verify for particular conditions. For example, to verify temperature for different temperature ranges. This can be done by creating a grouping column for each range of observed temperature. Here the workflow would be to use `mutate()` from the _dplyr_ package in association with the base R function `cut()` to create a grouping column on the observations before joining to the forecast.

```{r cond-temp-read-fcst-obs, message=FALSE, warning=FALSE}
fcst <- read_point_forecast(
  dttm       = seq_dttm(2023080100, 2023083100, "24h"),
  fcst_model = c("meps", "ifsens"),
  fcst_type  = "eps",
  parameter  = "T2m",
  file_path  = fcst_dir
) |> 
  scale_param(-273.15, "degC") |> 
  common_cases()

obs <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "T2m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
) |> 
  scale_param(-273.15, "degC", col = T2m)
```

We are going to classify the temperature by having the left side of each range open and right side closed. This basically means that each range goes from >= the lower value to < the upper value. We do this by setting `right = FALSE` in the call to `cut()`.

```{r cond-temp-cut-obs}
obs <- mutate(
  obs, 
  temp_range = cut(T2m, seq(5, 25, 2.5), right = FALSE)
)

obs
```

You will see that `cut()` labels values outside of the breaks as `NA`. This isn't necessarily meaningful, so we will use the _dplyr_ function `case_when()` to give more meaningful labels. We will also take the opportunity to format the other ranges a bit better.

```{r cond-temp-relabel}
obs <- mutate(
  obs, 
  temp_range = case_when(
    T2m <  5  ~ "< 5",
    T2m >= 25 ~ ">= 25",
    .default = gsub(",", ", ", temp_range)
  ), 
  temp_range = fct_relevel(
    temp_range, 
    c(
      "< 5", 
      gsub(
        ",", 
        ", ", 
        levels(cut(seq(5, 25), seq(5, 25, 2.5), right = FALSE))
      ), 
      ">= 25"
    )
  )
)
```

In the above we also set the factor levels to an order that makes sense for when we come to plot the scores. We can now continue as normal, adding "`temp_range"` as a grouping variable. 

```{r cond-temp-verif, message=FALSE, warning=FALSE, fig.align='center'}
fcst <- join_to_fcst(fcst, obs)

verif <- ens_verify(
  fcst, 
  T2m, 
  groupings = c("leadtime", "temp_range")
)

plot_point_verif(
  verif, 
  spread_skill, 
  facet_by = vars(temp_range)  
)
```


### Classification by a different parameter

One important aspect of forecast model performance is how it performs in different weather regimes. For example, it may be useful to know how the model performs for a particular parameter when the wind is coming from a certain direction. There are a couple of approaches to this, but it must be noted that it is important to include the possibility for false alarms in the verification. 

Here we will compute the verification scores for an ensemble forecast for 2m temperature when the wind is coming from the east, so let's say a wind direction between 45&deg; and 135&deg;. We will first read in the wind direction observations for the cases we already have for 2m temperature. 

```{r cond-wind-read-wind-obs, message=FALSE, warning=FALSE}
wind_dir <- read_point_obs(
  dttm      = unique_valid_dttm(fcst),
  parameter = "D10m",
  stations  = unique_stations(fcst),
  obs_path  = obs_dir 
)
```

We will now create a column to have the groups "westerly" and "other" based on the values in the `"D10m"` column. 

```{r cond-wind-is-west}
wind_dir <- mutate(
  wind_dir, 
  wind_direction = case_when(
    between(D10m, 45, 135) ~ "easterly",
    .default = "other"
  )
)
```

We can now join the wind direction data to the 2m temperature data that we want to verify. Here we have to set `force = TRUE` in `join_to_fcst()` since it will fail the check for the forecasts and observations having the same units. 

```{r cond-wind-join, message=FALSE, warning=FALSE}
fcst <- join_to_fcst(fcst, wind_dir, force = TRUE)
```

We can now verify 2m temperature and group by the wind direction. 

```{r cond-wind-verif, message=FALSE, warning=FALSE, fig.align='center'}
verif <- ens_verify(
  fcst, 
  T2m, 
  groupings = c("lead_time", "wind_direction")
)

plot_point_verif(
  verif, 
  mean_bias, 
  facet_by = vars(wind_direction)
)
```

A more thorough method might be to include whether easterly winds were forecast or not in the analysis, such that we have cases of westerly being observed only, forecasted only, both observed and forecasted, and neither observed nor forecasted. 

So now we need read in the wind direction forecasts as well. In this case we need to set the units to "degrees" as they are incorrect in the file (due to a bug :/ ). 

```{r cond-wind-read-fcst-wind, message=FALSE, warning=FALSE}
fcst_wind_dir <- read_point_forecast(
  dttm       = unique_fcst_dttm(fcst),
  fcst_model = c("meps", "ifsens"), 
  fcst_type  = "eps",
  parameter  = "wd10m",
  lead_time  = unique_col(fcst, lead_time), 
  stations   = unique_stations(fcst),
  file_path  = fcst_dir 
) |> 
  set_units("degrees")
```

Since the forecasts are from ensemble, we could identify forecasts that have westerly winds as those where at least one member has a wind direction between 45&deg; and and 135&deg; To identify these cases we can find the binary probability for each member and then compute the ensemble mean with `ens_stats()`. We can apply a function to multiple columns using `across()` from the _dplyr_ package together with `mutate()`. Since we are generating logical values, we need to tell `ens_stats()` not to compute the standard deviation. 

```{r cond-wind-members-westerly}
fcst_wind_dir <- mutate(
  fcst_wind_dir,
  across(contains("_mbr"), ~between(.x, 45, 135))
) |> 
  ens_stats(sd = FALSE)
```

Now we can join the wind direction observations, generate our groups and select only those columns we need. 

```{r cond-wind-join-wind-and-grp, message=FALSE, warning=FALSE}
fcst_wind_dir <- join_to_fcst(fcst_wind_dir, wind_dir)

fcst_wind_dir <- mutate(
  fcst_wind_dir, 
  easterly = case_when(
    ens_mean  > 0 & wind_direction == "easterly" ~ "both",
    ens_mean  > 0 & wind_direction == "other" ~ "forecast only",
    ens_mean == 0 & wind_direction == "easterly" ~ "observed only",
    ens_mean == 0 & wind_direction == "other" ~ "neither", 
    .default = NA
  )
) |> 
  select(fcst_dttm, lead_time, SID, easterly)
```

We can now join our data frame with the verification groups to the 2m temperature forecasts to be verified. Since we have two models, we can merge them into a single data frame using `bind()`. 

```{r cond-wind-join-wind-grp-to-temp, message=FALSE, warning=FALSE}
fcst <- join_to_fcst(
  fcst, 
  bind(fcst_wind_dir), 
  force = TRUE
)
```

Now we can verify and plot for our new groups. `fct_relevel()` is used to get the plot facets in the desired order. 

```{r cond-wind-fcst-grp-verif-plt, message=FALSE, warning=FALSE, fig.align='center'}
verif <- ens_verify(
  fcst, 
  T2m,
  groupings = c("lead_time", "easterly")
)

plot_point_verif(
  verif, 
  mean_bias, 
  facet_by       = vars(fct_relevel(easterly, ~.x[c(2, 4, 1, 3)])),
  num_facet_cols = 2
)
```

In the next tutorial will put everything you have learned here into practice by building a script that could be used for repeated verification tasks. 

::: grid
::: g-col-1
<a href=read-forecast.html><i class="bi bi-arrow-left-circle-fill"></i></a>
:::

::: g-col-10
:::

::: g-col-1
<a href=build-verif-script.html><i class="bi bi-arrow-right-circle-fill"></i></a>
:::
:::
