[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Getting Started\n\n\nInstallation and setup\n\n\n\n\n\n\n\n\n\nReading data\n\n\nFrom basic reading of files to complex lagged systems\n\n\n\n\n\n\n\n\n\nPoint Verification\n\n\nFrom basic verification to multiple verification groups and conditions\n\n\n\n\n\n\n\n\n\nBuild a Script\n\n\nWriting scripts for verification tasks\n\n\n\n\n\n\n\n\n\nSpatial Verifcation\n\n\nHow to run spatial verification\n\n\n\n\n\n\n\n\n\nSpatial Data\n\n\nExplore, plot and manipulate spatial data\n\n\n\n\n\n\n\n\n\nExternal Data Formats\n\n\nWrite functions to read other data formats\n\n\n\n\n\n\n\n\n\nContributing\n\n\nGit, Github, harphub, Pull Requests\n\n\n\n\n\n\n\n\n\nharp’s Future\n\n\nInteroperability with Python"
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "Each day will be split into a morning and afternoon session, with a tea / coffee break in the middle of each session.\nFor the most part, instruction will be “code-along” sessions with the instructor with an opportunity to go through some of your own work and / or problems in the second half of the week."
  },
  {
    "objectID": "agenda.html#day-1-monday-4-march-2024",
    "href": "agenda.html#day-1-monday-4-march-2024",
    "title": "Agenda",
    "section": "Day 1: Monday 4 March 2024",
    "text": "Day 1: Monday 4 March 2024\n\n13:30 - 17:00\n\nIntroduction and Getting started\nWe will give a brief introduction to harp and R and\n\n\nBasic reading of data\nAndrew will introduce the read_forecast() function and go through reading data in Grib, NetCDF and vfld formats. We will look at how to deal with complex lagged forecasting systems and how to transform gridded data to geographic point locations"
  },
  {
    "objectID": "agenda.html#day-2-tuesday-5-march-2024",
    "href": "agenda.html#day-2-tuesday-5-march-2024",
    "title": "Agenda",
    "section": "Day 2: Tuesday 5 March 2024",
    "text": "Day 2: Tuesday 5 March 2024\n\n09:00 - 12:30\n\nPoint Verification Workflow\nAndrew will go through the workflow for verifying point forecasts. From basic preparation of the observations, through to reading the forecasts and computing and plotting the verification scores. The complexity will be increased by grouping the data with the groupings argument and adding conditions to the verification\n\n\n\n13:30 - 17:00\n\nBuilding a Verification Script\nAndrew will build on the morning’s work by converting our verification workflow into a script that could be run in a production environment. We will cover how to loop over different forecast parameters, how to deal with harp’s non-standard evaluation by embracing variables with {{ }} and then discuss some real uses of harp point verification in an operational environment."
  },
  {
    "objectID": "agenda.html#day-3-wednesday-march-2024",
    "href": "agenda.html#day-3-wednesday-march-2024",
    "title": "Agenda",
    "section": "Day 3: Wednesday March 2024",
    "text": "Day 3: Wednesday March 2024\n\n09:00 - 12:30\n\nSpatial Verification Workflow\nAlex will take you through the workflow required to to do spatial verification for scores including the Fractions Skill Score (FSS) and SAL (Structure Amplitude and Location). The new HIRA score will also be introduced and sources of spatial data will be discussed.\nAndrew will briefly go through the workflow for harp’s new ensemble fractions skill score implementation and its distribution and error components.\n\n\n\n13:30 - 17:00\n\nPlotting and Manipulating Spatial Data\nAndrew will take you through how harp data can work with (probably R’s most popular package) ggplot2. We will have the opportunity to experiment with the harp ggplot geoms geom_georaster(), geom_geocontour() and geom_geocontour_filled() and different methods to map data to colours. In addition, we will go through harp’s geographical transformation functions, how to define your own georeferenced grids and how to deal with cross-sections."
  },
  {
    "objectID": "agenda.html#day-4-thursay-7-march-2024",
    "href": "agenda.html#day-4-thursay-7-march-2024",
    "title": "Agenda",
    "section": "Day 4: Thursay 7 March 2024",
    "text": "Day 4: Thursay 7 March 2024\n\n09:00 - 12:30\n\nBuilding a Function to Read External Observations\nThis session will be a case study of how to use harp’s function recognition system to use read_obs() to read observations from an external source. We will use MET Norway’s Frost database, which can be queried using a REST API.\nIn addition, there will be time to discuss any issues that you are having with harp\n\n\n\n13:30 - 17:00\n\nContributing to harp\nThis session will be a case study of how to contribute to harp. We will fork a harp repository, create a feature branch, modify a function, test it, merge with the develop branch and make a pull request. The plan is to modify harpPoint’s functionality for computing probabilities by adding a comparator argument, such that categorical scores can be computed for cases of less than, greater than, equal to or between thresholds.\nIn addition, there will be time to discuss any issues that you are having with harp"
  },
  {
    "objectID": "agenda.html#day-5-8-march-2024",
    "href": "agenda.html#day-5-8-march-2024",
    "title": "Agenda",
    "section": "Day 5: 8 March 2024",
    "text": "Day 5: 8 March 2024\n\n09:00 - 12:30\n\nInteroperability of harp with Python\nJuanje will introduce some work done with Python to do Langrangian spatial verification of Python. We will discuss the prospects for integrating Python functionalities into harp\nIn addition, Andrew will talk about some ideas for future directions for harp, such as functions to return harp verification functions.\nThis will also be a final opportunity to get help with specific issues with harp and R."
  },
  {
    "objectID": "get-started.html",
    "href": "get-started.html",
    "title": "Getting started",
    "section": "",
    "text": "harp is a set of R packages. You will therefore need R to be installed.\nWe will be running the course using RStudio as our IDE as it provides many useful features for working in R.\nIf you are unable to install R or RStudio, another solution may be to work with RStudio on Posit Cloud. However, it should be noted that the free tier limits RAM to 1 GB, which may not be sufficient to follow all of the training course.\nYou will also need the following system libraries\n\nlibproj-dev\nlibeccodes-dev\nlibnetcdf-dev\n\n\n\n\n\n\n\nMore information about these system libraries\n\n\n\n\n\n\nlibproj-dev is essential to install harp. It provides the functionality for doing geographic transformations.\nlibeccodes-dev is ECMWF’s eccodes library. It powers the Rgrib2 package that enables reading of GRIB files.\nlibnetcdf-dev is used to enable reading of NetCDF files.\n\nThe system libraries for reading GRIB and NetCDF files are not essential for installing harp as these features are optional."
  },
  {
    "objectID": "get-started.html#prerequisites",
    "href": "get-started.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "harp is a set of R packages. You will therefore need R to be installed.\nWe will be running the course using RStudio as our IDE as it provides many useful features for working in R.\nIf you are unable to install R or RStudio, another solution may be to work with RStudio on Posit Cloud. However, it should be noted that the free tier limits RAM to 1 GB, which may not be sufficient to follow all of the training course.\nYou will also need the following system libraries\n\nlibproj-dev\nlibeccodes-dev\nlibnetcdf-dev\n\n\n\n\n\n\n\nMore information about these system libraries\n\n\n\n\n\n\nlibproj-dev is essential to install harp. It provides the functionality for doing geographic transformations.\nlibeccodes-dev is ECMWF’s eccodes library. It powers the Rgrib2 package that enables reading of GRIB files.\nlibnetcdf-dev is used to enable reading of NetCDF files.\n\nThe system libraries for reading GRIB and NetCDF files are not essential for installing harp as these features are optional."
  },
  {
    "objectID": "get-started.html#installation",
    "href": "get-started.html#installation",
    "title": "Getting started",
    "section": "Installation",
    "text": "Installation\nThe harp packages are stored on Github under the harphub area. This means that to install harp you will need the remotes package. remotes is an official R package. All official R packages can be installed from the official repository of R packages, CRAN, using the install.packages() function.\n\ninstall.packages(\"remotes\")\n\nWhen we want to use functions from a package, we attach that package using library()\n\nlibrary(remotes)\n\nWe can now use the install_github() function to install harp.\n\ninstall_github(\"harphub/harp\")\n\nThis will likely take some time (possibly around 30 minutes) as harp needs to compile and install a large number of dependencies.\n\n\n\n\n\n\nTip: Get a Github PAT\n\n\n\n\n\nGithub can sometimes throttle downloads, which may cause harp to fail to install. One solution is to wait for an hour and continue the installation, but it makes things much easier if you have a Github PAT (Personal Access Token).\nTo get a Github PAT, you first need to register for an account at Github. Once you have an account, follow the instructions here to generate a personal access token. Make sure to copy your token to the clipboard and then paste it into the file $HOME/.Renviron like this:\nGITHUB_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nbut using your own Github PAT. Make sure that this file ends with a new line and restart R for the PAT to be recognised."
  },
  {
    "objectID": "get-started.html#setting-up-a-project",
    "href": "get-started.html#setting-up-a-project",
    "title": "Getting started",
    "section": "Setting up a project",
    "text": "Setting up a project\nWe are going to work in a clean directory that will be used as a root directory for all of the work we do during this course.\nIn RStudio:\n\nClick File &gt; New Project\nClick on New Directory and then on New Project\nGive the project a name under Directory name (e.g. harp-training-2024)\nChoose where the directory you want your project to be under\nClick on Create Project\n\nThe first thing we will do is install the here package, which will enable us to refer to all directories in the project relative to its top level directory.\n\ninstall.packages(\"here\")\nlibrary(here)\n\n\n\n\n\n\n\nClick for setting up a project outside of RStudio\n\n\n\n\n\nIn R create your project directory and navigate to it, e.g.\n\ndir.create(\"/path/to/my/project/harp-training-2024\")\nsetwd(\"/path/to/my/project/harp-training-2024\")\n\nNote that you may need to set recursive = TRUE in dir.create() if there is more than the last directory in the tree doesn’t exist.\nNext install the here package and set the project root as the current directory.\n\ninstall.packages(\"here\")\nlibrary(here)\nset_here()"
  },
  {
    "objectID": "get-started.html#data",
    "href": "get-started.html#data",
    "title": "Getting started",
    "section": "Data",
    "text": "Data\nWe are now almost ready to start practising with harp. We just need some data to work with. First let’s create a directory to keep our data.\n\ndir.create(here(\"data\"))\n\nThe data we are going to use can be downloaded from here Copy the data into your new data directory and unpack it using\n\nsystem(\"tar -zxvf harpTrainnigData2024.tar.gz\")\n\nThere are also extra datasets for Wednesday’s sessions. These can be downloaded from here and should be unpacked in your data directory."
  },
  {
    "objectID": "read-forecast.html",
    "href": "read-forecast.html",
    "title": "Reading forecast data",
    "section": "",
    "text": "In this section you will learn how to use read_grid(), read_forecast() and read_analysis() to read gridded data from files of various formats.\nBefore we begin, make sure that you are in your project directory for the training course and attach the packages that we are going to need.\nlibrary(harp)\nlibrary(here)\nYou should have copied the data for the course into your data directory. If not, follow the instructions here."
  },
  {
    "objectID": "read-forecast.html#read_grid",
    "href": "read-forecast.html#read_grid",
    "title": "Reading forecast data",
    "section": "read_grid()",
    "text": "read_grid()\n\nBasic Usage\nread_grid() is used to read data from a single file. You give it the file name and the parameter you want to read as well as other options to describe the data you want to read and the output.\nLet’s begin by reading 2m temperature from a grib file.\n\nread_grid(here(\"data/grib/exp1/mbr001/fc2022071012+000grib2_fp\"), \"t2m\")\n\neidb : Temperature  \nTime:\n 2022/07/10 12:00 +0\nDomain summary:\n161 x 211 domain\nProjection summary:\nproj= lcc \nNE = ( 13.73206 , 57.99135 )\nSW = ( 7.971006 , 53.08321 )\nData summary:\n286.6064 289.4569 290.5917 290.6924 292.0204 300.3515 \n\n\nThe output of read_grid() is an object with a class of geofield. A geofield is a 2d array with information about the grid dimensions and its co-ordinate reference system. More about this later. When printing a geofield to the screen, some information about the domain is shown as well as a summary of the data. The values are the minimum, the first quartile, the median, the mean, the third quartile and the maximum of the data in the geofield.\n\n\nParameter names\nThe second argument to read_grid() is the name of the parameter to read. Here we use \"t2m\", which is the parameter name that harp uses for 2m temperature. You can see all of the parameter names used by harp with show_param_defs()\n\nshow_param_defs()\n\n# A tibble: 61 × 2\n   name          description                                                    \n   &lt;chr&gt;         &lt;chr&gt;                                                          \n 1 accpcp12h     12-hour accumulated precipitation                              \n 2 accpcp1h      1-hour accumulated precipitation                               \n 3 accpcp24h     24-hour accumulated precipitation                              \n 4 accpcp3h      3-hour accumulated precipitation                               \n 5 accpcp6h      6-hour accumulated precipitation                               \n 6 caf           Cloud area fraction at vertical levels                         \n 7 cape          Convective available potential energy                          \n 8 cbase         Height of cloud base                                           \n 9 cc_below_7500 Cloud cover below 7500m                                        \n10 cchigh        High level cloud cover                                         \n11 cclow         Low level cloud cover                                          \n12 ccmed         Medium level cloud cover                                       \n13 cctot         Total integrated cloud cover                                   \n14 cin           Convective inhibition                                          \n15 d             Wind direction                                                 \n16 d10m          Wind direction at 10m above the ground                         \n17 g             Wind gust                                                      \n18 g10m          Wind gust at 10m above the ground                              \n19 gh            Geopotential height                                            \n20 gmax          Maximum wind gust at 10m above the ground                      \n21 lsm           land-sea mask                                                  \n22 pcp           Accumulated precipitaion                                       \n23 pmsl          Air pressure at mean sea level                                 \n24 pressure      Atmospheric air pressure                                       \n25 psfc          Surface air pressure                                           \n26 q             Specific humidity of air                                       \n27 q2m           Specific humidity of air at 2m above the ground                \n28 rh            Relative humidity of air                                       \n29 rh2m          Relative humidity of air at 2m above the ground                \n30 s             Wind speed                                                     \n31 s10m          Wind speed at 10m above the ground                             \n32 sea_ice       Sea ice concentration                                          \n33 sfc_geo       Surface geopotential                                           \n34 smax          Maximum wind speed at 10m above the ground                     \n35 snow          Snow depth                                                     \n36 sst           Sea surface temperature                                        \n37 t             Air temperature                                                \n38 t0m           Skin temperature                                               \n39 t2m           Air temperature at 2m above the ground                         \n40 td            Dew point temperature                                          \n41 td2m          Dew point temperature at 2m above the ground                   \n42 tmax          Maximum air temperature at 2m above the ground                 \n43 tmin          Minimum air temperature at 2m above the ground                 \n44 topo          Height of topography above sea level                           \n45 u             Wind speed in u direction                                      \n46 u10m          Wind speed in u direction at 10m above the ground              \n47 ugust         Wind gust in U direction                                       \n48 ugust10m      Wind gust at 10m above the ground in U direction               \n49 v             Wind speed in v direction                                      \n50 v10m          Wind speed in v direction at 10m above the ground              \n51 vgust         Wind gust in V direction                                       \n52 vgust10m      Wind gust at 10m above the ground in V direction               \n53 vis           Visibility in air                                              \n54 w             Vertical (upward) wind speed                                   \n55 wd            Wind direction calculated from U and V winds                   \n56 wd10m         Wind direction at 10m above the ground calculated from U and V…\n57 wg            Wind gust calculated from U and V gusts                        \n58 wg10m         Wind gust at 10m above the ground calculated from U gust and V…\n59 ws            Wind speed calculated from U and V winds                       \n60 ws10m         Wind speed at 10m above the ground calculated from U and V win…\n61 z             Geopotential                                                   \n\n\nYou can see how harp translates the parameter name for a particular file format with get_param_def()\n\nget_param_def(\"t2m\", \"grib\")\n\n$name\n[1] \"2t\" \"t\" \n\n$level_type\n[1] \"heightAboveGround\" \"surface\"          \n\n$level\n[1] 2\n\n\nYou can see that with parameter = \"t2m\" harp will get the grib message with a shortName of t or 2t with a levelType of heightAboveGround or surface.\n\n\nOther file formats\nharp has built in functionality to read Grib, NetCDF, FA and vfld, vobs and obsoul files. Note that although the latter three do not contain gridded data, they can still be read by read_grid().\n\nread_grid(\n  here(\"data/netcdf/meps_lagged/2024/02/15/07/mbr03/meps_sfc_24_20240215T07Z.nc\"),\n  \"pcp\"\n)\n\n : pcp kg/m^2 \nTime:\n 2024/02/16 07:00\nDomain summary:\n201 x 201 domain\nProjection summary:\nproj= lcc \nNE = ( 13.82773 , 62.55298 )\nSW = ( 5.582523 , 57.69936 )\nData summary:\n1.470703 9.265625 13.08203 15.30311 19.65625 63.47949 \n\n\n\n\n\n\n\n\nTip: vfld files need a lead_time argument\n\n\n\n\n\nvfld files do not include enough metadata to get the lead time from the contents of the files. Therefore you need to give read_grid() the lead time in order to fully populate the output. You wouldn’t normally read vfld files directly with read_grid(), but would use read_forecast() instead.\n\n\n\n\nread_grid(\n  here(\"data/vfld/MEPS_preop/vfldMEPS_preopmbr000202402190003\"), \n  \"T2m\", \n  lead_time = 3\n) \n\n# A tibble: 2,083 × 9\n     SID   lat   lon model_elevation parameter station_data members lead_time\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1001  70.9 -8.67             9   T2m               273.      NA         3\n 2  1010  69.3 16.1              4.7 T2m               275.      NA         3\n 3  1014  69.2 17.9             46.4 T2m               272.      NA         3\n 4  1015  69.6 17.8              0.6 T2m               274.      NA         3\n 5  1018  69.2 16.0            153.  T2m               273.      NA         3\n 6  1021  70.0 18.7             24.4 T2m               275.      NA         3\n 7  1022  70.0 18.7             24.4 T2m               275.      NA         3\n 8  1023  69.1 18.5             66.2 T2m               271.      NA         3\n 9  1025  69.7 18.9              3.9 T2m               272.      NA         3\n10  1026  69.7 18.9              1.3 T2m               272.      NA         3\n# ℹ 2,073 more rows\n# ℹ 1 more variable: units &lt;chr&gt;\n\n\nSince vfld files contain point data, the output is a data frame with columns for other metadata. The data are in the station_data column.\n\n\nMultiple geofields for one parameter\nFiles can contain multiple entries for a single parameter. For example a file could contain multiple lead times, multiple ensemble members, multiple vertical levels, or any combination of these. When many entries for a single parameter exist, a geolist is returned.\n\nread_grid(\n  here(\"data/netcdf/arome_arctic/2024/02/19/arome_arctic_2_5km_20240219T12Z.nc\"),\n  \"t2m\"\n)\n\n\n\n&lt;harp_geolist[13]&gt;\n [[1]] &lt;numeric geofield [200 x 200] Min = 257.195 Max = 277.968 Mean = 270.880&gt;\n [[2]] &lt;numeric geofield [200 x 200] Min = 257.907 Max = 278.076 Mean = 270.867&gt;\n [[3]] &lt;numeric geofield [200 x 200] Min = 257.632 Max = 278.233 Mean = 270.622&gt;\n [[4]] &lt;numeric geofield [200 x 200] Min = 256.557 Max = 278.322 Mean = 270.353&gt;\n [[5]] &lt;numeric geofield [200 x 200] Min = 255.851 Max = 278.176 Mean = 270.146&gt;\n [[6]] &lt;numeric geofield [200 x 200] Min = 254.634 Max = 278.187 Mean = 269.967&gt;\n [[7]] &lt;numeric geofield [200 x 200] Min = 253.723 Max = 278.262 Mean = 269.810&gt;\n [[8]] &lt;numeric geofield [200 x 200] Min = 253.297 Max = 278.318 Mean = 269.667&gt;\n [[9]] &lt;numeric geofield [200 x 200] Min = 252.770 Max = 278.224 Mean = 269.533&gt;\n[[10]] &lt;numeric geofield [200 x 200] Min = 252.329 Max = 277.918 Mean = 269.387&gt;\n# 3 more geofields\n# Use `print(n = ...)` to see more\n\n\nA geolist is simply a list of geofields that are on the same domain. To get more metadata about each geofield you can set data_frame = TRUE to get the output in the form of a data frame.\n\nread_grid(\n  here(\"data/netcdf/arome_arctic/2024/02/19/arome_arctic_2_5km_20240219T12Z.nc\"),\n  \"t2m\",\n  data_frame = TRUE\n) \n\n\n\n# A tibble: 13 × 8\n   fcst_dttm           valid_dttm          lead_time gridded_data units\n   &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt;    &lt;geolist&gt; &lt;chr&gt;\n 1 2024-02-19 12:00:00 2024-02-19 12:00:00         0  [200 × 200] K    \n 2 2024-02-19 12:00:00 2024-02-19 13:00:00         1  [200 × 200] K    \n 3 2024-02-19 12:00:00 2024-02-19 14:00:00         2  [200 × 200] K    \n 4 2024-02-19 12:00:00 2024-02-19 15:00:00         3  [200 × 200] K    \n 5 2024-02-19 12:00:00 2024-02-19 16:00:00         4  [200 × 200] K    \n 6 2024-02-19 12:00:00 2024-02-19 17:00:00         5  [200 × 200] K    \n 7 2024-02-19 12:00:00 2024-02-19 18:00:00         6  [200 × 200] K    \n 8 2024-02-19 12:00:00 2024-02-19 19:00:00         7  [200 × 200] K    \n 9 2024-02-19 12:00:00 2024-02-19 20:00:00         8  [200 × 200] K    \n10 2024-02-19 12:00:00 2024-02-19 21:00:00         9  [200 × 200] K    \n11 2024-02-19 12:00:00 2024-02-19 22:00:00        10  [200 × 200] K    \n12 2024-02-19 12:00:00 2024-02-19 23:00:00        11  [200 × 200] K    \n13 2024-02-19 12:00:00 2024-02-20 00:00:00        12  [200 × 200] K    \n# ℹ 3 more variables: parameter &lt;chr&gt;, level_type &lt;chr&gt;, level &lt;dbl&gt;\n\n\nOther arguments to read_grid() allow you to specify which geofields for a parameter to get from the file. For example, to get lead times of 6 and 12 hours you would do this:\n\nread_grid(\n  here(\"data/netcdf/arome_arctic/2024/02/19/arome_arctic_2_5km_20240219T12Z.nc\"),\n  \"t2m\",\n  lead_time  = c(6, 12),\n  data_frame = TRUE\n) \n\n\n\n# A tibble: 2 × 8\n  fcst_dttm           valid_dttm          lead_time gridded_data units parameter\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt;    &lt;geolist&gt; &lt;chr&gt; &lt;chr&gt;    \n1 2024-02-19 12:00:00 2024-02-19 18:00:00         6  [200 × 200] K     t2m      \n2 2024-02-19 12:00:00 2024-02-20 00:00:00        12  [200 × 200] K     t2m      \n# ℹ 2 more variables: level_type &lt;chr&gt;, level &lt;dbl&gt;\n\n\n\n\nFile format options\n\nGrib\nSometimes there isn’t sufficient information in the file, or the defaults are incorrect. Take, for example, a grib2 file that uses non standard parameter numbers for total precipitation.\n\nread_grid(\n  here(\"data/grib/exp1/mbr001/fc2022071012+006grib2_fp\"), \n  \"pcp\"\n)\n\nWarning: Parameter \"pcp\" (shortName: \"tp\", typeOfLevel: \"heightAboveGround\" /\n\"surface\" for level(s) 0) not found in grib file.\n\n\nError: None of the requested data could be read from grib file: /home/andrewts/R-projects/harp_training_2024/data/grib/exp1/mbr001/fc2022071012+006grib2_fp\n\n\nWe see that it is looking for a parameter with shortName \"tp\", but cannot find it. Yet we know the file contains total precipitation. If you have access to the grib tables used to encode this grib file you can look up the correct information to get the data. In this case total precipitation uses the grib2 parameterNumber 8.\nOptions for the file format are passed through the file_format_opts argument, and those options can be generated with helper functions for the file format. For grib files those options can be generated by grib_opts().\n\ngrib_opts()\n\n$meta\n[1] TRUE\n\n$multi\n[1] FALSE\n\n$param_find\nNULL\n\n$level_find\nNULL\n\n$step_find\nNULL\n\n\nHere, one of the arguments is param_find and we use that in conjunction with the use_grib_*() function use_grib_parameterNumber(). The param_find argument takes a named list, with the name the same as the parameter so that it knows which parameter to which to apply those options.\n\nread_grid(\n  here(\"data/grib/exp1/mbr001/fc2022071012+006grib2_fp\"), \n  \"pcp\",\n  file_format_opts = grib_opts(\n    param_find = list(pcp = use_grib_parameterNumber(8))\n  )\n)\n\neidb : Total precipitation  \nTime:\n 2022/07/10 12:00 +0\nDomain summary:\n161 x 211 domain\nProjection summary:\nproj= lcc \nNE = ( 13.73206 , 57.99135 )\nSW = ( 7.971006 , 53.08321 )\nData summary:\n0 0 0 0.0005843104 0 0.07226562 \n\n\n\n\nNetCDF\nReading NetCDF files with harp used to require the user to always specify the names of the dimensions and the projection variable in the file. This has improved in recent versions, and as shown above you can often read from NetCDF files without providing extra options. However, in some cases you still have to pass some information about the content of the file. This happens when the x and y dimensions are not named “x” and “y”, and/or the projection is not a lambert projection. Additionally if the data are stored in reverse order this must be specified. An example of this is would be forecasts from a global model like ECMWF’s IFS.\nIn the below example, we tell the function that the x and y dimensions are “longitude” and “latitude”, the projection variable is “projection_regular_ll” and that the data are stored in reverse order in the y dimension (north - south as opposed to south - north).\n\nread_grid(\n  here(\"data/netcdf/ifsens/ifsens_20240219T000000Z.nc\"), \n  \"t2m\",\n  lead_time        = 6,\n  members          = 0,\n  file_format_opts = netcdf_opts(\n    x_dim     = \"longitude\",\n    y_dim     = \"latitude\",\n    y_rev     = TRUE,\n    proj4_var = \"projection_regular_ll\"\n  )\n)\n\n : t2m K \nTime:\n 2024/02/19 00:00\nDomain summary:\n81 x 80 domain\nProjection summary:\nproj= longlat \nNE = ( -4 , 57.9 )\nSW = ( -12 , 50 )\nData summary:\n276.9579 281.7704 283.1766 282.7288 283.8329 285.3329"
  },
  {
    "objectID": "read-forecast.html#read_forecast",
    "href": "read-forecast.html#read_forecast",
    "title": "Reading forecast data",
    "section": "read_forecast()",
    "text": "read_forecast()\nread_grid() is used to read data from a single file. In many cases, you will need to read data from multiple files. This is where read_forecast() comes in.\nread_forecast() takes at a minimum the date-times you want to read, the name of the forecast model to give to the data, the parameter you want to read, the path of the files and a file template. Taking the first file we read in this tutorial, here(\"data/grib/exp1/mbr001/fc2022071012+000grib2_fp\"), we can read the same data using read_forecast() with the following expression:\n\nread_forecast(\n  dttm          = 2022071012,\n  fcst_model    = \"exp1\",\n  parameter     = \"t2m\",\n  lead_time     = 0,\n  members       = 1,\n  file_path     = here(\"data/grib\"),\n  file_template = \"{fcst_model}/mbr{MBR3}/fc{YYYY}{MM}{DD}{HH}+{LDT3}grib2_fp\",\n  return_data   = TRUE \n)\n\n\n\n::ensemble gridded forecast:: # A tibble: 1 × 11\n  fcst_dttm           valid_dttm          lead_time parameter exp1_mbr001\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;       &lt;geolist&gt;\n1 2022-07-10 12:00:00 2022-07-10 12:00:00         0 t2m       [161 × 211]\n# ℹ 6 more variables: fcst_model &lt;chr&gt;, step_range &lt;chr&gt;, level_type &lt;chr&gt;,\n#   level &lt;int&gt;, units &lt;chr&gt;, fcst_cycle &lt;chr&gt;\n\n\n\n\n\n\n\n\nTip: You need to tell read_forecast() to return data\n\n\n\n\n\nOne of the primary functions of read_forecast() is to process large volumes of data to interpolate to points and write these interpolated data to new files. Such an operation would often lead to running out of memory if the data were returned to the global environment so the default behaviour is to not return any data. Therefore, if you want the data returned to your global environment, you must set return_data = TRUE. Note that this default behaviour is under review and may be changed in future versions.\n\n\n\n\nFile Name Templates\nIn the above example, the file names are generated by replacing everything that is inside braces with dynamic data. That is to say, values that can change depending on the date-time, the lead time, the ensemble member and the name of the forecast model. We refer to the embraced values as substitutions, and the available substitutions are listed below:\n\n\n\nSubstitution\nDescription\n\n\n\n\n{YYYY}\n4 digit year\n\n\n{MM}\n2 digit month with leading zeros\n\n\n{M}\nSingle digit month\n\n\n{DD}\n2 digit day with leading zeros\n\n\n{D}\nSingle digit day\n\n\n{HH}\n2 digit hour with leading zeros\n\n\n{H}\nSingle digit hour\n\n\n{mm}\n2 digit minute with leading zeros\n\n\n{m}\nSingle digit minute\n\n\n{LDTx}\nLead time\n\n\n{MBRx}\nEnsemble member\n\n\n\nIn the above table, the substitutions {LDTx} and {MBRx} have an optional number of digits to use, where smaller values use leading zeros. The x should be replaced by the number of digits that the file name uses. Leaving off a value for x means that no leading zeros are used.\nGetting the file template correct can often be quite a trying process, so harp includes some built in templates that can be seen with show_file_templates().\n\nshow_file_templates()\n\n# A tibble: 34 × 2\n   template_name          template                                              \n   &lt;chr&gt;                  &lt;chr&gt;                                                 \n 1 arome_arctic_extracted /lustre/storeB/immutable/archive/projects/metproducti…\n 2 arome_arctic_full      /lustre/storeB/immutable/archive/projects/metproducti…\n 3 arome_arctic_sfx       /lustre/storeB/immutable/archive/projects/metproducti…\n 4 fctable                {file_path}/{fcst_model}/{YYYY}/{MM}/FCTABLE_{paramet…\n 5 fctable_det            {file_path}/{det_model}/{YYYY}/{MM}/FCTABLE_{paramete…\n 6 fctable_eps            {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{paramete…\n 7 fctable_eps_all_cycles {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{paramete…\n 8 fctable_eps_all_leads  {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{paramete…\n 9 glameps_grib           {file_path}/{eps_model}/{sub_model}/{YYYY}/{MM}/{DD}/…\n10 harmoneps_grib         {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{…\n11 harmoneps_grib_fp      {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{…\n12 harmoneps_grib_sfx     {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{…\n13 harmonie_grib          {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH…\n14 harmonie_grib_fp       {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH…\n15 harmonie_grib_sfx      {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH…\n16 meps_cntrl_extracted   /lustre/storeB/immutable/archive/projects/metproducti…\n17 meps_cntrl_sfx         /lustre/storeB/immutable/archive/projects/metproducti…\n18 meps_det               /lustre/storeB/immutable/archive/projects/metproducti…\n19 meps_extracted         /lustre/storeB/immutable/archive/projects/metproducti…\n20 meps_full              /lustre/storeB/immutable/archive/projects/metproducti…\n21 meps_lagged_6h_subset  /lustre/storeB/immutable/archive/projects/metproducti…\n22 meps_sfx               /lustre/storeB/immutable/archive/projects/metproducti…\n23 meps_subset            /lustre/storeB/immutable/archive/projects/metproducti…\n24 obsoul                 {file_path}/obsoul_1_xxxxxy_{country}_{YYYY}{MM}{DD}{…\n25 obstable               {file_path}/OBSTABLE_{YYYY}.sqlite                    \n26 vfld                   {file_path}/{fcst_model}/vfld{fcst_model}{YYYY}{MM}{D…\n27 vfld_det               {file_path}/{det_model}/vfld{det_model}{YYYY}{MM}{DD}…\n28 vfld_det_noexp         {file_path}/{det_model}/vfld{YYYY}{MM}{DD}{HH}{LDT2}  \n29 vfld_eps               {file_path}/{sub_model}/vfld{sub_model}mbr{MBR3}{YYYY…\n30 vfld_eps_noexp         {file_path}/{sub_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{H…\n31 vfld_multimodel        {file_path}/{sub_model}/vfld{sub_model}mbr{MBR3}{YYYY…\n32 vfld_multimodel_noexp  {file_path}/{sub_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{H…\n33 vfld_noexp             {file_path}/{fcst_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{…\n34 vobs                   {file_path}/vobs{YYYY}{MM}{DD}{HH}                    \n\n\nOften the templates are a bit long to be seen on screen so a single template can be shown by selecting the number for that template.\n\nshow_file_templates(29)\n\n\ntemplate_name:\n vfld_eps \n \ntemplate:\n {file_path}/{sub_model}/vfld{sub_model}mbr{MBR3}{YYYY}{MM}{DD}{HH}{LDT2} \n\n\nThis means we can, for example, read an ensemble vfld file with the \"vfld_eps\" template. Here we are setting parameter = NULLto read all parameters from the vfld file (this is only an option for vfld files).\n\nread_forecast(\n  dttm          = 2024021900,\n  fcst_model    = \"MEPS_preop\",\n  parameter     = NULL,\n  lead_time     = 3,\n  members       = 0,\n  file_path     = here(\"data/vfld\"),\n  file_template = \"vfld_eps\",\n  return_data   = TRUE \n)\n\n\n\n::ensemble point forecast:: # A tibble: 80,746 × 12\n   fcst_dttm           lead_time parameter   SID MEPS_preop_mbr000 units\n   &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;             &lt;dbl&gt; &lt;chr&gt;\n 1 2024-02-19 00:00:00         3 CCtot      1001                 8 oktas\n 2 2024-02-19 00:00:00         3 CCtot      1010                 8 oktas\n 3 2024-02-19 00:00:00         3 CCtot      1014                 8 oktas\n 4 2024-02-19 00:00:00         3 CCtot      1015                 8 oktas\n 5 2024-02-19 00:00:00         3 CCtot      1018                 8 oktas\n 6 2024-02-19 00:00:00         3 CCtot      1021                 8 oktas\n 7 2024-02-19 00:00:00         3 CCtot      1022                 8 oktas\n 8 2024-02-19 00:00:00         3 CCtot      1023                 7 oktas\n 9 2024-02-19 00:00:00         3 CCtot      1025                 8 oktas\n10 2024-02-19 00:00:00         3 CCtot      1026                 8 oktas\n# ℹ 80,736 more rows\n# ℹ 6 more variables: fcst_model &lt;chr&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, p &lt;dbl&gt;,\n#   valid_dttm &lt;dttm&gt;, fcst_cycle &lt;chr&gt;\n\n\n\n\nData classes\nUnlike read_grid(), read_forecast() always returns a data frame. These data frames have a harp_df class and then an attempt is made to assign a subclass depending on the data. In the above examples you will see one output shown as\n::ensemble gridded forecast::\nand the other as\n::ensemble point forecast::\nThis means that the function has recognised that these are ensemble gridded forecasts and ensemble point forecasts and assigned the appropriate sub classes. Since in both of these cases only one ensemble member has been read in, they can be converted to deterministic forecasts using as_det()\n\nread_forecast(\n  dttm          = 2024021900,\n  fcst_model    = \"MEPS_preop\",\n  parameter     = NULL,\n  lead_time     = 3,\n  members       = 0,\n  file_path     = here(\"data/vfld\"),\n  file_template = \"vfld_eps\",\n  return_data   = TRUE \n) |&gt; \n  as_det()\n\n\n\n::deterministic point forecast:: # A tibble: 80,746 × 12\n   fcst_dttm           lead_time parameter   SID  fcst units fcst_model   lat\n   &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 2024-02-19 00:00:00         3 CCtot      1001     8 oktas MEPS_preop  70.9\n 2 2024-02-19 00:00:00         3 CCtot      1010     8 oktas MEPS_preop  69.3\n 3 2024-02-19 00:00:00         3 CCtot      1014     8 oktas MEPS_preop  69.2\n 4 2024-02-19 00:00:00         3 CCtot      1015     8 oktas MEPS_preop  69.6\n 5 2024-02-19 00:00:00         3 CCtot      1018     8 oktas MEPS_preop  69.2\n 6 2024-02-19 00:00:00         3 CCtot      1021     8 oktas MEPS_preop  70.0\n 7 2024-02-19 00:00:00         3 CCtot      1022     8 oktas MEPS_preop  70.0\n 8 2024-02-19 00:00:00         3 CCtot      1023     7 oktas MEPS_preop  69.1\n 9 2024-02-19 00:00:00         3 CCtot      1025     8 oktas MEPS_preop  69.7\n10 2024-02-19 00:00:00         3 CCtot      1026     8 oktas MEPS_preop  69.7\n# ℹ 80,736 more rows\n# ℹ 4 more variables: lon &lt;dbl&gt;, p &lt;dbl&gt;, valid_dttm &lt;dttm&gt;, fcst_cycle &lt;chr&gt;\n\n\nThe output is now labelled as ::deterministic point forecast:: and the data column is now simply fcst.\n\n\n\n\n\n\nTip: |&gt; The pipe operator\n\n\n\nIn the above we also use R’s pipe operator |&gt;. The pipe operator takes the result of the function that comes before it and passes it to the function that comes after it as the first argument. It can be thought of as “and then” or “and send to”.\n\n\nWhile the harp_df classes may not be something you need to know much about, many functions in harp rely on these classes in order to know what to do with the data.\nAny data frame can be converted to a harp_df data frame as long as it has a valid_dttm column. That is to say a column that contains the valid date time for each row of data. Deterministic data frames are initially recognised by having a column name that ends with “_det” and ensemble columns are recognised by column names that end with “_mbrXXX”, where XXX is a 3 digit member number with leading zeros.\nTo demonstrate we can use some columns from harp’s built in test data to construct a data frame and give it a harp class using as_harp_df().\n\npoint_df &lt;- data.frame(\n  fcst_dttm  = det_point_df$fcst_dttm[1:5],\n  valid_dttm = det_point_df$valid_dttm[1:5],\n  lead_time  = det_point_df$lead_time[1:5],\n  SID        = det_point_df$SID[1:5],\n  point_det  = det_point_df$fcst[1:5]\n)\n\nclass(point_df)\n\n[1] \"data.frame\"\n\npoint_df &lt;- as_harp_df(point_df)\n\nclass(point_df)\n\n[1] \"harp_det_point_df\" \"harp_point_df\"     \"harp_df\"          \n[4] \"tbl_df\"            \"tbl\"               \"data.frame\"       \n\n\nThe harp_df class and subclasses can be removed with deharp(). This can be important as a small number functions that take simple data frames will not work with harp_df data frames.\n\nclass(deharp(point_df))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nGeographic transformations\nread_forecast() and read_grid() have the ability to perform geographic transformations on the data at read time. These transformations include interpolating to point locations, regridding to another grid, taking a subset of a grid or pulling out a line section through a grid to create cross sections. Here we will concentrate on the interpolation to point locations, transformation = \"interpolate\"\n\nInterpolation to geographic point locations\nWhen selecting a transformation, there is an accompanying transformation_opts argument to pass the options for that transformation. Each transformation has a function that generates those options - in the case of the “interpolate” transformation, that is interpolate_opts(), which has a number of default options.\n\ninterpolate_opts()\n\nNo stations specified. Using default stations: 'station_list'\n\n\n$stations\n# A tibble: 13,417 × 5\n     SID   lat   lon  elev name         \n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n 1  1001  70.9 -8.67   9.4 JAN MAYEN    \n 2  1002  80.1 16.2    8   VERLEGENHUKEN\n 3  1003  77   15.5   11.1 HORNSUND     \n 4  1004  78.9 11.9    8   NY-ALESUND II\n 5  1006  78.3 22.8   14   EDGEOYA      \n 6  1007  78.9 11.9    7.7 NY-ALESUND   \n 7  1008  78.2 15.5   26.8 SVALBARD AP  \n 8  1009  80.7 25.0    5   KARL XII OYA \n 9  1010  69.3 16.1   13.1 ANDOYA       \n10  1011  80.1 31.5   10   KVITOYA      \n# ℹ 13,407 more rows\n\n$method\n[1] \"nearest\"\n\n$correct_t2m\n[1] TRUE\n\n$keep_model_t2m\n[1] FALSE\n\n$lapse_rate\n[1] 0.0065\n\n$clim_file\nNULL\n\n$clim_file_format\nNULL\n\n$clim_file_opts\nNULL\n\n$clim_param\n[1] \"sfc_geo\"\n\n$use_mask\n[1] FALSE\n\n$weights\nNULL\n\n$keep_raw_data\n[1] FALSE\n\n\nThe most important information for the “interpolate” transformation is the locations to which to interpolate the data. This is provided in the stations argument, and by default harp’s built in list of meteorological stations, station_list is used. If not using the default, stations needs to be a data frame with the following columns:\n\n\n\nColumn name\nDescription\n\n\n\n\nSID\nA unique identifier for the location\n\n\nlat\nThe latitude of the location in decimal degrees\n\n\nlon\nThe longitude of the location in decimal degrees\n\n\nelev*\nThe elevation of the location in meters\n\n\nname*\nA name for the location\n\n\n\nColumn names marked with * are optional.\nFor a first example we will read in exp1 10m wind speed (“S10m”) for lead times 0 - 3 hours, and members 1 -3, and interpolate to the the default stations (in this case we do not need to set anything for transformation_opts as we will use the defaults).\n\nread_forecast(\n  dttm           = 2022071012,\n  fcst_model     = \"exp1\",\n  parameter      = \"s10m\",\n  lead_time      = seq(0, 3),\n  members        = seq(1, 3),\n  file_path      = here(\"data/grib\"),\n  file_template  = \"{fcst_model}/mbr{MBR3}/fc{YYYY}{MM}{DD}{HH}+{LDT3}grib2_fp\",\n  transformation = \"interpolate\",\n  return_data    = TRUE \n)\n\n\n\nWarning: 'transformation_opts' not set for transformation = 'interpolate'.\nUsing default interpolate_opts()\n\n\nWarning: Parameter \"sfc_geo\" (shortName: \"z\", typeOfLevel: \"heightAboveGround\"\n/ \"surface\" for level(s) 0) not found in grib file.\n\n\nError : None of the requested data could be read from grib file: /home/andrewts/R-projects/harp_training_2024/data/grib/exp1/mbr001/fc2022071012+000grib2_fp\n\n\n::ensemble point forecast:: # A tibble: 628 × 16\n   fcst_dttm           valid_dttm          lead_time   SID exp1_mbr001\n   &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n 1 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2512        5.29\n 2 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2513        4.98\n 3 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2516        4.96\n 4 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2517        4.28\n 5 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2518        5.92\n 6 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2526        5.84\n 7 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2527        5.85\n 8 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2531        3.90\n 9 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2536        5.45\n10 2022-07-10 12:00:00 2022-07-10 12:00:00         0  2539        5.42\n# ℹ 618 more rows\n# ℹ 11 more variables: exp1_mbr002 &lt;dbl&gt;, exp1_mbr003 &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst_model &lt;chr&gt;, parameter &lt;chr&gt;, step_range &lt;chr&gt;, level_type &lt;chr&gt;,\n#   level &lt;int&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, fcst_cycle &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote: Warnings and errors\n\n\n\n\n\nInterpolation weights need to be computed from one of the fields in the file. By default it tries to get these from the surface geopotential. If surface geopotential is not found in the file a warning and error are thrown, but it simply continues to compute the interpolation weights from the first parameter to be read. It’s nothing to worry about!\n\n\n\nIt is also possible to interpolate to any point location (as long as it exists inside the domain - in this case over Denmark) with a data frame to be sent to the stations argument. In this case, we will also use bilinear interpolation (the default is nearest neighbour).\n\nmy_stations &lt;- data.frame(\n  SID = c(\"CPH\", \"AAR\"),\n  lon = c(12.64, 10.62),\n  lat = c(55.61, 56.31)\n)\n\n\n\n\n\n\n\nTip: Station IDs (SID) do not have to be numbers\n\n\n\n\n\n\n\nread_forecast(\n  dttm                = 2022071012,\n  fcst_model          = \"exp1\",\n  parameter           = \"s10m\",\n  lead_time           = seq(0, 3),\n  members             = seq(1, 3),\n  file_path           = here(\"data/grib\"),\n  file_template       = \"{fcst_model}/mbr{MBR3}/fc{YYYY}{MM}{DD}{HH}+{LDT3}grib2_fp\",\n  transformation      = \"interpolate\",\n  transformation_opts = interpolate_opts(\n    stations = my_stations,\n    method   = \"bilinear\"\n  ),\n  return_data         = TRUE \n)\n\n\n\nWarning: No 'elev' column found in stations, and correct_t2m = TRUE. Setting\ncorrect_t2m = FALSE\n\n\nWarning: Parameter \"sfc_geo\" (shortName: \"z\", typeOfLevel: \"heightAboveGround\"\n/ \"surface\" for level(s) 0) not found in grib file.\n\n\nError : None of the requested data could be read from grib file: /home/andrewts/R-projects/harp_training_2024/data/grib/exp1/mbr001/fc2022071012+000grib2_fp\n\n\n::ensemble point forecast:: # A tibble: 8 × 16\n  fcst_dttm           valid_dttm          lead_time SID   exp1_mbr001\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 2022-07-10 12:00:00 2022-07-10 12:00:00         0 CPH          6.23\n2 2022-07-10 12:00:00 2022-07-10 12:00:00         0 AAR          8.32\n3 2022-07-10 12:00:00 2022-07-10 13:00:00         1 CPH          6.65\n4 2022-07-10 12:00:00 2022-07-10 13:00:00         1 AAR          8.36\n5 2022-07-10 12:00:00 2022-07-10 14:00:00         2 CPH          6.73\n6 2022-07-10 12:00:00 2022-07-10 14:00:00         2 AAR          8.14\n7 2022-07-10 12:00:00 2022-07-10 15:00:00         3 CPH          6.93\n8 2022-07-10 12:00:00 2022-07-10 15:00:00         3 AAR          7.94\n# ℹ 11 more variables: exp1_mbr002 &lt;dbl&gt;, exp1_mbr003 &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst_model &lt;chr&gt;, parameter &lt;chr&gt;, step_range &lt;chr&gt;, level_type &lt;chr&gt;,\n#   level &lt;int&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, fcst_cycle &lt;chr&gt;\n\n\nOnce the data are interpolated they can be output to files in SQLite format. SQLite files allow the data to be filtered and read much more quickly for further use, for example in verification. This is done with the argument output_file_opts and the options can be set with fctable_opts(), with the most important of those options being the path to which to write the files.\n\nread_forecast(\n  dttm                = 2022071012,\n  fcst_model          = \"exp1\",\n  parameter           = \"s10m\",\n  lead_time           = seq(0, 3),\n  members             = seq(1, 3),\n  file_path           = here(\"data/grib\"),\n  file_template       = \"{fcst_model}/mbr{MBR3}/fc{YYYY}{MM}{DD}{HH}+{LDT3}grib2_fp\",\n  transformation      = \"interpolate\",\n  transformation_opts = interpolate_opts(\n    stations = my_stations,\n    method   = \"bilinear\"\n  ),\n  output_file_opts = fctable_opts(path = here(\"data/FCTABLE\"))\n)\n\n\n\nWarning: No 'elev' column found in stations, and correct_t2m = TRUE. Setting\ncorrect_t2m = FALSE\n\n\nWarning: Parameter \"sfc_geo\" (shortName: \"z\", typeOfLevel: \"heightAboveGround\"\n/ \"surface\" for level(s) 0) not found in grib file.\n\n\nError : None of the requested data could be read from grib file: /home/andrewts/R-projects/harp_training_2024/data/grib/exp1/mbr001/fc2022071012+000grib2_fp\n\n\nWhen interpolating 2m temperature to geographic point locations an attempt is made to correct the temperature at the topographic height in the model to the actual topographic heights of the geographic point locations. This is done using a simple lapse rate conversion of 0.0065 K.m-1, although this can be set in interpolate_opts(). In order for this correction to happen, the stations data frame needs to have an “elev” column that has the contains the elevation of the station in meters. Furthermore, there needs to be information about the model elevation. By default the model elevation is inferred from the surface geopotential (“sfc_geo”), but it can be set to topographic elevation by setting the clim_param argument to interpolate_opts() to “topo”. This information about model elevation can be in the same file as the forecast, or in a separate file, the path to which can be set in the clim_file argument to interpolate_opts().\nIn the following example, the surface geopotential is in the same file as the forecast, but it requires different options to read the NetCDF variable (there is no time dimension), which we can use modify_opts() to modify the existing options. Since we need to set clim_file_opts, as things stand this means we also have to populate the clim_file argument. Since we are going to use the file format options repeatedly, it makes sense to write them to a variable.\n\nifsens_fmt_opts &lt;- netcdf_opts(\n  x_dim     = \"longitude\", \n  y_dim     = \"latitude\",\n  y_rev     = TRUE,\n  proj4_var = \"projection_regular_ll\"\n)\n\nFirst we will read the height corrected 2m temperature\n\nt2m &lt;- read_forecast(\n  dttm                = 2024021900,\n  fcst_model          = \"ifsens\",\n  parameter           = \"t2m\",\n  lead_time           = 0,\n  members             = 0,\n  file_path           = here(\"data/netcdf\"),\n  file_template       = \"{fcst_model}/{fcst_model}_{YYYY}{MM}{DD}T{HH}{mm}00Z.nc\",\n  file_format_opts    = ifsens_fmt_opts,\n  transformation      = \"interpolate\",\n  transformation_opts = interpolate_opts(\n    clim_file      = here(\"data/netcdf/ifsens/ifsens_20240219T000000Z.nc\"),\n    clim_file_opts = modify_opts(ifsens_fmt_opts, time_var = NA)\n  ),\n  return_data         = TRUE\n)\n\nAnd now the uncorrected, by setting correct_t2m = FALSE in interpolate_opts()\n\nt2m_uncorrected &lt;- read_forecast(\n  dttm                = 2024021900,\n  fcst_model          = \"ifsens\",\n  parameter           = \"t2m\",\n  lead_time           = 0,\n  members             = 0,\n  file_path           = here(\"data/netcdf\"),\n  file_template       = \"{fcst_model}/{fcst_model}_{YYYY}{MM}{DD}T{HH}{mm}00Z.nc\",\n  file_format_opts    = ifsens_fmt_opts,\n  transformation      = \"interpolate\",\n  transformation_opts = interpolate_opts(correct_t2m = FALSE),\n  return_data         = TRUE\n)\n\n\n\nError in nc_id[[\"var\"]][[nc_param]][[\"dim\"]][[idx]] : \n  attempt to select less than one element in get1index\n\n\nAnd we can then compare the impact of the temperature correction.\n\nt2m$ifsens_mbr000 - t2m_uncorrected$ifsens_mbr000\n\n [1]  0.1458409286  0.0414019990 -0.3619363903  1.3419084521  0.1547063988\n [6]  0.0507633395  0.4325378796 -3.9913955694 -0.1321044335 -4.7310049222\n[11]  1.4207357056  1.5684432188  1.9425989706  1.4803179003  0.3218133717\n[16] -0.0740768766  0.5275350526  0.4473404526  1.9519767322  0.5423946145\n[21]  0.5653087660  0.3477330038 -0.1038059902  0.9476696306  0.5350001595\n[26] -0.5841755696 -0.5483523687  0.2738574474 -0.2281737633  0.1697904205\n[31] -1.4340309741 -0.4900098497  0.6684409930 -0.2920713961 -0.1685245416\n[36]  0.0447920530 -0.0175268444 -0.3850984770  0.6235608309 -0.0081554177\n[41] -0.0016554177  0.0040337500  0.0945613069 -0.1910567042 -0.3376315915\n[46] -0.1266898831 -0.0464718076  0.2549739661  0.7939446467  0.1764446467\n[51]  0.2554832418  0.5228946145 -0.0522917743  0.2616012528  0.3904855446\n[56] -0.3578620560  0.0427422151  0.1355783016  0.0090695662 -0.3385631367\n[61]  0.1972646904  0.4642036200  0.0509845927  0.1260929935  0.6516448525\n[66] -0.0276085632 -0.0617536552  0.4499873715 -0.6436678067 -0.1589722066\n[71]  0.2974116092 -0.0125245416 -0.1436168226 -0.0905580551 -0.0002582609\n[76]  0.2583546686 -0.4707912983 -0.0034010500  0.1112770795 -0.8161154717\n[81] -0.0619781631  0.1267503825  0.1419074795 -0.1062954280\n\n\n\n\nReading SQLite files\nOnce SQLite files have been written by read_forecast(), they can then be read using read_point_forecast(), which works in much the same way as read_forecast(). The main difference is that you have to specify whether you are reading ensemble (“eps”) or deterministic (“det”) forecasts via the fcst_type argument.\n\nread_point_forecast(\n  dttm       = 2022071012,\n  fcst_model = \"exp1\", \n  fcst_type  = \"eps\",\n  parameter  = \"s10m\",\n  file_path  = here(\"data/FCTABLE\") \n)\n\n\n\n::ensemble point forecast:: # A tibble: 4 × 12\n  fcst_dttm           valid_dttm          lead_time SID   exp1_mbr001\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 2022-07-10 12:00:00 2022-07-10 12:00:00         0 AAR          8.32\n2 2022-07-10 12:00:00 2022-07-10 12:00:00         0 CPH          6.23\n3 2022-07-10 12:00:00 2022-07-10 15:00:00         3 AAR          7.94\n4 2022-07-10 12:00:00 2022-07-10 15:00:00         3 CPH          6.93\n# ℹ 7 more variables: exp1_mbr002 &lt;dbl&gt;, exp1_mbr003 &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst_model &lt;chr&gt;, parameter &lt;chr&gt;, z &lt;int&gt;, fcst_cycle &lt;chr&gt;\n\n\nread_point-forecast() will be explained further in the Point Verification Workflow tutorial\n\n\n\nLagged ensembles\nMany ensembles are generated with time lags with the goal of maximising the number of ensemble members while making most efficient use of computing resources. This means that output files for different members are produced for different forecast times. This creates a challenge in reading in the data using read_forecast()’s templating system. However, the lags argument allows you to specify which members have output files for which forecast times. This is done by having a vector that is the same length as the vector for members with the corresponding lag for each member.\nMEPS, the ensemble run by MetCoOp (a collaboration between MET Norway, SMHI and FMI) is one such example. It produces 5 ensemble members every hour, resulting in 15 independent members every 3 hours. If we take the 12:00 UTC 15 member ensemble as an example, the members are allocated as follows:\n\n\n\nTime (UTC)\nMembers\n\n\n\n\n12:00\n0, 1, 2, 9, 12\n\n\n11:00\n5, 6, 8, 11, 14\n\n\n10:00\n3, 4, 7, 10, 13\n\n\n\nThis means that the members at 11:00 UTC have a lag of 1 hour and those at 10:00 UTC have a lag of 2 hours.\nRather than have to write the members and lags out as a vector, we can make a nested list of the lagging information and a function to generate those vectors.\n\nmeps_mbrs &lt;- list(\n  list(lag = 0, members = c(0, 1, 2, 9, 12)),\n  list(lag = 1, members = c(5, 6, 8, 11, 14)),\n  list(lag = 2, members = c(3, 4, 7, 10, 13))\n)\n\nget_mbrs &lt;- function(x, mbr = \"members\") {\n  unlist(lapply(x, \\(d) d[[mbr]]))\n}\n\nget_lags &lt;- function(x, lag = \"lag\", mbr = \"members\") {\n  unlist(lapply(x, \\(d) rep(d[[lag]], length(d[[mbr]]))))\n}\n\nget_mbrs(meps_mbrs)\n\n [1]  0  1  2  9 12  5  6  8 11 14  3  4  7 10 13\n\nget_lags(meps_mbrs)\n\n [1] 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2\n\n\nAnd now we can run read_forecast() for those lags and members\n\nread_forecast(\n  dttm             = 2024021512, \n  fcst_model       = \"meps\",\n  parameter        = \"pcp\",\n  lead_time        = 0, \n  members          = get_mbrs(meps_mbrs),\n  lags             = get_lags(meps_mbrs),\n  file_path        = here(\"data/netcdf\"), \n  file_template    = \"{fcst_model}_lagged/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR2}/{fcst_model}_sfc_{LDT2}_{YYYY}{MM}{DD}T{HH}Z.nc\",\n  file_format_opts = netcdf_opts(ref_time_var = \"forecast_reference_time\"), \n  return_data      = TRUE \n)\n\n\n\n::ensemble gridded forecast:: # A tibble: 1 × 24\n  fcst_dttm           valid_dttm          lead_time parameter meps_mbr000\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;chr&gt;       &lt;geolist&gt;\n1 2024-02-15 12:00:00 2024-02-15 12:00:00         0 pcp       [201 × 201]\n# ℹ 19 more variables: meps_mbr001 &lt;geolist&gt;, meps_mbr002 &lt;geolist&gt;,\n#   meps_mbr009 &lt;geolist&gt;, meps_mbr012 &lt;geolist&gt;, meps_mbr005_lag1h &lt;geolist&gt;,\n#   meps_mbr006_lag1h &lt;geolist&gt;, meps_mbr008_lag1h &lt;geolist&gt;,\n#   meps_mbr011_lag1h &lt;geolist&gt;, meps_mbr014_lag1h &lt;geolist&gt;,\n#   meps_mbr003_lag2h &lt;geolist&gt;, meps_mbr004_lag2h &lt;geolist&gt;,\n#   meps_mbr007_lag2h &lt;geolist&gt;, meps_mbr010_lag2h &lt;geolist&gt;,\n#   meps_mbr013_lag2h &lt;geolist&gt;, units &lt;chr&gt;, fcst_model &lt;chr&gt;, …\n\n\n\n\n\n\n\n\nErrors and Warnings\n\n\n\n\n\nYou may see warnings along the likes of Ensemble members were requested for  'pcp' but there is no member information. This occurs because the NetCDF files do not have an ensemble_member dimension since each file only contains one ensemble member.\n\n\n\nSince these 15 members are produced by MEPS every three hours, then to read more than one ensemble the value for dttm should have a 3 hour time step, This can be achieved by using seq_dttm() to generate a sequence of date-times as will be seen in the next example.\n\n\n\n\n\n\nTip: Writing interpolated lagged forecasts to SQLite\n\n\n\nWhen writing lagged forecasts to SQLite files the members are not all collected together to form the full ensemble as was returned in the above example. Rather the members for each lag for the full ensemble are collected together in SQLite files for each date-time that contributes to the ensemble. This allows for more flexibility in constructing lagged ensembles from SQLite files and has implications for how you specify lags for read_point_forecast() as will be seen below.\n\n\nHere we will get the lagged ensemble for 2 forecasts 3 hours apart, interpolate the data to points and write the results to SQLite files.\n\nread_forecast(\n  dttm             = seq_dttm(2024021509, 2024021512, \"3h\"), \n  fcst_model       = \"meps\",\n  parameter        = \"pcp\",\n  lead_time        = seq(0, 6), \n  members          = get_mbrs(meps_mbrs),\n  lags             = get_lags(meps_mbrs),\n  file_path        = here(\"data/netcdf\"), \n  file_template    = \"{fcst_model}_lagged/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR2}/{fcst_model}_sfc_{LDT2}_{YYYY}{MM}{DD}T{HH}Z.nc\",\n  file_format_opts = netcdf_opts(ref_time_var = \"forecast_reference_time\"), \n  transformation   = \"interpolate\",\n  output_file_opts = fctable_opts(path = here(\"data/FCTABLE\"))\n)\n\n\n\nError : None of the requested parameters found in file: /home/andrewts/R-projects/harp_training_2024/data/netcdf/meps_lagged/2024/02/15/09/mbr00/meps_sfc_00_20240215T09Z.nc\n\n\nWhen reading the data back with read_point_forecast(), we only need to provide the lags (here we have to specify that they are in hours), since all of the members are collected together for each lag.\n\nread_point_forecast(\n  dttm       = seq_dttm(2024021509, 2024021512, \"3h\"), \n  fcst_model = \"meps\",\n  fcst_type  = \"eps\",\n  parameter  = \"pcp\",\n  lags       = paste0(seq(0, 2), \"h\"),\n  file_path  = here(\"data/FCTABLE\") \n)\n\n\n\n::ensemble point forecast:: # A tibble: 1,038 × 23\n   fcst_dttm           valid_dttm          lead_time   SID meps_mbr000\n   &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n 1 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1203           0\n 2 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1207           0\n 3 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1209           0\n 4 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1218           0\n 5 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1230           0\n 6 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1233           0\n 7 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1238           0\n 8 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1239           0\n 9 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1250           0\n10 2024-02-15 09:00:00 2024-02-15 09:00:00         0  1265           0\n# ℹ 1,028 more rows\n# ℹ 18 more variables: meps_mbr001 &lt;dbl&gt;, meps_mbr002 &lt;dbl&gt;, meps_mbr009 &lt;dbl&gt;,\n#   meps_mbr012 &lt;dbl&gt;, meps_mbr005 &lt;dbl&gt;, meps_mbr006 &lt;dbl&gt;, meps_mbr008 &lt;dbl&gt;,\n#   meps_mbr011 &lt;dbl&gt;, meps_mbr014 &lt;dbl&gt;, meps_mbr003 &lt;dbl&gt;, meps_mbr004 &lt;dbl&gt;,\n#   meps_mbr007 &lt;dbl&gt;, meps_mbr010 &lt;dbl&gt;, meps_mbr013 &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst_model &lt;chr&gt;, parameter &lt;chr&gt;, fcst_cycle &lt;chr&gt;\n\n\nHere the member column names do not include how much that member is lagged.\nSince the data are precipitation, we can return the accumulated precipitation for given accumulation periods. This is done by prefixing the parameter name with “Acc” and following it with the accumulation period. So, 3 hour accumulated precipitation would be “Accpcp3h”. We can read any selection of stations by specifying their IDs (SID) in the stations argument.\n\nread_point_forecast(\n  dttm       = seq_dttm(2024021509, 2024021512, \"3h\"), \n  fcst_model = \"meps\",\n  fcst_type  = \"eps\",\n  parameter  = \"Accpcp3h\",\n  lags       = paste0(seq(0, 2), \"h\"),\n  stations   = c(1425, 1439), \n  file_path  = here(\"data/FCTABLE\") \n)\n\n\n\n::ensemble point forecast:: # A tibble: 8 × 23\n  fcst_dttm           valid_dttm          lead_time   SID meps_mbr000\n  &lt;dttm&gt;              &lt;dttm&gt;                  &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 2024-02-15 09:00:00 2024-02-15 12:00:00         3  1425        2.77\n2 2024-02-15 09:00:00 2024-02-15 12:00:00         3  1439        3.48\n3 2024-02-15 09:00:00 2024-02-15 15:00:00         6  1425        5.74\n4 2024-02-15 09:00:00 2024-02-15 15:00:00         6  1439        5.28\n5 2024-02-15 12:00:00 2024-02-15 15:00:00         3  1425        6.02\n6 2024-02-15 12:00:00 2024-02-15 15:00:00         3  1439        5.47\n7 2024-02-15 12:00:00 2024-02-15 18:00:00         6  1425        9.06\n8 2024-02-15 12:00:00 2024-02-15 18:00:00         6  1439        5.98\n# ℹ 18 more variables: meps_mbr001 &lt;dbl&gt;, meps_mbr002 &lt;dbl&gt;, meps_mbr009 &lt;dbl&gt;,\n#   meps_mbr012 &lt;dbl&gt;, meps_mbr005 &lt;dbl&gt;, meps_mbr006 &lt;dbl&gt;, meps_mbr008 &lt;dbl&gt;,\n#   meps_mbr011 &lt;dbl&gt;, meps_mbr014 &lt;dbl&gt;, meps_mbr003 &lt;dbl&gt;, meps_mbr004 &lt;dbl&gt;,\n#   meps_mbr007 &lt;dbl&gt;, meps_mbr010 &lt;dbl&gt;, meps_mbr013 &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst_model &lt;chr&gt;, parameter &lt;chr&gt;, fcst_cycle &lt;chr&gt;\n\n\nIn the next tutorial we will going through the workflow for doing point verification."
  },
  {
    "objectID": "point-verif-workflow.html",
    "href": "point-verif-workflow.html",
    "title": "Point Verifcation Workflow",
    "section": "",
    "text": "When doing a point verification there are a number of steps to the workflow. Some of these steps you have to do every time, and others might depend on the forecast parameter being verified, the scores you want to compute, or any conditions that you want to apply to the verification. The compulsory steps can be described as:\n\nRead forecast\nRead observations\nJoin\nVerify\n(Save / Plot)\n\nIn this tutorial we will go through each of these steps in turn and then introduce some optional steps, increasing the complexity as we go. It is assumed that SQLite files for both forecasts and observations have been prepared."
  },
  {
    "objectID": "point-verif-workflow.html#workflow-steps",
    "href": "point-verif-workflow.html#workflow-steps",
    "title": "Point Verifcation Workflow",
    "section": "",
    "text": "When doing a point verification there are a number of steps to the workflow. Some of these steps you have to do every time, and others might depend on the forecast parameter being verified, the scores you want to compute, or any conditions that you want to apply to the verification. The compulsory steps can be described as:\n\nRead forecast\nRead observations\nJoin\nVerify\n(Save / Plot)\n\nIn this tutorial we will go through each of these steps in turn and then introduce some optional steps, increasing the complexity as we go. It is assumed that SQLite files for both forecasts and observations have been prepared."
  },
  {
    "objectID": "point-verif-workflow.html#basic-deterministic-verification",
    "href": "point-verif-workflow.html#basic-deterministic-verification",
    "title": "Point Verifcation Workflow",
    "section": "Basic deterministic verification",
    "text": "Basic deterministic verification\nHere we will demonstrate the workflow for a simple deterministic verification of 2m temperature. The forecasts come from the AROME-Arctic model that is run operationally by MET Norway, and we will do the verification for August 2023.\n\nRead forecast\nAs always, the first thing we need to do is to attach the packages that we are going to need. There may be some unfamiliar packages here, but they will be explained as we begin to use functions from them.\n\nlibrary(harp)\nlibrary(here)\nlibrary(dplyr)\nlibrary(forcats)\n\nAll of our forecasts and observations use the same root directories so we’ll set them here\n\nfcst_dir &lt;- here(\"data\", \"FCTABLE\")\nobs_dir  &lt;- here(\"data\", \"OBSTABLE\")\n\nForecasts are read in with read_point_forecast(). We will read in the 00:00 UTC forecasts for lead times 0 - 24 hours every 3 hours.\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083100, \"24h\"),\n  fcst_model = \"arome_arctic\",\n  fcst_type  = \"det\",\n  parameter  = \"T2m\",\n  file_path  = fcst_dir\n)\n\n\n\nRead observations\nObservations are read in with read_point_obs(). Observations files often contain more times and locations than we have for the forecasts. Therefore, we can tell the the function which times and locations to read with the help of unique_valid_dttm() and unique_station().\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n)\n\n\n\nJoin\nNow that we have the forecasts and observations, we need to match the forecasts and observations to the same date-times and locations. We do this by joining the forecast and observations to each other using join_to_fcst(). Basically this is doing an inner join between the forecast and observations data frames with an extra check to make sure the forecast and observations data have the same units.\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\n\n\nVerify\nWe are now ready to verify. For deterministic forecasts this is done with det_verify(). By default the verification is stratified by lead time. All we need to tell the function is which column contains the observations. In this case, that would be “T2m”.\n\ndet_verify(fcst, T2m)\n\n::det_summary_scores:: # A tibble: 9 × 9\n  fcst_model lead_time num_cases num_stations    bias  rmse   mae  stde hexbin  \n  &lt;chr&gt;          &lt;int&gt;     &lt;int&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;  \n1 arome_arc…         0      5611          191  0.501   1.50 1.06   1.42 &lt;tibble&gt;\n2 arome_arc…         3      5669          192  0.427   1.50 1.05   1.44 &lt;tibble&gt;\n3 arome_arc…         6      5703          194 -0.0388  1.16 0.848  1.16 &lt;tibble&gt;\n4 arome_arc…         9      5723          193 -0.222   1.38 1.05   1.36 &lt;tibble&gt;\n5 arome_arc…        12      5715          194 -0.268   1.47 1.11   1.44 &lt;tibble&gt;\n6 arome_arc…        15      5745          193 -0.265   1.42 1.06   1.40 &lt;tibble&gt;\n7 arome_arc…        18      5664          194  0.135   1.28 0.934  1.27 &lt;tibble&gt;\n8 arome_arc…        21      5692          192  0.572   1.75 1.24   1.66 &lt;tibble&gt;\n9 arome_arc…        24      5615          191  0.695   2.00 1.41   1.88 &lt;tibble&gt;\n\n--harp verification for T2m--\n # for forecasts from 00:00 UTC 01 aug. 2023 to 00:00 UTC 31 aug. 2023\n # using 194 observation stations\n # for verification groups: \n    -&gt; lead_time\n\n\nWe can also compute categorical scores by adding some thresholds. This will compute scores for &gt;= threshold categories.\n\ndet_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))\n\n::det_summary_scores:: # A tibble: 9 × 9\n  fcst_model lead_time num_cases num_stations    bias  rmse   mae  stde hexbin  \n  &lt;chr&gt;          &lt;int&gt;     &lt;int&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;  \n1 arome_arc…         0      5611          191  0.501   1.50 1.06   1.42 &lt;tibble&gt;\n2 arome_arc…         3      5669          192  0.427   1.50 1.05   1.44 &lt;tibble&gt;\n3 arome_arc…         6      5703          194 -0.0388  1.16 0.848  1.16 &lt;tibble&gt;\n4 arome_arc…         9      5723          193 -0.222   1.38 1.05   1.36 &lt;tibble&gt;\n5 arome_arc…        12      5715          194 -0.268   1.47 1.11   1.44 &lt;tibble&gt;\n6 arome_arc…        15      5745          193 -0.265   1.42 1.06   1.40 &lt;tibble&gt;\n7 arome_arc…        18      5664          194  0.135   1.28 0.934  1.27 &lt;tibble&gt;\n8 arome_arc…        21      5692          192  0.572   1.75 1.24   1.66 &lt;tibble&gt;\n9 arome_arc…        24      5615          191  0.695   2.00 1.41   1.88 &lt;tibble&gt;\n\n::det_threshold_scores:: # A tibble: 45 × 41\n   fcst_model   lead_time threshold num_stations num_cases_for_threshold_total\n   &lt;chr&gt;            &lt;int&gt;     &lt;dbl&gt;        &lt;int&gt;                         &lt;int&gt;\n 1 arome_arctic         0      280           191                          5335\n 2 arome_arctic         0      282.          191                          4646\n 3 arome_arctic         0      285           191                          3216\n 4 arome_arctic         0      288.          191                          1667\n 5 arome_arctic         0      290           191                           561\n 6 arome_arctic         3      280           192                          5314\n 7 arome_arctic         3      282.          192                          4549\n 8 arome_arctic         3      285           192                          3125\n 9 arome_arctic         3      288.          192                          1565\n10 arome_arctic         3      290           192                           494\n# ℹ 35 more rows\n# ℹ 36 more variables: num_cases_for_threshold_observed &lt;dbl&gt;,\n#   num_cases_for_threshold_forecast &lt;dbl&gt;, cont_tab &lt;list&gt;,\n#   threat_score &lt;dbl&gt;, hit_rate &lt;dbl&gt;, miss_rate &lt;dbl&gt;,\n#   false_alarm_rate &lt;dbl&gt;, false_alarm_ratio &lt;dbl&gt;, heidke_skill_score &lt;dbl&gt;,\n#   pierce_skill_score &lt;dbl&gt;, kuiper_skill_score &lt;dbl&gt;, percent_correct &lt;dbl&gt;,\n#   frequency_bias &lt;dbl&gt;, equitable_threat_score &lt;dbl&gt;, odds_ratio &lt;dbl&gt;, …\n\n--harp verification for T2m--\n # for forecasts from 00:00 UTC 01 aug. 2023 to 00:00 UTC 31 aug. 2023\n # using 194 observation stations\n # for verification groups: \n    -&gt; lead_time & threshold\n\n\n\n\nSave / Plot\nOnce the verification is done we can save the data using save_point_verif() and plot the data using plot_point_verif(). For plotting we give the function the verification data and the score we want to plot. The scores are basically column names in any of the verification data’s data frames.\nFirst we need to write the verification to a variable, and then save.\n\nverif &lt;- det_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))\n\nsave_point_verif(verif, verif_path = here(\"data\", \"verification\", \"det\"))\n\nWe can now plot some scores\n\nplot_point_verif(verif, bias)\n\nplot_point_verif(verif, rmse)\n\n\n\n\n\n\n\nplot_point_verif(verif, frequency_bias)\n\n\n\n\n\n\n\n\nThe last plot looks strange. That’s because the scores exist for each threshold and they are not being separated. We can separate them by mapping the colour to each threshold, by faceting by threshold (faceting means to separate into panels), or by filtering to a single threshold.\n\nplot_point_verif(verif, frequency_bias, colour_by = threshold)\n\n\n\n\n\n\n\nplot_point_verif(verif, frequency_bias, facet_by = vars(threshold))\n\n\n\n\n\n\n\nplot_point_verif(verif, frequency_bias, filter_by = vars(threshold == 285))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: facet_by and filter_by values must be wrapped in vars()\n\n\n\n\n\nThis is because faceting and filtering can be done by more than one variable and the vars() function facilitates that.\n\n\n\nFor hexbin plots, the plots are automatically faceted by the grouping variable of the verification - in this case lead time.\n\nplot_point_verif(verif, hexbin)"
  },
  {
    "objectID": "point-verif-workflow.html#basic-ensemble-verification",
    "href": "point-verif-workflow.html#basic-ensemble-verification",
    "title": "Point Verifcation Workflow",
    "section": "Basic ensemble verification",
    "text": "Basic ensemble verification\nThe workflow for verifying ensemble forecasts is much the same as that for deterministic verification. The only real difference is using the ens_verify() function to compute the score. In this example we will use data from the MEPS model for the same time period as before.\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083100, \"1d\"),\n  fcst_model = \"meps\",\n  fcst_type  = \"eps\",\n  parameter  = \"T2m\", \n  file_path  = fcst_dir \n)\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n)\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\n\nverif &lt;- ens_verify(fcst, T2m, thresholds = seq(280, 290, 2.5))\n\nverif\n\n::ens_summary_scores:: # A tibble: 9 × 16\n  fcst_model lead_time num_cases num_stations mean_bias  stde  rmse spread\n  &lt;chr&gt;          &lt;int&gt;     &lt;int&gt;        &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 meps               0      5830          192   0.219    1.40  1.41  0.952\n2 meps               3      5884          193   0.227    1.49  1.51  0.942\n3 meps               6      5920          195   0.127    1.20  1.21  0.770\n4 meps               9      5940          194   0.0789   1.27  1.27  0.942\n5 meps              12      5933          195   0.00602  1.32  1.32  1.01 \n6 meps              15      5966          194  -0.0223   1.27  1.27  0.989\n7 meps              18      5884          195   0.292    1.22  1.26  0.782\n8 meps              21      5909          193   0.511    1.61  1.69  0.812\n9 meps              24      5832          192   0.593    1.82  1.92  0.878\n# ℹ 8 more variables: dropped_members_spread &lt;dbl&gt;, spread_skill_ratio &lt;dbl&gt;,\n#   dropped_members_spread_skill_ratio &lt;dbl&gt;, hexbin &lt;list&gt;,\n#   rank_histogram &lt;list&gt;, crps &lt;dbl&gt;, crps_potential &lt;dbl&gt;,\n#   crps_reliability &lt;dbl&gt;\n\n::ens_threshold_scores:: # A tibble: 45 × 19\n   fcst_model lead_time threshold fair_brier_score sample_climatology\n   &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n 1 meps               0      280           0.0457               0.892\n 2 meps               3      280           0.0578               0.878\n 3 meps               6      280           0.0132               0.975\n 4 meps               9      280           0.00352              0.991\n 5 meps              12      280           0.00304              0.994\n 6 meps              15      280           0.00362              0.993\n 7 meps              18      280           0.00339              0.990\n 8 meps              21      280           0.0451               0.941\n 9 meps              24      280           0.0719               0.891\n10 meps               0      282.          0.0753               0.732\n# ℹ 35 more rows\n# ℹ 14 more variables: bss_ref_climatology &lt;dbl&gt;, num_stations &lt;int&gt;,\n#   num_cases_total &lt;int&gt;, num_cases_observed &lt;int&gt;, num_cases_forecast &lt;int&gt;,\n#   brier_score &lt;dbl&gt;, brier_skill_score &lt;dbl&gt;, brier_score_reliability &lt;dbl&gt;,\n#   brier_score_resolution &lt;dbl&gt;, brier_score_uncertainty &lt;dbl&gt;,\n#   reliability &lt;list&gt;, roc &lt;list&gt;, roc_area &lt;dbl&gt;, economic_value &lt;list&gt;\n\n::det_summary_scores:: # A tibble: 54 × 11\n   fcst_model sub_model member lead_time num_cases num_stations    bias  rmse\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;     &lt;int&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 meps       meps      mbr000         0      5830          192  0.244   1.29\n 2 meps       meps      mbr000         3      5884          193  0.231   1.44\n 3 meps       meps      mbr000         6      5920          195  0.147   1.21\n 4 meps       meps      mbr000         9      5940          194  0.108   1.32\n 5 meps       meps      mbr000        12      5933          195  0.0298  1.39\n 6 meps       meps      mbr000        15      5966          194 -0.0109  1.35\n 7 meps       meps      mbr000        18      5884          195  0.331   1.33\n 8 meps       meps      mbr000        21      5909          193  0.557   1.76\n 9 meps       meps      mbr000        24      5832          192  0.634   1.99\n10 meps       meps      mbr001         0      5830          192  0.0680  1.60\n# ℹ 44 more rows\n# ℹ 3 more variables: mae &lt;dbl&gt;, stde &lt;dbl&gt;, hexbin &lt;list&gt;\n\n--harp verification for T2m--\n # for forecasts from 00:00 UTC 01 aug. 2023 to 00:00 UTC 31 aug. 2023\n # using 195 observation stations\n # for verification groups: \n    -&gt; lead_time\n\nsave_point_verif(verif, here(\"data\", \"verification\", \"ens\"))\n\nSince this is ensemble data, ensemble specific scores have been computed as well as deterministic summary scores for each member. There are a couple of scores that plot_point_verif() can derive that are not columns in the data frames - these are spread_skill and brier_score_decomposition.\n\nplot_point_verif(verif, spread_skill)\n\nplot_point_verif(verif, crps)\n\n\n\n\n\n\n\nplot_point_verif(verif, brier_score, facet_by = vars(threshold))\n\n\n\n\n\n\n\nplot_point_verif(verif, brier_score_decomposition, facet_by = vars(threshold))\n\n\n\n\n\n\n\nplot_point_verif(verif, reliability, facet_by = vars(threshold))\n\n\n\n\n\n\n\n\nAgain the last one has issues with overplotting. This is because there should be one plot for each threshold and each grouping variable (in this case lead_time), so we need to filter.\n\nplot_point_verif(\n  verif, \n  reliability, \n  facet_by = vars(threshold),\n  filter_by = vars(lead_time == 12)\n)"
  },
  {
    "objectID": "point-verif-workflow.html#comparing-forecast-models",
    "href": "point-verif-workflow.html#comparing-forecast-models",
    "title": "Point Verifcation Workflow",
    "section": "Comparing forecast models",
    "text": "Comparing forecast models\nVerification scores are often useful for comparing the performance of different models, or model developments. With harp it is straightforward to compare different models - it is simply case of reading multiple forecasts in in one go. Here we are going to do a deterministic comparison of the AROME-Arctic model and member 0 of the MEPS and IFSENS ensembles. We need to read the ensemble and deterministic forecasts in separately due to the different fcst_type argument. When there are multiple forecast models, we get a harp_list, which works in the same way as a standard list in R. This means that when we read for the second time we can put the output in a named element of the harp_list\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083100, \"1d\"),\n  fcst_model = c(\"meps\", \"ifsens\"),\n  fcst_type  = \"eps\",\n  parameter  = \"T2m\", \n  members    = 0, \n  file_path  = fcst_dir \n) |&gt; \n  as_det()\n\nfcst$arome_arctic &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083100, \"1d\"),\n  fcst_model = \"arome_arctic\",\n  fcst_type  = \"det\",\n  parameter  = \"T2m\", \n  file_path  = fcst_dir \n)\n\nfcst\n\n• meps\n::deterministic point forecast:: # A tibble: 67,239 × 10\n   fcst_model fcst_dttm           valid_dttm          lead_time   SID parameter\n * &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 67,229 more rows\n# ℹ 4 more variables: fcst_cycle &lt;chr&gt;, model_elevation &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst &lt;dbl&gt;\n\n• ifsens\n::deterministic point forecast:: # A tibble: 37,355 × 10\n   fcst_model fcst_dttm           valid_dttm          lead_time   SID parameter\n * &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 37,345 more rows\n# ℹ 4 more variables: fcst_cycle &lt;chr&gt;, model_elevation &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst &lt;dbl&gt;\n\n• arome_arctic\n::deterministic point forecast:: # A tibble: 64,800 × 11\n   fcst_model  fcst_dttm           valid_dttm          lead_time   SID parameter\n   &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 64,790 more rows\n# ℹ 5 more variables: z &lt;dbl&gt;, fcst &lt;dbl&gt;, fcst_cycle &lt;chr&gt;,\n#   model_elevation &lt;dbl&gt;, units &lt;chr&gt;\n\n\nWhen we have multiple forecast models we should ensure that we are comparing like with like. We therefore only want to verify the cases that are common to all of the forecast models. We can select these cases using common_cases(). Note the number of rows in each of the data frames before selecting the common cases above and after selecting the common cases below.\n\nfcst &lt;- common_cases(fcst)\n\nfcst\n\n• meps\n::deterministic point forecast:: # A tibble: 36,000 × 11\n   fcst_model fcst_dttm           valid_dttm          lead_time   SID parameter\n   &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 meps       2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 35,990 more rows\n# ℹ 5 more variables: fcst_cycle &lt;chr&gt;, model_elevation &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst &lt;dbl&gt;, z &lt;dbl&gt;\n\n• ifsens\n::deterministic point forecast:: # A tibble: 36,000 × 11\n   fcst_model fcst_dttm           valid_dttm          lead_time   SID parameter\n   &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 ifsens     2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 35,990 more rows\n# ℹ 5 more variables: fcst_cycle &lt;chr&gt;, model_elevation &lt;dbl&gt;, units &lt;chr&gt;,\n#   fcst &lt;dbl&gt;, z &lt;dbl&gt;\n\n• arome_arctic\n::deterministic point forecast:: # A tibble: 36,000 × 11\n   fcst_model  fcst_dttm           valid_dttm          lead_time   SID parameter\n   &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;                  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1001 T2m      \n 2 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1010 T2m      \n 3 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1014 T2m      \n 4 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1015 T2m      \n 5 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1018 T2m      \n 6 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1023 T2m      \n 7 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1025 T2m      \n 8 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1026 T2m      \n 9 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1027 T2m      \n10 arome_arct… 2023-08-01 00:00:00 2023-08-01 00:00:00         0  1028 T2m      \n# ℹ 35,990 more rows\n# ℹ 5 more variables: z &lt;dbl&gt;, fcst &lt;dbl&gt;, fcst_cycle &lt;chr&gt;,\n#   model_elevation &lt;dbl&gt;, units &lt;chr&gt;\n\n\nThe rest of the verification workflow is exactly the same.\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n)\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\n\nverif &lt;- det_verify(fcst, T2m , thresholds = seq(280, 290, 2.5))\n\nsave_point_verif(verif, here(\"data\", \"verification\", \"det\"))\n\nWhen plotting the scores, each of the forecast models is automatically assigned a different colour.\n\nplot_point_verif(verif, stde)\n\n\n\n\n\n\n\n\nWe could however plot each forecast model in a separate panel and assign, for example, threshold to control the colour.\n\nplot_point_verif(\n  verif, \n  frequency_bias, \n  colour_by = threshold,\n  facet_by  = vars(fcst_model)\n)"
  },
  {
    "objectID": "point-verif-workflow.html#observation-errors",
    "href": "point-verif-workflow.html#observation-errors",
    "title": "Point Verifcation Workflow",
    "section": "Observation errors",
    "text": "Observation errors\n\nGross error check\nWhen reading observations, read_point_obs() will do a gross error check on the observations to make sure that they have realistic values. You can set the bounds with the min_obs_allowed and max_obs_allowed arguments. For some parameters, there are default values for the minimum and maximum allowed. These can be seen in the parameter definitions with get_param_def(). For example, for 2m temperature we see that the minimum and maximum values are 223 and 333 Kelvin.\n\nget_param_def(\"t2m\")\n\n$description\n[1] \"Air temperature at 2m above the ground\"\n\n$min\n[1] 223\n\n$max\n[1] 333\n\n$grib\n$grib$name\n[1] \"2t\" \"t\" \n\n$grib$level_type\n[1] \"heightAboveGround\" \"surface\"          \n\n$grib$level\n[1] 2\n\n\n$netcdf\n$netcdf$name\n[1] \"air_temperature_2m\"\n\n\n$v\n$v$harp_param\n[1] \"T2m\"\n\n$v$name\n[1] \"TT\"\n\n$v$param_units\n[1] \"K\"\n\n$v$type\n[1] \"SYNOP\"\n\n\n$wrf\n$wrf$name\n[1] \"T2\"\n\n\n$fa\n$fa$name\n[1] \"CLSTEMPERATURE  \"\n\n$fa$units\n[1] \"K\"\n\n\n$obsoul\n$obsoul$name\n[1] 39\n\n$obsoul$units\n[1] \"K\"\n\n$obsoul$harp_name\n[1] \"T2m\"\n\n\n\n\nChecking observations against forecasts\nAnother check that can be done is to compare the values of the observations with forecasts. This is done with the check_obs_against_fcst() function. By default the data are grouped by each station ID and time of day, where the day is split into 4 parts of the day [00:00, 06:00), [06, 12:00), [12:00, 18:00) and [18:00, 00:00) and the standard deviation of the forecast for each of these is computed. The observations are then compared with the forecasts, and if the difference between the, is larger than a certain number of standard deviations then that row is removed from the data.\nUsing the default value of 6 standard deviations we see that no observations are removed.\n\nfcst &lt;- check_obs_against_fcst(fcst, T2m)\n\nℹ Using default `num_sd_allowed = 6` for `parameter = T2m`\n! 0 cases with more than 6 std devs error.\n\n\nIf we make the check more strict to remove all cases where the observations are more than 1 standard deviation away from the forecast, we see that now 723 cases are removed. We can see the removed cases in the \"removed_cases\" attribute of the result.\n\nfcst &lt;- check_obs_against_fcst(fcst, T2m, num_sd_allowed = 1)\n\n! 723 cases with more than 1 std dev error.\nℹ Removed cases can be seen in the \"removed_cases\" attribute.\n\nattr(fcst, \"removed_cases\")\n\n# A tibble: 723 × 5\n     SID valid_dttm            T2m dist_to_fcst tolerance\n   &lt;int&gt; &lt;dttm&gt;              &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1  1001 2023-08-09 12:00:00  283.         2.94      1.21\n 2  1001 2023-08-10 00:00:00  282.         1.54      1.09\n 3  1001 2023-08-10 06:00:00  282.         1.20      1.20\n 4  1001 2023-08-13 00:00:00  279.         2.89      1.09\n 5  1001 2023-08-19 00:00:00  280.         1.17      1.09\n 6  1001 2023-08-21 00:00:00  282.         1.55      1.09\n 7  1001 2023-08-23 00:00:00  276.         2.33      1.09\n 8  1001 2023-08-24 00:00:00  276.         1.30      1.09\n 9  1001 2023-08-24 06:00:00  276.         1.66      1.20\n10  1001 2023-08-30 18:00:00  280.         1.39      1.15\n# ℹ 713 more rows\n\n\n\n\nEnsemble rescaling\nWhen verifying ensembles, we can take the observation error into account by attempting to rescale the distribution of the ensemble forecast to that of the observations. This is done by adding an assumed error distribution to the ensemble forecast by sampling from the error distribution and adding each random draw to an ensemble member. If the error distribution has a mean of 0, the impact will be negligible on most scores. However, for the rank histogram and ensemble spread, this will in effect take the observation errors into account. In harp we call this jittering the forecast and is done by using the jitter_fcst() function.\nTake for example the rank histograms for our MEPS and IFSENS ensembles for 2m temperature.\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083112, \"12h\"), \n  fcst_model = c(\"meps\", \"ifsens\"),\n  fcst_type  = \"eps\", \n  parameter  = \"T2m\",\n  file_path  = fcst_dir \n) |&gt; \n  scale_param(-273.15, \"degC\") |&gt; \n  common_cases()\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n) |&gt; \n  scale_param(-273.15, \"degC\", col = T2m)\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\nverif &lt;- ens_verify(fcst, T2m)\n\nplot_point_verif(\n  verif, \n  normalized_rank_histogram, \n  rank_is_relative = TRUE\n)\n\n\n\n\n\n\n\n\n\nplot_point_verif(\n  verif, \n  spread_skill\n)\n\n\n\n\n\n\n\n\nWe see that both models are underdispersed with a U-shaped rank histogram. If we say that the observations have a noraml error distribution with a mean of 0 and standard deviation of 1 we can jitter the ensemble forecast by adding random draws from that distribution.\n\nfcst &lt;- jitter_fcst(\n  fcst, \n  function(x) x + rnorm(length(x), mean = 0, sd = 1)\n)\n\nverif &lt;- ens_verify(fcst, T2m)\n\nplot_point_verif(\n  verif, \n  normalized_rank_histogram, \n  rank_is_relative = TRUE\n)\n\n\n\n\n\n\n\n\n\nplot_point_verif(\n  verif, \n  spread_skill\n)\n\n\n\n\n\n\n\n\nNow we see that the forecast is much less underdispersed with many of the ranks have a normalized frequency close to 1 and the ensemble spread much closer to the ensemble RMSE.\n\n\n\n\n\n\nWarning: Error Distributions\n\n\n\nharp does not include any estimates for error distributions. It is the user’s responsibility to provide those error distributions. In this example, a normal distribution with a mean of 0 and standard deviation of 1 is for illustrative purposes only."
  },
  {
    "objectID": "point-verif-workflow.html#grouped-verification-and-scaling",
    "href": "point-verif-workflow.html#grouped-verification-and-scaling",
    "title": "Point Verifcation Workflow",
    "section": "Grouped Verification and Scaling",
    "text": "Grouped Verification and Scaling\n\nBasic Grouping\nSo far we have just done verification stratified by lead_time. We can use the groupings argument to tell the verification function how to group the data together to compute scores. The most common grouping after lead time would be to group by the forecast cycle. Rather than read in only the forecasts initialized at 00:00 UTC, we will read in the 12:00 UTC forecasts as well.\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083112, \"12h\"),\n  fcst_model = c(\"meps\", \"ifsens\"),\n  fcst_type  = \"eps\",\n  parameter  = \"T2m\", \n  members    = 0, \n  file_path  = fcst_dir \n) |&gt; \n  as_det()\n\nfcst$arome_arctic &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083112, \"12h\"),\n  fcst_model = \"arome_arctic\",\n  fcst_type  = \"det\",\n  parameter  = \"T2m\", \n  file_path  = fcst_dir \n)\n\nfcst &lt;- common_cases(fcst)\n\nThe forecasts are in Kelvin, but it may be more useful to have them in °C. We can scale the data using scale_param(). At a minimum we need to give the function the scaling to apply and a new name for the units. By default the scaling is additive, but it can be made multiplicative by setting mult = TRUE.\n\nfcst &lt;- scale_param(fcst, -273.15, new_units = \"degC\")\n\nNow we can read the observations and join to the forecast.\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n)\n\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\nWarning: .fcst has units: degC and .join has units: K\n\n\nError: Join will not be done due to units incompatibility. You can force the join by setting force = TRUE\nOR, units imcompatibility can be fixed with the set_units(), or scale_param() functions.\n\n\nHere the join fails because the forecasts and observations do not have the same units. We therefore also need to scale the observations with scale_param(). When scaling observations, the name of the observations column also needs to be provided.\n\nobs &lt;- scale_param(obs, -273.15, new_units = \"degC\", col = T2m)\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\nNow we can verify. This time we will tell det_verify() that we want scores for each lead time and forecast cycle.\n\nverif &lt;- det_verify(\n  fcst, \n  T2m, \n  thresholds = seq(10, 20, 2.5),\n  groupings  = c(\"lead_time\", \"fcst_cycle\")  \n)\n\nplot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))\n\n\n\n\n\n\n\n\nWe now have plots for each forecast cycle, but what if we also want the combined forecast cycles as well? There are two ways we could tackle that - firstly we could compute the verification with groupings = \"lead_time\" (i.e. the default) and bind the output to what we already have with bind_point_verif().\n\nverif &lt;- bind_point_verif(\n  verif, \n  det_verify(fcst, T2m, thresholds = seq(10, 20, 2.5))\n)\n\nplot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))\n\n\n\n\n\n\n\n\n\n\nGrouping Lists\nAn easier way would be to pass a list to groupings. Each element in the list is treated as a separate verification and then they are bound together at the end.\n\nverif &lt;- det_verify(\n  fcst, \n  T2m, \n  thresholds = seq(10, 20, 2.5),\n  groupings  = list(\n    \"lead_time\",\n    c(\"lead_time\", \"fcst_cycle\")  \n  )\n)\n\nsave_point_verif(verif, here(\"data\", \"verification\", \"det\", \"fcst-cycle\"))\n\nplot_point_verif(verif, rmse, facet_by = vars(fcst_cycle))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Controlling the order of facets\n\n\n\nFacets are plotted in alphabetical order if the faceting variable is a string or coerced into a string. This means that the facets can be in an unexpected order. The fct_*() functions from the forcats package can be used to help reorder the facets. Quite often the order in which the values of the variable appear in the data is the order you want the facets to be in so fct_inorder() can be used to reorder the facets.\n\n\n\nplot_point_verif(verif, rmse, facet_by = vars(fct_inorder(fcst_cycle)))\n\n\n\n\n\n\n\n\n\n\nComplex Grouping Lists\n\nAdding a station characteristic\nIt is often desirable to group stations together by common characteristics, whether that be location, whether it’s on the coast or in the mountains or any other characteristic. harp includes a built in data frame of station groups that can be joined to the forecast and used to group the verification. We can join the station groups using join_to_fcst() with force = TRUE since we don’t want a check on common units between the data frames to be joined together.\n\nfcst &lt;- join_to_fcst(fcst, station_groups, force = TRUE)\n\nThis has added a \"station_group\" column to the forecast which we can use in the groupings argument. Now we want verification for each lead time for all forecast cycles and station groups; for each lead time and each forecast cycle for all station groups; for each lead time and each station group for all forecast cycles; and for each lead time for each station group for each forecast cycle. Therefore we need a list of 4 different groupings.\n\nverif &lt;- det_verify(\n  fcst, \n  T2m, \n  thresholds = seq(10, 20, 2.5),\n  groupings  = list(\n    \"lead_time\",\n    c(\"lead_time\", \"fcst_cycle\"),\n    c(\"lead_time\", \"station_group\"),\n    c(\"lead_time\", \"fcst_cycle\", \"station_group\")\n  ) \n)\n\nsave_point_verif(verif, here(\"data\", \"verification\", \"det\", \"stations\"))\n\nPlotting now becomes quite complicated as you have to do a lot of filtering and faceting. For example:\n\nplot_point_verif(\n  verif, \n  equitable_threat_score, \n  facet_by  = vars(station_group),\n  filter_by = vars(grepl(\";\", fcst_cycle), threshold == 15) \n)\n\n\n\n\n\n\n\n\nYou may have noticed the saving of each verification result into specific directories. This is so that they can be plotted using an interactive app that runs in a web browser. The app is made using R Shiny so we refer to it as a Shiny app. save_point_verif() uses very specific file names that describe some of the information about the verification object, but the grouping strategy is not one of those pieces of information, hence the separate directories. The Shiny app will create dropdown menus allowing you to choose the group for which you want to see the scores.\nYou can start the shiny app with:\n\nshiny_plot_point_verif(\n  start_dir           = here(\"data\", \"verification\"),\n  full_dir_navigation = FALSE, \n  theme               = \"light\"\n) \n\n\n\n\n\n\n\nTip: Shiny App Options\n\n\n\n\n\nWhen you start the shiny app it is best to give it a directory to start from to aid navigation. By setting full_dir_navigation = FALSE the use of modal windows to navigate your directory system is disabled, and all directories below the start directory are searched for harp point verification files and the Select Verfication Directory dropdown is populated - this is experimental and may not give the smoothest ride, but is often less cumbersome than navigation by modal windows. Finally you can choose between “light”, “dark” and “white” for the colour theme of the app (the default is “dark”).\n\n\n\n\n\n\nChanging the Time Axis\nYou can also use the groupings argument to specify different time axes to use for the verification. So far we have used the lead time for the time axis. However we could also use the time of day to get the diurnal cycle, or simply get the scores for each date-time in the data set. In this example we will still group by the forecast cycle as well. To get the time of day, we first need to run expand_date() on the data to get a column for \"valid_hour\".\n\nfcst &lt;- expand_date(fcst, valid_dttm)\n\nverif &lt;- det_verify(\n  fcst, \n  T2m, \n  thresholds = seq(10, 20, 2.5),\n  groupings  = list(\n    \"lead_time\",\n    c(\"lead_time\", \"fcst_cycle\"),\n    \"valid_hour\",\n    c(\"valid_hour\", \"fcst_cycle\"),\n    \"valid_dttm\",\n    c(\"valid_dttm\", \"fcst_cycle\")\n  ) \n)\n\n\n\n\n\n\n\nNote: A necessary hack\n\n\n\n\n\nSince the data are 6 hourly there are only 4 valid hours in the data set. When there are fewer than five different values in group the verification functions separate them with a “;” in the group value when they are all together, otherwise they are labelled as “All”. plot_point_verif() only searches for “All” when figuring out which data to get for each different x-axis.\n\n\n\nWe need to use mutate_list() in conjunction with case_when() from the dplyr package to modify the \"valid_hour\" column where all valid hours are collected together. mutate_list() use mutate() from the dplyr package to modify columns of data frames in a list whilst retaining the attributes of the list.\n\nverif &lt;- mutate_list(\n  verif, \n  valid_hour = case_when(\n    grepl(\";\", valid_hour) ~ \"All\",\n    .default = valid_hour\n  )\n)\n\nsave_point_verif(verif, here(\"data\", \"verification\", \"det\", \"fcst-cycle\"))\n\nWe can now use the x-axis argument to plot_point_verif() to decide which times to use on the x-axis.\n\nplot_point_verif(verif, mae, facet_by = vars(fct_inorder(fcst_cycle)))\n\n\n\n\n\n\n\nplot_point_verif(\n  verif, \n  mae, \n  x_axis   = valid_hour, \n  facet_by = vars(fct_inorder(fcst_cycle))\n)\n\n\n\n\n\n\n\nplot_point_verif(\n  verif, \n  mae, \n  x_axis     = valid_dttm, \n  facet_by   = vars(fct_inorder(fcst_cycle)),\n  point_size = 0 \n)"
  },
  {
    "objectID": "point-verif-workflow.html#vertical-profiles",
    "href": "point-verif-workflow.html#vertical-profiles",
    "title": "Point Verifcation Workflow",
    "section": "Vertical Profiles",
    "text": "Vertical Profiles\nAnother application where grouping is important in the verification of vertical profiles. In general the workflow is once again the same, but there are some aspects where you need to take into account that the data are on vertical levels.\nThe first difference is that when reading in the data, you need to tell the read function what vertical coordinate the data are on via the vertical_coordinate argument. In most cases the data will be on pressure levels, but they could also be on height levels or model levels.\nThe data we are using here come from the AROME-Arctic model and the control member of MEPS, which at MET Norway is archived as MEPS_det (i.e. MEPS deterministic). There are forecasts available every 6 hours at 00-, 06-, 12- and 18-UTC.\n\nfcst &lt;- read_point_forecast(\n  dttm                = seq_dttm(2023080100, 2023083118, \"6h\"),\n  fcst_model          = c(\"meps\", \"arome_arctic\"),\n  fcst_type           = \"det\",\n  parameter           = \"T\",\n  file_path           = fcst_dir,\n  vertical_coordinate = \"pressure\"\n) |&gt; \n  scale_param(-273.15, \"degC\")\n\nWhen finding the common cases, the default behaviour is to compare the \"fcst_dttm\", \"lead_time\" and \"SID\" columns. When finding common cases for vertical profiles we also need to make sure that only the vertical levels that are common to all forecast models are included in the verification. We do this by adding the pressure column (p) to common_cases().\n\nfcst &lt;- common_cases(fcst, p)\n\nSimilar to reading the forecasts, we also need to tell read_point_obs that the vertical coordinate is pressure.\n\nobs &lt;- read_point_obs(\n  dttm                = unique_valid_dttm(fcst),\n  parameter           = \"T\", \n  stations            = unique_stations(fcst),\n  obs_path            = obs_dir,\n  vertical_coordinate = \"pressure\"\n) |&gt; \n  scale_param(-273.15, \"degC\", col = T)\n\nJoining works exactly the same as for single level variables.\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\nNow we can verify, making sure that we have \"p\" as one of the grouping variables.\n\nverif &lt;- det_verify(\n  fcst, \n  T, \n  groupings = list(\n    c(\"lead_time\", \"p\"),\n    c(\"lead_time\", \"p\", \"fcst_cycle\")\n  )\n)\n\nsave_point_verif(\n  verif, \n  here(\"data\", \"verification\", \"det\", \"fcst-cycle\")\n)\n\nWe can now plot the profile verification using plot_profile_verif() making sure to filter and facet appropriately (for example, there is one profile for each lead time).\n\nplot_profile_verif(\n  verif, \n  mae, \n  filter_by = vars(grepl(\";\", fcst_cycle)),\n  facet_by  = vars(lead_time)\n)\n\n\n\n\n\n\n\n\nWe could also make a plot for a single vertical level in the normal way. We may also want to remove times when there are very few cases.\n\nplot_point_verif(\n  verif, \n  bias, \n  filter_by = vars(p == 925, num_cases &gt; 5),\n  facet_by  = vars(fct_inorder(fcst_cycle)) \n)"
  },
  {
    "objectID": "point-verif-workflow.html#conditional-verification",
    "href": "point-verif-workflow.html#conditional-verification",
    "title": "Point Verifcation Workflow",
    "section": "Conditional Verification",
    "text": "Conditional Verification\n\nClassification by observed value\nOn occasion, it may be instructive to verify for particular conditions. For example, to verify temperature for different temperature ranges. This can be done by creating a grouping column for each range of observed temperature. Here the workflow would be to use mutate() from the dplyr package in association with the base R function cut() to create a grouping column on the observations before joining to the forecast.\n\nfcst &lt;- read_point_forecast(\n  dttm       = seq_dttm(2023080100, 2023083100, \"24h\"),\n  fcst_model = c(\"meps\", \"ifsens\"),\n  fcst_type  = \"eps\",\n  parameter  = \"T2m\",\n  file_path  = fcst_dir\n) |&gt; \n  scale_param(-273.15, \"degC\") |&gt; \n  common_cases()\n\nobs &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"T2m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n) |&gt; \n  scale_param(-273.15, \"degC\", col = T2m)\n\nWe are going to classify the temperature by having the left side of each range open and right side closed. This basically means that each range goes from &gt;= the lower value to &lt; the upper value. We do this by setting right = FALSE in the call to cut().\n\nobs &lt;- mutate(\n  obs, \n  temp_range = cut(T2m, seq(5, 25, 2.5), right = FALSE)\n)\n\nobs\n\n# A tibble: 23,756 × 8\n   valid_dttm            SID   lon   lat  elev units   T2m temp_range\n   &lt;dttm&gt;              &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n 1 2023-08-01 00:00:00  1001 -8.67  70.9    10 degC   6.75 [5,7.5)   \n 2 2023-08-01 00:00:00  1010 16.1   69.3    13 degC  14.4  [12.5,15) \n 3 2023-08-01 00:00:00  1015 17.8   69.6    33 degC  18.6  [17.5,20) \n 4 2023-08-01 00:00:00  1023 18.5   69.1    77 degC  13.1  [12.5,15) \n 5 2023-08-01 00:00:00  1025 18.9   69.7     9 degC  13.0  [12.5,15) \n 6 2023-08-01 00:00:00  1026 18.9   69.7   115 degC  14.2  [12.5,15) \n 7 2023-08-01 00:00:00  1027 18.9   69.7    20 degC  13.0  [12.5,15) \n 8 2023-08-01 00:00:00  1028 19.0   74.5    16 degC   4.55 &lt;NA&gt;      \n 9 2023-08-01 00:00:00  1033 19.5   70.2    24 degC  14.5  [12.5,15) \n10 2023-08-01 00:00:00  1035 20.1   69.6   710 degC  12.9  [12.5,15) \n# ℹ 23,746 more rows\n\n\nYou will see that cut() labels values outside of the breaks as NA. This isn’t necessarily meaningful, so we will use the dplyr function case_when() to give more meaningful labels. We will also take the opportunity to format the other ranges a bit better.\n\nobs &lt;- mutate(\n  obs, \n  temp_range = case_when(\n    T2m &lt;  5  ~ \"&lt; 5\",\n    T2m &gt;= 25 ~ \"&gt;= 25\",\n    .default = gsub(\",\", \", \", temp_range)\n  ), \n  temp_range = fct_relevel(\n    temp_range, \n    c(\n      \"&lt; 5\", \n      gsub(\n        \",\", \n        \", \", \n        levels(cut(seq(5, 25), seq(5, 25, 2.5), right = FALSE))\n      ), \n      \"&gt;= 25\"\n    )\n  )\n)\n\nIn the above we also set the factor levels to an order that makes sense for when we come to plot the scores. We can now continue as normal, adding “temp_range\" as a grouping variable.\n\nfcst &lt;- join_to_fcst(fcst, obs)\n\nverif &lt;- ens_verify(\n  fcst, \n  T2m, \n  groupings = c(\"leadtime\", \"temp_range\")\n)\n\nplot_point_verif(\n  verif, \n  spread_skill, \n  facet_by = vars(temp_range)  \n)\n\n\n\n\n\n\n\n\n\n\nClassification by a different parameter\nOne important aspect of forecast model performance is how it performs in different weather regimes. For example, it may be useful to know how the model performs for a particular parameter when the wind is coming from a certain direction. There are a couple of approaches to this, but it must be noted that it is important to include the possibility for false alarms in the verification.\nHere we will compute the verification scores for an ensemble forecast for 2m temperature when the wind is coming from the east, so let’s say a wind direction between 45° and 135°. We will first read in the wind direction observations for the cases we already have for 2m temperature.\n\nwind_dir &lt;- read_point_obs(\n  dttm      = unique_valid_dttm(fcst),\n  parameter = \"D10m\",\n  stations  = unique_stations(fcst),\n  obs_path  = obs_dir \n)\n\nWe will now create a column to have the groups “westerly” and “other” based on the values in the \"D10m\" column.\n\nwind_dir &lt;- mutate(\n  wind_dir, \n  wind_direction = case_when(\n    between(D10m, 45, 135) ~ \"easterly\",\n    .default = \"other\"\n  )\n)\n\nWe can now join the wind direction data to the 2m temperature data that we want to verify. Here we have to set force = TRUE in join_to_fcst() since it will fail the check for the forecasts and observations having the same units.\n\nfcst &lt;- join_to_fcst(fcst, wind_dir, force = TRUE)\n\nWe can now verify 2m temperature and group by the wind direction.\n\nverif &lt;- ens_verify(\n  fcst, \n  T2m, \n  groupings = c(\"lead_time\", \"wind_direction\")\n)\n\nplot_point_verif(\n  verif, \n  mean_bias, \n  facet_by = vars(wind_direction)\n)\n\n\n\n\n\n\n\n\nA more thorough method might be to include whether easterly winds were forecast or not in the analysis, such that we have cases of westerly being observed only, forecasted only, both observed and forecasted, and neither observed nor forecasted.\nSo now we need read in the wind direction forecasts as well. In this case we need to set the units to “degrees” as they are incorrect in the file (due to a bug :/ ).\n\nfcst_wind_dir &lt;- read_point_forecast(\n  dttm       = unique_fcst_dttm(fcst),\n  fcst_model = c(\"meps\", \"ifsens\"), \n  fcst_type  = \"eps\",\n  parameter  = \"wd10m\",\n  lead_time  = unique_col(fcst, lead_time), \n  stations   = unique_stations(fcst),\n  file_path  = fcst_dir \n) |&gt; \n  set_units(\"degrees\")\n\nSince the forecasts are from ensemble, we could identify forecasts that have westerly winds as those where at least one member has a wind direction between 45° and and 135° To identify these cases we can find the binary probability for each member and then compute the ensemble mean with ens_stats(). We can apply a function to multiple columns using across() from the dplyr package together with mutate(). Since we are generating logical values, we need to tell ens_stats() not to compute the standard deviation.\n\nfcst_wind_dir &lt;- mutate(\n  fcst_wind_dir,\n  across(contains(\"_mbr\"), ~between(.x, 45, 135))\n) |&gt; \n  ens_stats(sd = FALSE)\n\nNow we can join the wind direction observations, generate our groups and select only those columns we need.\n\nfcst_wind_dir &lt;- join_to_fcst(fcst_wind_dir, wind_dir)\n\nfcst_wind_dir &lt;- mutate(\n  fcst_wind_dir, \n  easterly = case_when(\n    ens_mean  &gt; 0 & wind_direction == \"easterly\" ~ \"both\",\n    ens_mean  &gt; 0 & wind_direction == \"other\" ~ \"forecast only\",\n    ens_mean == 0 & wind_direction == \"easterly\" ~ \"observed only\",\n    ens_mean == 0 & wind_direction == \"other\" ~ \"neither\", \n    .default = NA\n  )\n) |&gt; \n  select(fcst_dttm, lead_time, SID, easterly)\n\nWe can now join our data frame with the verification groups to the 2m temperature forecasts to be verified. Since we have two models, we can merge them into a single data frame using bind().\n\nfcst &lt;- join_to_fcst(\n  fcst, \n  bind(fcst_wind_dir), \n  force = TRUE\n)\n\nNow we can verify and plot for our new groups. fct_relevel() is used to get the plot facets in the desired order.\n\nverif &lt;- ens_verify(\n  fcst, \n  T2m,\n  groupings = c(\"lead_time\", \"easterly\")\n)\n\nplot_point_verif(\n  verif, \n  mean_bias, \n  facet_by       = vars(fct_relevel(easterly, ~.x[c(2, 4, 1, 3)])),\n  num_facet_cols = 2\n)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "build-verif-script.html",
    "href": "build-verif-script.html",
    "title": "Build a Point Verification Script",
    "section": "",
    "text": "In this tutorial we are going to build a basic script that could be used to run verification tasks in a production environment."
  },
  {
    "objectID": "build-verif-script.html#basic-skeleton",
    "href": "build-verif-script.html#basic-skeleton",
    "title": "Build a Point Verification Script",
    "section": "Basic Skeleton",
    "text": "Basic Skeleton\nWe will start our script from a skeleton of comments that describe the basic point verification workflow, and then populate it with the variable assignments and functions. First this is the basic workflow.\n\n# Attach libraries\n\n# Read Forecasts\n\n# Scale\n\n# Select Common cases\n\n# Read observations\n\n# Join\n\n# Observation errors\n\n# Verify \n\n# Save\n\nWe can now add the function calls for each section\n\n# Attach libraries\nlibrary(harp)\nlibrary(here)\n\n# Read Forecasts\nfcst &lt;- read_point_forecast(\n  dttm        = seq_dttm(...),\n  fcst_model  = c(\"...\"),\n  fcst_type   = \"det\", \n  parameter   = \"...\",\n  lead_time   = seq(...),\n  file_path   = \"...\"\n)\n\n# Scale\nfcst &lt;- scale_param(fcst, ..., \"...\")\n\n# Select Common cases\nfcst &lt;- common_cases(fcst)\n\n# Read observations\nobs &lt;- read_point_obs(\n  dttm        = unique_valid_dttm(fcst),\n  parameter   = \"...\",\n  stations    = unique_stations(fcst),\n  obs_path    = \"...\",\n  min_allowed = ...,\n  max_allowed = ...\n)\n\n# Scale\nobs &lt;- scale_param(obs, ..., \"...\", col = ...)\n\n# Join\nfcst &lt;- join_to_fcst(fcst, obs)\n\n# Observation errors\nfcst &lt;- check_obs_against_fcst(fcst, ..., num_sd_allowed = ...)\n\n# Verify\nverif &lt;- det_verify(fcst, ..., thresholds = ..., groupings = c(\"...\"))\n\n# Save\nsave_point_verif(verif, \"...\")\n\nEverything we have marked with ... is essentially a variable that we can set at the beginning of the script. So we can assign those variables and test the first iteration of the script.\n\n# Attach libraries\nlibrary(harp)\nlibrary(here)\n\n# Paths\nfcst_dir    &lt;- here(\"data\", \"FCTABLE\")\nobs_dir     &lt;- here(\"data\", \"OBSTABLE\")\nverif_dir   &lt;- here(\"data\", \"verification\")\n\n# Parameters\nprm         &lt;- \"T2m\"\n\n# Forecast variables\ndate_times  &lt;- seq_dttm(2023080100, 2023083118, \"6h\")\nfcst_models &lt;- \"arome_arctic\"\nlt          &lt;- seq(0, 24, 3)\nfc_scaling  &lt;- -273.15\nfc_units    &lt;- \"degC\"\n\n# Obs variables\nobs_scaling &lt;- -273.15\nobs_units   &lt;- \"degC\"\nmin_obs     &lt;- 223\nmax_obs     &lt;- 333\nerror_sd    &lt;- 4\n\n# Verif variables\nthresh      &lt;- seq(-5, 25, 5)\ngrps        &lt;- list(\n  \"lead_time\", \n  c(\"lead_time\", \"fcst_cycle\")\n)\n\n# Read Forecasts\nfcst &lt;- read_point_forecast(\n  dttm        = date_times,\n  fcst_model  = fcst_models,\n  fcst_type   = \"det\", \n  parameter   = prm,\n  lead_time   = lt,\n  file_path   = fcst_dir\n)\n\n# Scale\nfcst &lt;- scale_param(fcst, fc_scaling, fc_units)\n\n# Select Common cases\nfcst &lt;- common_cases(fcst)\n\n# Read observations\nobs &lt;- read_point_obs(\n  dttm        = unique_valid_dttm(fcst),\n  parameter   = prm,\n  stations    = unique_stations(fcst),\n  obs_path    = obs_dir,\n  min_allowed = min_obs,\n  max_allowed = max_obs\n)\n\n# Scale\nobs &lt;- scale_param(obs, obs_scaling, obs_units, col = prm)\n\nError in `[[&lt;-`:\n! Assigned data `op(x[[col_name]], scaling)` must be compatible with\n  existing data.\n✖ Existing data has 48353 rows.\n✖ Assigned data has 0 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 0 to size 48353.\n\n# Join\nfcst &lt;- join_to_fcst(fcst, obs)\n\nError: Join will not be done due to units incompatibility. You can force the join by setting force = TRUE\nOR, units imcompatibility can be fixed with the set_units(), or scale_param() functions.\n\n# Observation errors\nfcst &lt;- check_obs_against_fcst(fcst, prm, num_sd_allowed = error_sd)\n\nError in `check_obs_against_fcst()`:\n✖ `parameter = prm` not found in data.\nℹ `.fcst` does not include column `prm`.\n\n# Verify\nverif &lt;- det_verify(fcst, prm, thresholds = thresh, groupings = grps)\n\nError: No column found for prm\n\n# Save\nsave_point_verif(verif, verif_dir)\n\nError in eval(expr, envir, enclos): object 'verif' not found\n\n\nWe have now run into our first problem as an error has occurred with scale_param() for the observations data. The error message isn’t that instructive (unfortunately). It is caused because many harp functions use Non Standard Evaluation (NSE)."
  },
  {
    "objectID": "build-verif-script.html#non-standard-evaluation",
    "href": "build-verif-script.html#non-standard-evaluation",
    "title": "Build a Point Verification Script",
    "section": "Non Standard Evaluation",
    "text": "Non Standard Evaluation\nharp takes a lot of inspiration from the tidyverse - a collection of R packages designed to provide a consistent way of working with data in R. Some of the functions that we have already seen, such as mutate(), are part of the tidyverse. These functions are designed to make working interactively smooth, and one aspect of this is passing arguments that are unquoted. This applies to the names of columns in data frames and harp follows suit. While working interactively is easier and more intuitive with NSE, it comes at the expense of making programming a little bit trickier.\nHowever, the solution is straightforward. When passing the name of a column to a function as a variable rather than the name itself it needs to be embraced. That is to say wrapped in double curly braces {{ }}. In the case of harp functions, this is often where the parameter name is passed to a function to identify the column that contains observations. We can now modify our basic script to embrace arguments where appropriate.\n\n# Attach libraries\nlibrary(harp)\nlibrary(here)\n\n# Paths\nfcst_dir    &lt;- here(\"data\", \"FCTABLE\")\nobs_dir     &lt;- here(\"data\", \"OBSTABLE\")\nverif_dir   &lt;- here(\"data\", \"verification\")\n\n# Parameters\nprm         &lt;- \"T2m\"\n\n# Forecast variables\ndate_times  &lt;- seq_dttm(2023080100, 2023083118, \"6h\")\nfcst_models &lt;- \"arome_arctic\"\nlt          &lt;- seq(0, 24, 3)\nfc_scaling  &lt;- -273.15\nfc_units    &lt;- \"degC\"\n\n# Obs variables\nobs_scaling &lt;- -273.15\nobs_units   &lt;- \"degC\"\nmin_obs     &lt;- 223\nmax_obs     &lt;- 333\nerror_sd    &lt;- 4\n\n# Verif variables\nthresh      &lt;- seq(-5, 25, 2.5)\ngrps        &lt;- list(\n  \"lead_time\", \n  c(\"lead_time\", \"fcst_cycle\")\n)\n\n# Read Forecasts\nfcst &lt;- read_point_forecast(\n  dttm        = date_times,\n  fcst_model  = fcst_models,\n  fcst_type   = \"det\", \n  parameter   = prm,\n  lead_time   = lt,\n  file_path   = fcst_dir\n)\n\n# Scale\nfcst &lt;- scale_param(fcst, fc_scaling, fc_units)\n\n# Select Common cases\nfcst &lt;- common_cases(fcst)\n\n# Read observations\nobs &lt;- read_point_obs(\n  dttm        = unique_valid_dttm(fcst),\n  parameter   = prm,\n  stations    = unique_stations(fcst),\n  obs_path    = obs_dir,\n  min_allowed = min_obs,\n  max_allowed = max_obs\n)\n\n# Scale\nobs &lt;- scale_param(obs, obs_scaling, obs_units, col = {{prm}})\n\n# Join\nfcst &lt;- join_to_fcst(fcst, obs)\n\n# Observation errors\nfcst &lt;- check_obs_against_fcst(fcst, {{prm}}, num_sd_allowed = error_sd)\n\n# Verify\nverif &lt;- det_verify(fcst, {{prm}}, thresholds = thresh, groupings = grps)\n\n# Save\nsave_point_verif(verif, verif_dir)\n\nWe now have the beginnings of a verification script. But what if we want to do verification for different parameters. One approach would be for the user to simply change the variable assignments at the top of the script for each use. However, this still requires a lot of manual work. A more efficient approach would be to set up the script to handle multiple parameters."
  },
  {
    "objectID": "build-verif-script.html#script-for-multiple-parameters",
    "href": "build-verif-script.html#script-for-multiple-parameters",
    "title": "Build a Point Verification Script",
    "section": "Script for multiple parameters",
    "text": "Script for multiple parameters\nWhen dealing with multiple parameters, each parameter may require different scalings, different thresholds, different error settings, different parameter names for forecasts and observations, or possibly different groupings. This will require a little more setting up. One approach may be to set some defaults and then some specific settings for different parameters. Lists in R provide an excellent mechanism for doing this.\nLet’s now make the parameter part of our script more descriptive for multiple parameters.\n\n# Attach libraries\nlibrary(harp)\nlibrary(here)\n\n# Date times\nstart_dttm  &lt;- 2023080100\nend_dttm    &lt;- 2023083118\ndttm_step   &lt;- \"6h\"\n\n# Forecast variables\ndate_times  &lt;- seq_dttm(start_dttm, end_dttm, dttm_step)\nfcst_models &lt;- \"arome_arctic\"\nlt          &lt;- seq(0, 24, 3)\n\n# Paths\nfcst_dir    &lt;- here(\"data\", \"FCTABLE\")\nobs_dir     &lt;- here(\"data\", \"OBSTABLE\")\nverif_dir   &lt;- here(\"data\", \"verification\")\n\ndefaults &lt;- list(\n  grps = list(\n    \"lead_time\",\n    c(\"lead_time\", \"fcst_cycle\")\n  ),\n  error_sd = 6\n)\n\n# Parameters\nparams &lt;- list(\n  \n  T2m = list(\n    fc_scaling  = list(\n      scaling   = -273.15,\n      new_units = \"degC\"\n    ), \n    obs_scaling = list(\n      scaling   = -273.15,\n      new_units = \"degC\"\n    ),\n    min_obs     = 223,\n    max_obs     = 333,\n    error_sd    = 4,\n    thresh      = seq(-5, 25, 5)\n  ),\n  \n  S10m = list(\n    fc_param    = \"ws10m\",\n    min_obs     = 0,\n    max_obs     = 100,\n    thresh      = c(1, 2, 3, 5, 7.5, 10, 15, 20, 25)\n  )\n)\n\n# Loop over parameters\nfor (prm in names(params)) {\n  \n  fc_prm &lt;- params[[prm]]$fc_param\n  if (is.null(fc_prm)) {\n    fc_prm &lt;- prm\n  }\n  \n  # Read Forecasts\n  fcst &lt;- read_point_forecast(\n    dttm        = date_times,\n    fcst_model  = fcst_models,\n    fcst_type   = \"det\", \n    parameter   = fc_prm,\n    lead_time   = lt,\n    file_path   = fcst_dir\n  )\n  \n  # Scale\n  if (!is.null(params[[prm]]$fc_scaling)) {\n    fcst &lt;- do.call(\n      scale_param, \n      c(list(x = fcst), params[[prm]]$fc_scaling)\n    )\n  }\n  \n  # Select Common cases\n  fcst &lt;- common_cases(fcst)\n  \n  # Read observations\n  obs_prm &lt;- params[[prm]]$obs_param\n  if (is.null(obs_prm)) {\n    obs_prm &lt;- prm\n  }\n    \n  obs &lt;- read_point_obs(\n    dttm        = unique_valid_dttm(fcst),\n    parameter   = obs_prm,\n    stations    = unique_stations(fcst),\n    obs_path    = obs_dir,\n    min_allowed = params[[prm]]$min_obs,\n    max_allowed = params[[prm]]$max_obs\n  )\n  \n  # Scale\n  if (!is.null(params[[prm]]$obs_scaling)) { \n    obs &lt;- do.call(\n      scale_param, \n      c(\n        list(x = obs), \n        params[[prm]]$obs_scaling, \n        list(col = {{obs_prm}})\n      )\n    )\n  }\n  \n  # Join\n  fcst &lt;- join_to_fcst(fcst, obs)\n  \n  # Observation errors\n  error_sd &lt;- params[[prm]]$error_sd\n  if (is.null(error_sd)) {\n    error_sd &lt;- defaults$error_sd\n  }\n  \n  fcst &lt;- check_obs_against_fcst(\n    fcst, \n    {{obs_prm}}, \n    num_sd_allowed = error_sd\n  )\n  \n  # Verify\n  grps &lt;- params[[prm]]$grps\n  if (is.null(grps)) {\n    grps &lt;- defaults$grps\n  }\n  \n  thresh &lt;- params[[prm]]$thresh\n  \n  verif &lt;- det_verify(\n    fcst, \n    {{obs_prm}}, \n    thresholds = thresh, \n    groupings  = grps\n  )\n  \n  # Save\n  save_point_verif(verif, verif_dir)\n  \n}"
  },
  {
    "objectID": "build-verif-script.html#function-instead-of-loop",
    "href": "build-verif-script.html#function-instead-of-loop",
    "title": "Build a Point Verification Script",
    "section": "Function instead of loop",
    "text": "Function instead of loop\nR is designed to be a functional programming language. One of the advantages of this is that a function can be called repeatedly with different arguments making for (hopefully!) more structured and repeatable programs. If a function fails, it is also much easier to move onto the next iteration.\nLet’s rewrite the contents of our loop as a function that takes some arguments.\n\nrun_verif &lt;- function(\n  param_list,\n  param_name,\n  fc_models,\n  dttm,\n  ld_times,\n  fc_data_dir,\n  obs_data_dir,\n  dflts,\n  vrf_data_dir\n) {\n  \n  fc_prm &lt;- param_list$fc_param\n  if (is.null(fc_prm)) {\n    fc_prm &lt;- param_name\n  }\n  \n  # Read Forecasts\n  fcst &lt;- read_point_forecast(\n    dttm        = dttm,\n    fcst_model  = fc_models,\n    fcst_type   = \"det\", \n    parameter   = fc_prm,\n    lead_time   = ld_times,\n    file_path   = fc_data_dir\n  )\n  \n  # Scale\n  if (!is.null(param_list$fc_scaling)) {\n    fcst &lt;- do.call(\n      scale_param, \n      c(list(x = fcst), param_list$fc_scaling)\n    )\n  }\n  \n  # Select Common cases\n  fcst &lt;- common_cases(fcst)\n  \n  # Read observations\n  obs_prm &lt;- param_list$obs_param\n  if (is.null(obs_prm)) {\n    obs_prm &lt;- param_name\n  }\n    \n  obs &lt;- read_point_obs(\n    dttm        = unique_valid_dttm(fcst),\n    parameter   = obs_prm,\n    stations    = unique_stations(fcst),\n    obs_path    = obs_dir,\n    min_allowed = param_list$min_obs,\n    max_allowed = param_list$max_obs\n  )\n  \n  # Scale\n  if (!is.null(param_list$obs_scaling)) { \n    obs &lt;- do.call(\n      scale_param, \n      c(\n        list(x = obs), \n        param_list$obs_scaling, \n        list(col = {{obs_prm}})\n      )\n    )\n  }\n  \n  # Join\n  fcst &lt;- join_to_fcst(fcst, obs)\n  \n  # Observation errors\n  error_sd &lt;- param_list$error_sd\n  if (is.null(error_sd)) {\n    error_sd &lt;- dflts$error_sd\n  }\n  \n  fcst &lt;- check_obs_against_fcst(\n    fcst, \n    {{obs_prm}}, \n    num_sd_allowed = error_sd\n  )\n  \n  # Verify\n  grps &lt;- param_list$grps\n  if (is.null(grps)) {\n    grps &lt;- dflts$grps\n  }\n  \n  thresh &lt;- param_list$thresh\n  \n  verif &lt;- det_verify(\n    fcst, \n    {{obs_prm}}, \n    thresholds = thresh, \n    groupings  = grps\n  )\n  \n  # Save\n  save_point_verif(verif, vrf_data_dir)\n  \n}\n\nNow instead of using a for loop we can use iwalk() from the purrr package. iwalk() calls a function for each element in the list that it is given. The first argument to that function is the list element and the second argument is the element name. iwalk() only calls the function for its side effects (e.g. writing a file) and returns the input untouched. Here we will call an anonymous function that calls run_verif().\n\nlibrary(purrr)\niwalk(\n  params,\n  \\(x, y) run_verif(\n    param_list   = x, \n    param_name   = y, \n    fc_models    = fcst_models,\n    dttm         = date_times,\n    ld_times     = lt,\n    fc_data_dir  = fcst_dir,\n    obs_data_dir = obs_dir,\n    dflts        = defaults,\n    vrf_data_dir = verif_dir\n  )\n)\n\nUsing this approach, the run_verif() function can be modified to do any manipulations of the data that the user can think of, or using lists in combination with defaults can allow the user to provide specific information without having to edit the run_verif() function."
  },
  {
    "objectID": "plot-manipulate-spatial.html",
    "href": "plot-manipulate-spatial.html",
    "title": "Plotting and manipulating spatial data",
    "section": "",
    "text": "In this tutorial, we will go through some of harp’s plotting functions for gridded data with most of our focus on harp’s ggplot geoms geom_georaster(), geom_geocontour() and geom_geocontour_filled(). We will also explore the geo_* family of functions for doing geographic transformations of georeferenced grids.\nWe are also going to make use of the scico package for some nice colour palettes. If you don’t have it, you can install it with\ninstall.packages(\"scico\")"
  },
  {
    "objectID": "plot-manipulate-spatial.html#simple-plotting-of-2d-fields",
    "href": "plot-manipulate-spatial.html#simple-plotting-of-2d-fields",
    "title": "Plotting and manipulating spatial data",
    "section": "Simple plotting of 2d fields",
    "text": "Simple plotting of 2d fields\nFor a quick look at a geofield the function plot_field() can be used. If you just want to see what domain the data are on, you can use plot_domain().\n\nlibrary(harp)\nlibrary(here)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(scico)\n\nWe’re going to be reading some data from MET Norway’s archive of IFSENS data (for a cutout over Ireland). This file requires some special options for reading the NetCDF that we can saved to a variable.\n\nopts &lt;- netcdf_opts(\"met_norway_ifsens\")\n\nt2m &lt;- read_grid(\n  here(\"data/netcdf/ifsens/ifsens_20240219T000000Z.nc\"),\n  \"t2m\",\n  lead_time = 6, \n  members = 1, \n  file_format_opts = opts\n)\n\nFor plot_field() the title is derived from some attributes of the geofield, some of which aren’t always there.\n\nplot_field(t2m)\n\n\n\n\nWe can do some simple things like change the colour palette, change the breaks and chamnge the title. Here we could use one of the scico colour palettes. You can check them out with\n\nscico_palette_show()\n\n\n\n\n\n\n\n\nIt’s not really the correct colour palette for these data, but when in Ireland…\n\nplot_field(\n  t2m, \n  palette = scico(256, palette = \"cork\"), \n  breaks  = seq(275, 290, 1.5), \n  title = \"2m temperature over Ireland\"\n)\n\n\n\n\n\n\n\n\nTo just plot the domain without any data, you can use plot_domain()\n\nplot_domain(t2m)"
  },
  {
    "objectID": "plot-manipulate-spatial.html#plotting-2d-fields-with-ggplot",
    "href": "plot-manipulate-spatial.html#plotting-2d-fields-with-ggplot",
    "title": "Plotting and manipulating spatial data",
    "section": "Plotting 2d fields with ggplot",
    "text": "Plotting 2d fields with ggplot\nIt is also possible to plot 2d fields with ggplot. Here we will use harp specific geoms geom_georaster(), geom_geocontour() and geom_geocontour_filled(). For an explanation of what geoms are and a comprehensive introduction to ggplot as well as the terminology used in building plots with ggplot, you are referred to the Visualize chapter of R for Data Science.\nggplot always requires the data to be in a data frame, and uses the aes() function to map data frame columns to aesthetics of the plot. That is to say, which columns affect how a particular geom is drawn. Plot geoms typically have some required aesthetics and optional aesthetics. In the case of the geom_geo functions a geolist column is always required. This is probably best illustrated with an example. We will read all of the data for 2m temperature in as a data frame using read_forecast(). A quick way to do this if you know the file name is to set the file_template as the file name.\n\nt2m &lt;- read_forecast(\n  20240219,\n  \"ifsens\",\n  \"t2m\",\n  file_path        = here(\"data\", \"netcdf\", \"ifsens\"),\n  file_template    = \"ifsens_20240219T000000Z.nc\",\n  file_format_opts = opts,\n  return_data      = TRUE \n) |&gt; \n  scale_param(-273.15, \"degC\")\n\nWe can now try and plot the data for member 0 using ggplot() and geom_georaster().\n\nggplot(t2m, aes(geofield = ifsens_mbr000)) + \n  geom_georaster()\n\nWarning: Computation failed in `stat_georaster()`.\nCaused by error in `compute_panel()`:\n! More than one geofield in data. Use facet_wrap or facet_grid, or filter data before plotting.\n\n\n\n\n\nHere ggplot() doesn’t know how to print multiple geofields unless you tell it how to do so. Let’s make things a little easier by filtering the data to only be for lead_time = 6, and extracting member 0 as deterministic.\n\ntt &lt;- filter(as_det(t2m, 0), lead_time == 6)\n\nSo since it is now deterministic, the column we want to plot is fcst.\n\nggplot(tt, aes(geofield = fcst)) + \n  geom_georaster()\n\n\n\n\n\n\n\n\nThere are a few ways we can improve the plot. Firstly for these data we probably don’t want the expansion zone around the data in the plot panel. We may also want (although this is lat-lon so might not be sensible) to set equal coordinate aspect ratio. We can do all of this with coord_equal(expand = FALSE).\n\nggplot(tt, aes(geofield = fcst)) + \n  geom_georaster() +\n  coord_equal(expand = FALSE)\n\n\n\n\n\n\n\n\nIn addition, the tick marks and axis labels aren’t actually much use here, so we can get rid of those with theme_harp_map()\n\nggplot(tt, aes(geofield = fcst)) + \n  geom_georaster() +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nIt might also be useful to have a map outline. We can get a map with get_map() telling it which column for which to get the domain for the map. We will also set polygon = FALSE so we only get paths.\n\nmap &lt;- get_map(tt, col = fcst, polygon = FALSE)\n\nWe can now add the map to the plot with geom_path(). Since the geoms use different data and aesthetics we will set those locally for each geom rather than globally for the plot.\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Higher resolution maps\n\n\n\n\n\nWe can get higher resolution maps with the rnaturalearth packages, which can be installed with\n\ninstall.packages(\"rnaturalearth\")\ninstall.packages(\"rnaturalearthdata\")\nremotes::install_github(\"ropensci/rnaturalearthhires\")\n\nTo use these packages with get_map() we need a development version of the maps package.\n\nremotes::install_github(\"adeckmyn/maps\", \"sf\")\n\nWe can then get a high resolution map with (for example)\n\nmap &lt;- get_map(\n  tt, col = fcst, map = rnaturalearthhires::countries10, polygon = FALSE\n)\n\n\n\n\n\nColour scales\nWe could also change the colour scale. Since this we can use the scale_fill_gradient() functions, scale_fill_viridis_c(), scale_fill_distiller(), or scale_fill_scico(). Here we will demonstrate some of these functions.\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradient(low = \"yellow\", high = \"red\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 6) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradientn(colours = heat.colors(256)) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_viridis_c(option = \"B\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_distiller(palette = \"PuBuGn\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_scico(palette = \"hawaii\", direction = -1) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nAs you can see, there are many choices of colour schemes. You could also bin the colours.\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_steps(low = \"white\", high = \"darkred\", breaks = seq(4, 12)) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nA final thing you might want to do is add a more meaningful title for the legend - it’s always z due to how the geom functions turn the geofield data into a data frame.\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tt) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_binned(\n    low    = \"seagreen3\", \n    high   = \"yellow\", \n    breaks = seq(4, 12)\n  ) +\n  labs(fill = bquote(\"[\"*degree*C*\"]\")) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nWe can now bring all of our data back and make a faceted plot\n\nggplot() + \n  geom_georaster(aes(geofield = ifsens_mbr000), t2m) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_binned(\n    low    = \"seagreen3\", \n    high   = \"yellow\", \n    breaks = seq(4, 12)\n  ) +\n  facet_wrap(~valid_dttm) +\n  labs(fill = bquote(\"[\"*degree*C*\"]\")) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nThere are maybe some other techniques we would use for other parameters. We might want to show an anomaly, like for example the difference of a member from the control.\n\nggplot() + \n  geom_georaster(aes(geofield = ifsens_mbr001 - ifsens_mbr000), t2m) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradient2(midpoint = 0) +\n  facet_wrap(~fct_reorder(paste0(\"T + \", lead_time, \"h\"), lead_time)) +\n  labs(fill = bquote(\"[\"*degree*C*\"]\")) +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nYou may also want to do something different with cloud cover…\n\ntcc &lt;- read_forecast(\n  20240219,\n  \"ifsens\",\n  \"tcc\",\n  members          = 0, \n  file_path        = here(\"data\", \"netcdf\", \"ifsens\"),\n  file_template    = \"ifsens_20240219T000000Z.nc\",\n  file_format_opts = opts,\n  return_data      = TRUE \n) |&gt; \n  as_det()\n\nFirst experiment with what’s there for one lead time…\n\nggplot() + \n  geom_georaster(aes(geofield = fcst), tcc[1, ]) +\n  geom_path(aes(x, y), map, colour = \"yellow\") +\n  scale_fill_gradient(low = \"white\", high = \"grey40\") +\n  labs(fill = \"Cloud fraction\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nMaybe it would be better in bins of oktas -\n\nggplot() + \n  geom_georaster(\n    aes(geofield = fcst), \n    mutate(tcc, fcst = round(fcst * 8))[1, ]\n  ) +\n  geom_path(aes(x, y), map, colour = \"yellow\") +\n  scale_fill_stepsn(\n    colours = c(\"transparent\", rev(grey.colors(7))), \n    breaks = seq(0, 8)\n  ) +\n  labs(fill = \"Cloud Cover\\n[Oktas]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nAgain, once we’ve got things how we want them we can do the faceted plot\n\nggplot() + \n  geom_georaster(\n    aes(geofield = fcst), \n    mutate(tcc, fcst = round(fcst * 8))\n  ) +\n  geom_path(aes(x, y), map, colour = \"yellow\") +\n  facet_wrap(~format(valid_dttm, \"%H:%M %a %d %b %Y\")) +\n  scale_fill_stepsn(\n    colours = c(\"transparent\", rev(grey.colors(7))), \n    breaks = seq(0, 8)\n  ) +\n  labs(fill = \"Cloud Cover\\n[Oktas]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nYou may also want to do something different with precipitation…\n\npcp &lt;- read_forecast(\n  20240219,\n  \"ifsens\",\n  \"pcp\",\n  file_path        = here(\"data\", \"netcdf\", \"ifsens\"),\n  file_template    = \"ifsens_20240219T000000Z.nc\",\n  file_format_opts = opts,\n  return_data      = TRUE \n) |&gt; \n  decum(6) |&gt; \n  filter(lead_time &gt; 0) |&gt; \n  scale_param(1000, \"kg/m^2\", mult = TRUE)\n\nFirst experiment with what’s there for one lead time…\n\nggplot() + \n  geom_georaster(aes(geofield = ifsens_mbr000), pcp[1, ]) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"oslo\", direction = -1)\n  ) +\n  labs(fill = \"6h Precip\\n[mm]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nMaybe it would be better with a logarithmic colour scale\n\nggplot() + \n  geom_georaster(aes(geofield = ifsens_mbr000), pcp[1, ]) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_gradientn(\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = seq_double(0.125, 8), \n    limits   = c(0.125, NA)\n  ) +\n  labs(fill = \"6h Precip\\n[mm]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nAnd perhaps better still with some colour bands\n\nggplot() + \n  geom_georaster(aes(geofield = ifsens_mbr000), pcp[1, ]) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_stepsn(\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = seq_double(0.125, 8), \n    limits   = c(0.125, NA),\n    oob      = scales::censor\n  ) +\n  labs(fill = \"6h Precip\\n[mm]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nAgain, once we’ve got things how we want them we can do the faceted plot. However, this time let’s plot each member for a specific lead time. To do this we need to get all members into a single column. We can achieve that with pivot_members().\n\nggplot() + \n  geom_georaster(\n    aes(geofield = fcst), \n    filter(pivot_members(pcp), lead_time == 24)\n  ) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_stepsn(\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = seq_double(0.125, 8), \n    limits   = c(0.125, NA),\n    oob      = scales::censor\n  ) +\n  facet_wrap(~member) +\n  labs(fill = \"6h Precip\\n[mm]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\nOr, you could go really crazy(!) and plot each member for each lead time\n\nggplot() + \n  geom_georaster(\n    aes(geofield = fcst), \n    pivot_members(pcp)\n  ) +\n  geom_path(aes(x, y), map, colour = \"grey30\") +\n  scale_fill_stepsn(\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = seq_double(0.125, 8), \n    limits   = c(0.125, NA),\n    oob      = scales::censor\n  ) +\n  facet_grid(rows = vars(lead_time), cols = vars(member)) +\n  labs(fill = \"6h Precip\\n[mm]\") +\n  coord_equal(expand = FALSE) +\n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\nHigh spatial resolution data\nHigh resolution plots can be quite slow - let’s read some high resolution data from MET Norway’s thredds server (here we have to tell read_forecast() that the file format is NetCDF)\n\nhires_pcp &lt;- read_analysis(\n  seq_dttm(2023090415, 2023090423, \"1h\"),\n  \"met_analysis\",\n  \"precipitation_amount\",\n  file_path        = \"https://thredds.met.no/thredds/dodsC/metpparchive\",\n  file_template    = \"{YYYY}/{MM}/{DD}/{fcst_model}_1_0km_nordic_{YYYY}{MM}{DD}T{HH}Z.nc\",\n  file_format      = \"netcdf\",\n  file_format_opts = netcdf_opts(proj4_var = \"projection_lcc\")\n)\n\nmap &lt;- get_map(hires_pcp, col = anl)\n\nJust plotting a single geofield with ggplot() is slow.\n\nbrks &lt;- seq_double(0.125, 10)\n\ncensor_low_squish_high &lt;- function(x, range = c(0, 1), only.finite = TRUE) {\n  force(range)\n  finite &lt;- if (only.finite) \n      is.finite(x)\n  else TRUE\n  x[finite & x &lt; range[1]] &lt;- NA_real_\n  x[finite & x &gt; range[2]] &lt;- range[2]\n  x\n}\n\nggplot() +\n  geom_polygon(aes(x, y, group = group), map, fill = \"seagreen3\") +\n  geom_georaster(aes(geofield = anl), hires_pcp[1, ]) +\n  geom_polygon(\n    aes(x, y, group = group), map, colour = \"grey30\", fill = \"transparent\"\n  ) +\n  scale_fill_stepsn(\n    \"mm\",\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = brks, \n    limits   = c(0.125, max(brks)),\n    oob      = censor_low_squish_high\n  ) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\nSo we have ways of speeding this up - the simplest is downsampling. This basically skips pixels when plotting - and given the resolution these are likely pixels you cannot see anyway. We do this by applying an upscale factor and using \"downsample\" as the upscale method.\n\nggplot() +\n  geom_polygon(aes(x, y, group = group), map, fill = \"seagreen3\") +\n  geom_georaster(\n    aes(geofield = anl), hires_pcp[1, ], \n    upscale_factor = 4, upscale_method = \"downsample\"\n  ) +\n  geom_polygon(\n    aes(x, y, group = group), map, colour = \"grey30\", fill = \"transparent\"\n  ) +\n  scale_fill_stepsn(\n    \"mm\",\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = brks, \n    limits   = c(0.125, max(brks)),\n    oob      = censor_low_squish_high\n  ) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\nAnd when we facet, the plots are even smaller so we can get away with a higher upscale_factor\n\nggplot() +\n  geom_polygon(aes(x, y, group = group), map, fill = \"seagreen3\") +\n  geom_georaster(\n    aes(geofield = anl), hires_pcp, \n    upscale_factor = 8, upscale_method = \"downsample\"\n  ) +\n  geom_polygon(\n    aes(x, y, group = group), map, colour = \"grey30\", fill = \"transparent\"\n  ) +\n  scale_fill_stepsn(\n    \"mm\",\n    colours  = scico(256, palette = \"oslo\", direction = -1, begin = 0.2),\n    trans    = \"log\", \n    na.value = \"transparent\", \n    breaks   = brks, \n    limits   = c(0.125, max(brks)),\n    oob      = censor_low_squish_high\n  ) +\n  facet_wrap(~fct_reorder(format(valid_dttm, \"%H:%M %a %d %b %Y\"), valid_dttm)) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()"
  },
  {
    "objectID": "spatial-verif.html",
    "href": "spatial-verif.html",
    "title": "Spatial Verification",
    "section": "",
    "text": "The ensemble fractions skill score is computed by ens_fss(). The function is experimental, meaning that the results have not been verified, and there are no specialised plotting functions for the score. Unlike other functions in harpSpatial it is designed to have the same API as the point verification functions. This means that the workflow follows the\n\nRead Forecast\nRead Observations\nJoin\nVerify\nPlot\n\npattern. For this example we will use some precipitation from the MEPS model. We have data for 7 of the ensemble members for lead times 0 - 12 for a domain over the north of Norway. We can read it in with read_forecast().\n\n\n\nlibrary(harp)\nlibrary(here)\nlibrary(scico)\nlibrary(dplyr)\nlibrary(forcats)\n\nfcst_tmplt &lt;- file.path(\n  \"{fcst_model}\",\n  \"{YYYY}\",\n  \"{MM}\",\n  \"{DD}\",\n  \"{fcst_model}_lagged_6_h_subset_2_5km_{YYYY}{MM}{DD}T{HH}Z.nc\"\n)\n\nfcst &lt;- read_forecast(\n  2024021912,\n  \"meps\",\n  \"pcp\",\n  lead_time     = seq(0, 12),\n  file_path     = here(\"data\", \"netcdf\"),\n  file_template = fcst_tmplt,\n  return_data   = TRUE \n)\n\nWe can plot the data with ggplot() together with geom_georaster(). There will be more about these in the next tutorial…\n\nfc_map &lt;- get_map(get_domain(fcst$meps_mbr000), polygon = FALSE)\n\nggplot() +\n  geom_georaster(aes(geofield = meps_mbr000), fcst) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\nWarning in scale_fill_gradientn(colours = scico(256, palette = \"davos\", :\nlog-2.718282 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nSince the data are from a forecast model, they are accumulated from time zero. For the verification we are going to want 1-hour accumulations. We can calculate these with decum().\n\nfcst &lt;- decum(fcst, 1) |&gt; \n  filter(lead_time &gt; 0)\n\nggplot() +\n  geom_georaster(aes(geofield = meps_mbr000), fcst) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_fill_gradientn(colours = scico(256, palette = \"davos\", :\nlog-2.718282 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\nWe are going to verify these against the MET Nordic analysis that has a 1 km resolution. For the purposes of this tutorial we have cut the domain to a similar area over northern Norway. We can read the data with read_analysis(), which works much in the same way as read_forecast(), except that the dttm argument is the valid date-time of the data.\n\nanl_tmplt  &lt;- file.path(\n  \"{fcst_model}\",\n  \"{fcst_model}_1_0km_nordic_{YYYY}{MM}{DD}T{HH}Z.nc\"\n)\n\nanl &lt;- read_analysis(\n  dttm = unique_valid_dttm(fcst),\n  analysis_model   = \"met_analysis\",\n  parameter        = \"precipitation_amount\",\n  file_path        = here(\"data\", \"netcdf\"),\n  file_template    = anl_tmplt, \n  file_format_opts = netcdf_opts(proj4_var = \"projection_lcc\")\n)\n\nanl_map &lt;- get_map(get_domain(anl$anl), polygon = FALSE)\n\n\nggplot() +\n  geom_georaster(aes(geofield = anl), anl) +\n  geom_path(aes(x, y), anl_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\nHowever, we can’t verify the forecasts agains the the analyses since they are on different domains. We have two options here. Firstly we could use geo_regrid() to regrid the analyses to the forecast grid. Notice how the dimensions in the anl column change to match those of fcst.\n\n# before\nanl\n\n::gridded analysis:: # A tibble: 12 × 7\n   anl_model    parameter valid_dttm          level_type level units         anl\n * &lt;chr&gt;        &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;geolist&gt;\n 1 met_analysis precipit… 2024-02-19 13:00:00 unknown       NA kg/m… [901 × 601]\n 2 met_analysis precipit… 2024-02-19 14:00:00 unknown       NA kg/m… [901 × 601]\n 3 met_analysis precipit… 2024-02-19 15:00:00 unknown       NA kg/m… [901 × 601]\n 4 met_analysis precipit… 2024-02-19 16:00:00 unknown       NA kg/m… [901 × 601]\n 5 met_analysis precipit… 2024-02-19 17:00:00 unknown       NA kg/m… [901 × 601]\n 6 met_analysis precipit… 2024-02-19 18:00:00 unknown       NA kg/m… [901 × 601]\n 7 met_analysis precipit… 2024-02-19 19:00:00 unknown       NA kg/m… [901 × 601]\n 8 met_analysis precipit… 2024-02-19 20:00:00 unknown       NA kg/m… [901 × 601]\n 9 met_analysis precipit… 2024-02-19 21:00:00 unknown       NA kg/m… [901 × 601]\n10 met_analysis precipit… 2024-02-19 22:00:00 unknown       NA kg/m… [901 × 601]\n11 met_analysis precipit… 2024-02-19 23:00:00 unknown       NA kg/m… [901 × 601]\n12 met_analysis precipit… 2024-02-20 00:00:00 unknown       NA kg/m… [901 × 601]\n\n#after\ngeo_regrid(anl, fcst$meps_mbr000)\n\n::gridded analysis:: # A tibble: 12 × 7\n   anl_model    parameter valid_dttm          level_type level units         anl\n   &lt;chr&gt;        &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;geolist&gt;\n 1 met_analysis precipit… 2024-02-19 13:00:00 unknown       NA kg/m… [300 × 200]\n 2 met_analysis precipit… 2024-02-19 14:00:00 unknown       NA kg/m… [300 × 200]\n 3 met_analysis precipit… 2024-02-19 15:00:00 unknown       NA kg/m… [300 × 200]\n 4 met_analysis precipit… 2024-02-19 16:00:00 unknown       NA kg/m… [300 × 200]\n 5 met_analysis precipit… 2024-02-19 17:00:00 unknown       NA kg/m… [300 × 200]\n 6 met_analysis precipit… 2024-02-19 18:00:00 unknown       NA kg/m… [300 × 200]\n 7 met_analysis precipit… 2024-02-19 19:00:00 unknown       NA kg/m… [300 × 200]\n 8 met_analysis precipit… 2024-02-19 20:00:00 unknown       NA kg/m… [300 × 200]\n 9 met_analysis precipit… 2024-02-19 21:00:00 unknown       NA kg/m… [300 × 200]\n10 met_analysis precipit… 2024-02-19 22:00:00 unknown       NA kg/m… [300 × 200]\n11 met_analysis precipit… 2024-02-19 23:00:00 unknown       NA kg/m… [300 × 200]\n12 met_analysis precipit… 2024-02-20 00:00:00 unknown       NA kg/m… [300 × 200]\n\n\nThe alternative is to do the regridding at read time. This would generally be the more sensible approach since it saves on reading unnecessary data into memory.\n\nanl &lt;- read_analysis(\n  dttm = unique_valid_dttm(fcst),\n  analysis_model      = \"met_analysis\",\n  parameter           = \"precipitation_amount\",\n  file_path           = here(\"data\", \"netcdf\"),\n  file_template       = anl_tmplt, \n  file_format_opts    = netcdf_opts(proj4_var = \"projection_lcc\"),\n  transformation      = \"regrid\",\n  transformation_opts = regrid_opts(get_domain(fcst$meps_mbr000))\n)\n\n\nggplot() +\n  geom_georaster(aes(geofield = anl), anl) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\n\nNow, just like in the point verification workflow, we can join the forecast and analysis. Here we have to take care that columns that aren’t common but don’t matter to the join are excluded otherwise the join will return an empty data frame. We can exclude the offending columns using dplyr’s select function.\n\nfcst &lt;- join_to_fcst(\n  fcst, \n  select(anl, -parameter, -level_type, -level)\n)\n\nAt this point we could compute some basic scores, like bias or RMSE (there are no harp functions to do this yet, but it’s quite straightforward using dplyr’s mutate() in combination with ens_stats()).\n\nbias &lt;- mutate(\n  ens_stats(fcst, sd = FALSE),\n  ens_bias = ens_mean - anl\n) |&gt; \n  select(-ens_mean, -anl)\n\nrmse &lt;- mutate(\n  fcst, \n  across(contains(\"_mbr\"), ~(.x - anl) ^ 2)\n) |&gt; \n  ens_stats(sd = FALSE) |&gt; \n  mutate(\n    ens_rmse = sqrt(ens_mean)\n  ) |&gt; \n  select(-ens_mean, -anl)\n\n\nggplot() +\n  geom_georaster(aes(geofield = ens_bias), bias) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradient2(\n    low = scales::muted(\"blue\"),\n    high = scales::muted(\"red\"), \n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_georaster(aes(geofield = ens_rmse), rmse) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"lajolla\", direction = -1)\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\n\nWe can now compute the ensemble fractions skill score. There are three different scores we can compute these are summarised below:\n\n\n\n\n\n\n\nFunction\nScore\n\n\n\n\nens_fss()\nmean FSS of the ensemble as a whole\n\n\nens_efss()\nerror FSS - the mean FSS of the ensemble mean\n\n\nens_dfss()\ndispersion FSS - the mean FSS of each ensemble member against all other ensemble members\n\n\n\nThe latter two can be thought of as the spatial equivalents of the skill and spread of the ensemble respectively.\nWhen calculating the FSS we need to provide two pieces of information - the thresholds for which to compute the FSS (by default it is done for forecast &gt;= threshold and analysis &gt;= threshold, but the comparator argument can be used to choose different comparisons with the threshold), and the radii of the neighbourhoods to compute the FSS. The radius of a neighbourhood is the number of grid squares in each direction from a central grid square, thus a radius of 1 results in a 3 x 3 neighbourhood.\nWe will calculate each of the scores with thresholds of 0.1, 0.5, 1, 2, 4 and 6 mm for neighbourhood radii of 0, 1, 2, 4, 8 and 16 and grid squares.\n\nthresh &lt;- c(0.1, 0.5, 1, 2, 4, 6)\nrad    &lt;- c(0, seq_double(1, 6))\n\npcp_fss  &lt;- ens_fss(fcst, anl, thresh ,rad)\npcp_efss &lt;- ens_efss(fcst, anl, thresh ,rad)\npcp_dfss &lt;- ens_dfss(fcst, thresh ,rad)\n\nA typical way to plot the fractions skill score is as a raster, with thresholds on one axis and neighbourhood lengths on the other, with each square coloured according to the fss.\n\nggplot(\n  pcp_fss, \n  aes(\n    fct_inseq(as.character(nbhd_length / 1000)), \n    fct_inseq(as.character(threshold)), \n    fill = fss\n  )\n) +\n  geom_raster() +\n  facet_wrap(~lead_time) +\n  scale_fill_gradient2(\n    midpoint = 0.5, \n    low      = scales::muted(\"blue\"),\n    high     = scales::muted(\"green\")\n  ) +\n  labs(\n    x    = \"Neighbourhood Length [km]\",\n    y    = \"Threshold [mm]\",\n    fill = \"FSS\"\n  ) + \n    coord_equal(expand = FALSE) \n\n\n\n\n\n\n\n\nHowever, it may be clearer to make a line plot facted by either neighbourhood length or threshold.\n\nggplot(\n  pcp_fss, \n  aes(\n    lead_time, \n    fss, \n    colour = fct_inseq(as.character(nbhd_length / 1000))\n  )\n) +\n  geom_line() +\n  facet_wrap(\n    ~fct_reorder(paste0(\"Precip &gt;= \", threshold, \"mm\"), threshold),\n    ncol = 1\n  ) + \n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"FSS\", \n    colour = \"Neighbourd\\nLength [km]\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(\n  pcp_fss, \n  aes(\n    lead_time, \n    fss, \n    colour = fct_inseq(as.character(threshold))\n  )\n) +\n  geom_line() +\n  facet_wrap(\n    ~fct_reorder(\n      paste0(\"Neighbouhood Length: \", nbhd_length / 1000, \"km\"), \n      nbhd_length\n    ),\n    ncol = 1\n  ) + \n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"FSS\", \n    colour = \"Threshold [mm]\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor the eFSS and dFSS, the important piece of information is for what neighbourhood length the FSS becomes skilful. A rule of thumb is that a “skilful” FSS is a value &gt;= 0.5. We can plot the FSS = 0.5 contour for both eFSS and dFSS. In order to do this, we need to bind the two data frames and make sure each has a column saying what it is.\n\npcp_fss_ed &lt;- bind_rows(\n  mutate(pcp_dfss, score = \"dFSS\"),\n  mutate(pcp_efss, score = \"eFSS\")\n)\n\nggplot(\n  filter(pcp_fss_ed, between(threshold, 0.5, 4)), \n  aes(lead_time, nbhd_length / 1000, z = fss, colour = score)\n) + \n  geom_contour(breaks = 0.5) +\n  facet_wrap(\n    ~fct_reorder(paste0(\"Precip &gt;= \", threshold, \"mm\"), threshold),\n    ncol = 1\n  ) +\n  scale_y_log10() +\n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"Neighbourhood Length (km)\",\n    colour = NULL\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn this tutorial, as well as going through the workflow to compute the ensemble fractions skill score, we have seen some methods for plotting data in harp that do not use the built in specialised functions. In the next tutorial we will look more closely at plotting harp data with ggplot()."
  },
  {
    "objectID": "spatial-verif.html#ensemble-fractions-skill-score",
    "href": "spatial-verif.html#ensemble-fractions-skill-score",
    "title": "Spatial Verification",
    "section": "",
    "text": "The ensemble fractions skill score is computed by ens_fss(). The function is experimental, meaning that the results have not been verified, and there are no specialised plotting functions for the score. Unlike other functions in harpSpatial it is designed to have the same API as the point verification functions. This means that the workflow follows the\n\nRead Forecast\nRead Observations\nJoin\nVerify\nPlot\n\npattern. For this example we will use some precipitation from the MEPS model. We have data for 7 of the ensemble members for lead times 0 - 12 for a domain over the north of Norway. We can read it in with read_forecast().\n\n\n\nlibrary(harp)\nlibrary(here)\nlibrary(scico)\nlibrary(dplyr)\nlibrary(forcats)\n\nfcst_tmplt &lt;- file.path(\n  \"{fcst_model}\",\n  \"{YYYY}\",\n  \"{MM}\",\n  \"{DD}\",\n  \"{fcst_model}_lagged_6_h_subset_2_5km_{YYYY}{MM}{DD}T{HH}Z.nc\"\n)\n\nfcst &lt;- read_forecast(\n  2024021912,\n  \"meps\",\n  \"pcp\",\n  lead_time     = seq(0, 12),\n  file_path     = here(\"data\", \"netcdf\"),\n  file_template = fcst_tmplt,\n  return_data   = TRUE \n)\n\nWe can plot the data with ggplot() together with geom_georaster(). There will be more about these in the next tutorial…\n\nfc_map &lt;- get_map(get_domain(fcst$meps_mbr000), polygon = FALSE)\n\nggplot() +\n  geom_georaster(aes(geofield = meps_mbr000), fcst) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\nWarning in scale_fill_gradientn(colours = scico(256, palette = \"davos\", :\nlog-2.718282 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\nSince the data are from a forecast model, they are accumulated from time zero. For the verification we are going to want 1-hour accumulations. We can calculate these with decum().\n\nfcst &lt;- decum(fcst, 1) |&gt; \n  filter(lead_time &gt; 0)\n\nggplot() +\n  geom_georaster(aes(geofield = meps_mbr000), fcst) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_fill_gradientn(colours = scico(256, palette = \"davos\", :\nlog-2.718282 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\nWe are going to verify these against the MET Nordic analysis that has a 1 km resolution. For the purposes of this tutorial we have cut the domain to a similar area over northern Norway. We can read the data with read_analysis(), which works much in the same way as read_forecast(), except that the dttm argument is the valid date-time of the data.\n\nanl_tmplt  &lt;- file.path(\n  \"{fcst_model}\",\n  \"{fcst_model}_1_0km_nordic_{YYYY}{MM}{DD}T{HH}Z.nc\"\n)\n\nanl &lt;- read_analysis(\n  dttm = unique_valid_dttm(fcst),\n  analysis_model   = \"met_analysis\",\n  parameter        = \"precipitation_amount\",\n  file_path        = here(\"data\", \"netcdf\"),\n  file_template    = anl_tmplt, \n  file_format_opts = netcdf_opts(proj4_var = \"projection_lcc\")\n)\n\nanl_map &lt;- get_map(get_domain(anl$anl), polygon = FALSE)\n\n\nggplot() +\n  geom_georaster(aes(geofield = anl), anl) +\n  geom_path(aes(x, y), anl_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\nHowever, we can’t verify the forecasts agains the the analyses since they are on different domains. We have two options here. Firstly we could use geo_regrid() to regrid the analyses to the forecast grid. Notice how the dimensions in the anl column change to match those of fcst.\n\n# before\nanl\n\n::gridded analysis:: # A tibble: 12 × 7\n   anl_model    parameter valid_dttm          level_type level units         anl\n * &lt;chr&gt;        &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;geolist&gt;\n 1 met_analysis precipit… 2024-02-19 13:00:00 unknown       NA kg/m… [901 × 601]\n 2 met_analysis precipit… 2024-02-19 14:00:00 unknown       NA kg/m… [901 × 601]\n 3 met_analysis precipit… 2024-02-19 15:00:00 unknown       NA kg/m… [901 × 601]\n 4 met_analysis precipit… 2024-02-19 16:00:00 unknown       NA kg/m… [901 × 601]\n 5 met_analysis precipit… 2024-02-19 17:00:00 unknown       NA kg/m… [901 × 601]\n 6 met_analysis precipit… 2024-02-19 18:00:00 unknown       NA kg/m… [901 × 601]\n 7 met_analysis precipit… 2024-02-19 19:00:00 unknown       NA kg/m… [901 × 601]\n 8 met_analysis precipit… 2024-02-19 20:00:00 unknown       NA kg/m… [901 × 601]\n 9 met_analysis precipit… 2024-02-19 21:00:00 unknown       NA kg/m… [901 × 601]\n10 met_analysis precipit… 2024-02-19 22:00:00 unknown       NA kg/m… [901 × 601]\n11 met_analysis precipit… 2024-02-19 23:00:00 unknown       NA kg/m… [901 × 601]\n12 met_analysis precipit… 2024-02-20 00:00:00 unknown       NA kg/m… [901 × 601]\n\n#after\ngeo_regrid(anl, fcst$meps_mbr000)\n\n::gridded analysis:: # A tibble: 12 × 7\n   anl_model    parameter valid_dttm          level_type level units         anl\n   &lt;chr&gt;        &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;geolist&gt;\n 1 met_analysis precipit… 2024-02-19 13:00:00 unknown       NA kg/m… [300 × 200]\n 2 met_analysis precipit… 2024-02-19 14:00:00 unknown       NA kg/m… [300 × 200]\n 3 met_analysis precipit… 2024-02-19 15:00:00 unknown       NA kg/m… [300 × 200]\n 4 met_analysis precipit… 2024-02-19 16:00:00 unknown       NA kg/m… [300 × 200]\n 5 met_analysis precipit… 2024-02-19 17:00:00 unknown       NA kg/m… [300 × 200]\n 6 met_analysis precipit… 2024-02-19 18:00:00 unknown       NA kg/m… [300 × 200]\n 7 met_analysis precipit… 2024-02-19 19:00:00 unknown       NA kg/m… [300 × 200]\n 8 met_analysis precipit… 2024-02-19 20:00:00 unknown       NA kg/m… [300 × 200]\n 9 met_analysis precipit… 2024-02-19 21:00:00 unknown       NA kg/m… [300 × 200]\n10 met_analysis precipit… 2024-02-19 22:00:00 unknown       NA kg/m… [300 × 200]\n11 met_analysis precipit… 2024-02-19 23:00:00 unknown       NA kg/m… [300 × 200]\n12 met_analysis precipit… 2024-02-20 00:00:00 unknown       NA kg/m… [300 × 200]\n\n\nThe alternative is to do the regridding at read time. This would generally be the more sensible approach since it saves on reading unnecessary data into memory.\n\nanl &lt;- read_analysis(\n  dttm = unique_valid_dttm(fcst),\n  analysis_model      = \"met_analysis\",\n  parameter           = \"precipitation_amount\",\n  file_path           = here(\"data\", \"netcdf\"),\n  file_template       = anl_tmplt, \n  file_format_opts    = netcdf_opts(proj4_var = \"projection_lcc\"),\n  transformation      = \"regrid\",\n  transformation_opts = regrid_opts(get_domain(fcst$meps_mbr000))\n)\n\n\nggplot() +\n  geom_georaster(aes(geofield = anl), anl) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"davos\", direction = -1),\n    trans = \"log\", \n    limits = c(0.125, NA),\n    breaks = seq_double(0.125, 12),\n    na.value = \"transparent\"\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\n\nNow, just like in the point verification workflow, we can join the forecast and analysis. Here we have to take care that columns that aren’t common but don’t matter to the join are excluded otherwise the join will return an empty data frame. We can exclude the offending columns using dplyr’s select function.\n\nfcst &lt;- join_to_fcst(\n  fcst, \n  select(anl, -parameter, -level_type, -level)\n)\n\nAt this point we could compute some basic scores, like bias or RMSE (there are no harp functions to do this yet, but it’s quite straightforward using dplyr’s mutate() in combination with ens_stats()).\n\nbias &lt;- mutate(\n  ens_stats(fcst, sd = FALSE),\n  ens_bias = ens_mean - anl\n) |&gt; \n  select(-ens_mean, -anl)\n\nrmse &lt;- mutate(\n  fcst, \n  across(contains(\"_mbr\"), ~(.x - anl) ^ 2)\n) |&gt; \n  ens_stats(sd = FALSE) |&gt; \n  mutate(\n    ens_rmse = sqrt(ens_mean)\n  ) |&gt; \n  select(-ens_mean, -anl)\n\n\nggplot() +\n  geom_georaster(aes(geofield = ens_bias), bias) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradient2(\n    low = scales::muted(\"blue\"),\n    high = scales::muted(\"red\"), \n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_georaster(aes(geofield = ens_rmse), rmse) +\n  geom_path(aes(x, y), fc_map) +\n  scale_fill_gradientn(\n    colours = scico(256, palette = \"lajolla\", direction = -1)\n  ) +\n  facet_wrap(~valid_dttm) +\n  coord_equal(expand = FALSE) + \n  theme_harp_map()\n\n\n\n\n\n\n\n\n\n\n\nWe can now compute the ensemble fractions skill score. There are three different scores we can compute these are summarised below:\n\n\n\n\n\n\n\nFunction\nScore\n\n\n\n\nens_fss()\nmean FSS of the ensemble as a whole\n\n\nens_efss()\nerror FSS - the mean FSS of the ensemble mean\n\n\nens_dfss()\ndispersion FSS - the mean FSS of each ensemble member against all other ensemble members\n\n\n\nThe latter two can be thought of as the spatial equivalents of the skill and spread of the ensemble respectively.\nWhen calculating the FSS we need to provide two pieces of information - the thresholds for which to compute the FSS (by default it is done for forecast &gt;= threshold and analysis &gt;= threshold, but the comparator argument can be used to choose different comparisons with the threshold), and the radii of the neighbourhoods to compute the FSS. The radius of a neighbourhood is the number of grid squares in each direction from a central grid square, thus a radius of 1 results in a 3 x 3 neighbourhood.\nWe will calculate each of the scores with thresholds of 0.1, 0.5, 1, 2, 4 and 6 mm for neighbourhood radii of 0, 1, 2, 4, 8 and 16 and grid squares.\n\nthresh &lt;- c(0.1, 0.5, 1, 2, 4, 6)\nrad    &lt;- c(0, seq_double(1, 6))\n\npcp_fss  &lt;- ens_fss(fcst, anl, thresh ,rad)\npcp_efss &lt;- ens_efss(fcst, anl, thresh ,rad)\npcp_dfss &lt;- ens_dfss(fcst, thresh ,rad)\n\nA typical way to plot the fractions skill score is as a raster, with thresholds on one axis and neighbourhood lengths on the other, with each square coloured according to the fss.\n\nggplot(\n  pcp_fss, \n  aes(\n    fct_inseq(as.character(nbhd_length / 1000)), \n    fct_inseq(as.character(threshold)), \n    fill = fss\n  )\n) +\n  geom_raster() +\n  facet_wrap(~lead_time) +\n  scale_fill_gradient2(\n    midpoint = 0.5, \n    low      = scales::muted(\"blue\"),\n    high     = scales::muted(\"green\")\n  ) +\n  labs(\n    x    = \"Neighbourhood Length [km]\",\n    y    = \"Threshold [mm]\",\n    fill = \"FSS\"\n  ) + \n    coord_equal(expand = FALSE) \n\n\n\n\n\n\n\n\nHowever, it may be clearer to make a line plot facted by either neighbourhood length or threshold.\n\nggplot(\n  pcp_fss, \n  aes(\n    lead_time, \n    fss, \n    colour = fct_inseq(as.character(nbhd_length / 1000))\n  )\n) +\n  geom_line() +\n  facet_wrap(\n    ~fct_reorder(paste0(\"Precip &gt;= \", threshold, \"mm\"), threshold),\n    ncol = 1\n  ) + \n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"FSS\", \n    colour = \"Neighbourd\\nLength [km]\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(\n  pcp_fss, \n  aes(\n    lead_time, \n    fss, \n    colour = fct_inseq(as.character(threshold))\n  )\n) +\n  geom_line() +\n  facet_wrap(\n    ~fct_reorder(\n      paste0(\"Neighbouhood Length: \", nbhd_length / 1000, \"km\"), \n      nbhd_length\n    ),\n    ncol = 1\n  ) + \n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"FSS\", \n    colour = \"Threshold [mm]\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor the eFSS and dFSS, the important piece of information is for what neighbourhood length the FSS becomes skilful. A rule of thumb is that a “skilful” FSS is a value &gt;= 0.5. We can plot the FSS = 0.5 contour for both eFSS and dFSS. In order to do this, we need to bind the two data frames and make sure each has a column saying what it is.\n\npcp_fss_ed &lt;- bind_rows(\n  mutate(pcp_dfss, score = \"dFSS\"),\n  mutate(pcp_efss, score = \"eFSS\")\n)\n\nggplot(\n  filter(pcp_fss_ed, between(threshold, 0.5, 4)), \n  aes(lead_time, nbhd_length / 1000, z = fss, colour = score)\n) + \n  geom_contour(breaks = 0.5) +\n  facet_wrap(\n    ~fct_reorder(paste0(\"Precip &gt;= \", threshold, \"mm\"), threshold),\n    ncol = 1\n  ) +\n  scale_y_log10() +\n  labs(\n    x      = \"Lead Time [h]\",\n    y      = \"Neighbourhood Length (km)\",\n    colour = NULL\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn this tutorial, as well as going through the workflow to compute the ensemble fractions skill score, we have seen some methods for plotting data in harp that do not use the built in specialised functions. In the next tutorial we will look more closely at plotting harp data with ggplot()."
  },
  {
    "objectID": "point-verif-workflow.html#use-of-harp-point-verification-in-production",
    "href": "point-verif-workflow.html#use-of-harp-point-verification-in-production",
    "title": "Point Verifcation Workflow",
    "section": "Use of harp Point Verification in Production",
    "text": "Use of harp Point Verification in Production\nThe following presentations were made during this part of the course.\n\nUse of harp in ACCORD\n\n&lt;p&gt;&lt;&gt;&lt;/p&gt;\n&lt;p&gt;In the next tutorial will put everything you have learned here into practice by building a script that could be used for repeated verification tasks.&lt;/p&gt;\n&lt;div class=\"grid\"&gt;\n&lt;div class=\"g-col-1\"&gt;\n&lt;p&gt;&lt;a href=read-forecast.html&gt;&lt;i class=\"bi bi-arrow-left-circle-fill\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"g-col-10\"&gt;\n\n&lt;/div&gt;\n&lt;div class=\"g-col-1\"&gt;\n&lt;p&gt;&lt;a href=build-verif-script.html&gt;&lt;i class=\"bi bi-arrow-right-circle-fill\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;harp Dublin 2024&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;harp Dublin 2024&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Agenda\"&gt;Agenda&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/agenda.html\"&gt;/agenda.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Lessons\"&gt;Lessons&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Getting started\"&gt;Getting started&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/get-started.html\"&gt;/get-started.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Reading forecast data\"&gt;Reading forecast data&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/read-forecast.html\"&gt;/read-forecast.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Point Verifcation Workflow\"&gt;Point Verifcation Workflow&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/point-verif-workflow.html\"&gt;/point-verif-workflow.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Build a Point Verification Script\"&gt;Build a Point Verification Script&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/build-verif-script.html\"&gt;/build-verif-script.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Spatial Verification\"&gt;Spatial Verification&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/spatial-verif.html\"&gt;/spatial-verif.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Plotting and manipulating spatial data\"&gt;Plotting and manipulating spatial data&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/plot-manipulate-spatial.html\"&gt;/plot-manipulate-spatial.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;harp Dublin 2024 - Point Verifcation Workflow&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;harp Dublin 2024 - Point Verifcation Workflow&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;harp Dublin 2024 - Point Verifcation Workflow&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;harp Dublin 2024&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  let localAlternateSentinel = 'alternate';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "harp Dublin 2024",
    "section": "",
    "text": "Link"
  },
  {
    "objectID": "build-verif-script.html#use-of-harp-point-verification-in-production",
    "href": "build-verif-script.html#use-of-harp-point-verification-in-production",
    "title": "Build a Point Verification Script",
    "section": "Use of harp Point Verification in Production",
    "text": "Use of harp Point Verification in Production\nThe following presentations were made during this part of the course.\n\nUse of harp in ACCORD\n\n\n\nFull Screen\n\n\n\nUse of harp in UWC-West\n\n\n\nFull Screen\n\nIn the next tutorial we will go through the workflow to verify data on regular grids."
  }
]